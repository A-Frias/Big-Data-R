---
title: "Teoria completa todo R"
output: html_document
---


```{r setup, include=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width=100)
library(knitr)

colFmt = function(x,color){
  outputFormat = opts_knit$get("rmarkdown.pandoc.to")
  if(outputFormat == 'latex')
    paste("\\textcolor{",color,"}{",x,"}",sep="")
  else if(outputFormat == 'html')
    paste("<font color='",color,"'>",x,"</font>",sep="")
  else if(outputFormat == 'docx')
    paste("<font color='",color,"'>",x,"</font>",sep="")
  else
    x
}
```

# Tu reto en esta unidad

Seguramente habrás trabajado alguna vez con datos y habrás realizado algunas manipulaciones con ellos y calculado algunas estadísticas.  Es posible que para ello hayas utilizado para ello una hoja de cálculo o algún paquete estadístico basado en menús y cuadros de dialogo, pero ¿serías capaz ahora mismo de hacer tus propios programas y empezar a transformar los datos, de forma eficiente, en conocimiento?

En esta unidad vamos a aprender a utilizar R, que no es solo un software estadístico sino un lenguaje de programación, que permite realizar análisis mucho más potentes y reproducibles. R es junto con Python, que introduciremos más adelante en este curso, el lenguaje preferido por los estadísticos y científicos de datos. La curva de aprendizaje de un lenguaje de este tipo puede ser dura, pero una vez superada merece la pena.

# Introducción

El objetivo de esta unidad es aprender el funcionamiento básico del lenguaje R. El enfoque principal del curso es aprender los conceptos a partir de ejemplos por lo que es necesario ir ejecutando simultáneamente el código que te proponemos en una sesión de R conforme lees este manual.

Para ello puedes ir copiando y pegando el código del manual a la consola de R, pero te recomendamos que lo hagas de otra manera.

En campus encontrarás unos ficheros con la extensión .Rmd asociados a cada capítulo. Abriendo estos ficheros desde Rstudio podrás identificar los bloques de código como los que ves en la imagen e ir ejecutando paso a paso todos los comandos. 

![](img/ejemplo_chunk.png)

**Autotexto**
Esta estrategia la vamos a utilizar a lo largo de todo el curso, así que te animamos a que no solo vayas ejecutando los comandos paso a paso sino a que además experimentes por ti mismo  a partir de los ejemplos propuestos. 
En campus pondremos a disposición algunos vídeos en los que mostraremos como sacar el mayor partido a esta metodología de aprendizaje. 
**Fin Autotexto**

## Descarga e instalación de R

Lo primero que necesitamos es descargar e instalar R. R es software libre y su descarga es gratuita.
Libre no quiere decir siempre gratuito, aunque R también lo es, significa que podemos acceder al código escrito por otros usuarios y modificarlo libremente.

A continuación os indicamos unas breves instrucciones de descarga e instalación en el sistema operativo Windows.

**Autotexto**
Para ver paso a paso el proceso de instalación de R hay un vídeo disponible en campus
**Fin Autotexto**

R también está disponible en otros sistemas operativos como Mac OS  y Linux.


### Descargar R

- Ir al sitio web principal de R <http://www.r-project.org>
- Click en  el botón CRAN, en la parte izquierda 
- Seleccionar un  mirror cercano en el menú que aparece
    - Desde  España <https://ftp.cixug.es/CRAN/> funciona bien 
- Selecciona tu plataforma (Linux, MacOS X, Windows)
    - Click en base y os direccionará a la página de descarga de la última versión de R.

### Instalar R

- Ejecutar el fichero descargado y seguir las instrucciones de instalación utilizando todas las opciones por defecto. 
- En Windows, se instalarán las versiones de 32 y 64 bits. Se recomienda el uso de la versión de 64 bits, a no ser que se quiera utilizar algún paquete específico solo disponible para 32 bits.
- Es posible copiar el directorio completo en el que se ha instalado R en una memoria USB o un disco duro y ejecutar los ejecutables de R desde allí. Es útil para ejecutar R en una máquina donde no se tienen privilegios de administrador. 

- Instrucciones para Mac Os X: <https://www.r-bloggers.com/installing-r-on-os-x/>
- Instrucciones para Linux: <http://www.tuxylinux.com/instalar-r-en-linux/>


### RStudio

R dispone de una consola de trabajo, sin embargo es más cómodo su uso desde un entorno de desarrollo. 

RStudio es un entorno de desarrollo integrado (IDE) para R. Incluye una consola, un editor con resaltado de sintaxis que soporta la ejecución directa de código, así como herramientas para representar gráficos, depuración de código, gestión de paquetes entre otras muchas funcionalidades. Está pensado para optimizar el trabajo de análisis interactivo de datos y la programación estadística. 

Descarga de RStudio:  <https://www.rstudio.com/products/rstudio/download>.

![Captura de pantalla de  Rstudio](img/CapturaRstudio.png)


### Ejecutar R online

Si no puedes instalar R, puedes probar, al menos algunas de sus funcionalidades, a través de diferentes proveedores de R online. Estos son algunos ejemplos

- <http://www.r-fiddle.org>
- <http://www.tutorialspoint.com/execute_r_online.php>
- <http://pbil.univ-lyon1.fr/Rweb/Rweb.general.html>

También es posible ejecutar R desde el móvil, desde la aplicación Telegram, uniéndote al canal @tele_r


### Qué es y por qué *R*

R es un sistema para  análisis estadístico, manipulación y  análisis de datos,  simulación  y representación gráfica creado por Ross Ihaka y Robert Gentleman en 1993. 

R más que un software estadístico puede considerarse como un lenguaje de programación en el que podemos programar nuestros procedimientos propios y aplicaciones. R  deriva  del lenguaje S creado por los Laboratorios AT&T Bell. 

Hoy en día el lenguaje R ha traspasado las fronteras de la estadística y es posible realizar casi cualquier cosa desde él. 

Es software de código abierto con licencia GNU-GPL, y además es gratuito. Sin embargo tiene un fuerte equipo que lo desarrolla y mantiene (el core group está formado por más de 20 personas actualmente) y todos su desarrollos han sido masivamente probados. Para ver la información sobre el equipo de desarrollo podemos teclear contributors() desde la consola de R o Rstudio.

R es el "estándar de facto" en computación estadística. Entre algunas de las grandes virtudes de R encontramos:

- Grandes capacidades gráficas.

- Miles de paquetes extienden su funcionalidad

- Gran comunidad de desarrolladores e usuarios. Es fácil obtener ayuda de los expertos a través de listas de correo, foros o redes sociales. 

- Muchos paquetes orientados al modelado estadístico y al *"machine learning"*

- Generación de documentación automática/reproducible (Rmarkdown) y aplicaciones web (shiny)

- Integración con herramientas Big Data: Hadoop, Spark, etc.


### Paquetes

R es un proyecto colaborativo y su principal virtud es la cantidad de paquetes disponibles que amplían sus funcionalidades. En la actualidad, hay casi 10000 indexados en CRAN (Comprehensive R Archive Network) y sigue creciendo a buen ritmo. La lista completa está en <http://cran.es.r-project.org/web/packages/>

En la instalación por defecto de R  además del paquete *base* que contiene las funciones fundamentales para su funcionamiento, encontramos otros paquetes como: *utils , stats , datasets , graphics ,grDevices , grid , methods , tools , parallel , compiler , splines , tcltk , stats4*.

Además se incluyen una serie de paquetes recomendados  como *boot , class , cluster , codetools , foreign , KernS-mooth , lattice , mgcv , nlme , rpart , survival , MASS , spatial , nnet , Matrix*.


Además de los paquetes oficiales, existen paquetes en desarrollo disponibles habitualmente a través de  GitHub (<https://github.com>)

```{r, eval=FALSE}
library(devtools)
install_github("cjgb/caRtociudad")
```

También pueden desarrollarse paquetes para uso personal o corporativo sin necesidad de ser compartidos con la comunidad. 


A lo largo de este curso emplearemos un buen número de paquetes, como podremos ver, de gran utilidad.

**Autotexto Ojo al dato**
La siguiente figura muestra el crecimiento de paquetes registrados en CRAN a lo largo de su historia

![](img/rpackages.png)

**Fin Autotexto**



### Referencias

Os resumimos aquí algunas referencias donde obtener información y aprender más sobre R 

**Documentación oficial de R**

- <https://cran.r-project.org/manuals.html>

Especialmente 

- Introducction to R: <https://cran.r-project.org/doc/manuals/r-release/R-intro.html>

**Webs/Tutoriales**

- <http://www.burns-stat.com/documents/tutorials/impatient-r/>
- <http://www.r-tutor.com/r-introduction>

**Blogs**

- R-bloggers: <http://www.r-bloggers.com/>
- Datanlytics: <https://www.datanalytics.com/>


**Comunidad**

- Listas de correo: <small> <http://www.r-project.org/mail.html> </small> 
- Stackoverflow: <small> <http://stackoverflow.com/questions/tagged/r> </small>
- Asociación R-hispano  <small> <http://r-es.org/> </small>
- Grupos de usuarios locales: <small> <http://r-es.org/grupos-locales/> </small>

**Libros**

- R para Principiantes, Emmanuel Paradis <small>  <http://cran.r-project.org/doc/contrib/rdebuts_es.pdf> </small>
- R Programming for Data Science, Roger D. Peng <small>  <http://leanpub.com/rprogramming> </small>
- icebreakeR: <small> <http://www.ms.unimelb.edu.au/~andrewpr/r-users/icebreakeR.pdf> </small>
- Introduction to Probability and Statistics Using R, G. Jay Kerns. Se obtiene en formato pdf descargando el paquete "IPSUR" desde R.


## Primeros pasos en R

Este apartado es muy importante y muy práctico. ¿Te gustaría que te hiciésemos una presentación de viva voz ?

**Autotexto Video**
Entra en Campus y allí encontrarás un vídeo de esta sección que te ayudará mucho con tus primeros pasos en R.  
**Fin Autotexto**

### Rstudio 

Exploremos en primer lugar el entorno de desarrollo Rstudio que vamos a utilizar a lo largo del curso.  

![Areas de trabajo de  Rstudio](img/CapturaRstudio.png)


- La **consola de R** es el área en la que se ejecuta código (`Ctrl + 1`)
     - Indica con > que está listo para aceptar comandos.
     - Indica con + que está a la espera de completar
     - Salir o interrumpir ejecución con Esc
     - Se puede recuperar comandos antiguos con flechas arriba y abajo.

- El **área de código** es donde se edita y almacena código (`Ctrl + 2`)
     - Escribir (y grabar) en área de código y enviar a consola (`Ctrl + Enter`)
     - Permite completar comandos con Tab
     
-  En los paneles laterales podemos explorar el espacio de trabajo (Workspace), el historial de comandos (History), las gráficos realizados (Plots), la ayuda (Help), entre otras cosas. 

- Mas ayuda sobre el uso de Rstudio en <https://support.rstudio.com/hc/en-us/sections/200107586-Using-RStudio>


### Sesión de ejemplo

Antes de empezar conocer los detalles de R hagamos una sesión de ejemplo que te va a permitir hacerte una idea del funcionamiento general de la herramienta de su potencial.


```{r,eval=FALSE}
# Ejecuta línea a línea desde el area de consola de Rstudio los siguientes comandos

# Para poner comentarios en el código se utiliza #
# El intérprete de R ignora todo aquello precedido del símbolo #

# La utilidad más sencilla de R es como calculadora

5+7
23*54
2^5
log(pi/2)
sqrt(3)
exp(2)

# La estructura más sencilla de datos es un vector numérico
# Podemos crear un vector como

x=c(1,2,5,7)
x

# o como una secuencia ordenada de números

y=1:4
y
# O a partir de un generador de números aleatorios
z=runif(4,0,10)
z
# Puedo redondear estos números 
z=round(z,0)

#  Las operaciones en R son vectoriales, en el sentido de que se realizan por defecto elemento a elemento
x+y
x+y+x
x*y
z^x

# Veamos un ejemplo más complejo
# Generamos un vector con los números del 1 a 100
# Observemos que  como operador de asignación R utiliza <-
x<-1:100
#Mostremos los 5 primeros valores de x
x[1:5]

# Generemos una variable y, relacionada linealmente con x, y con residuos aleatorios distribuidos según una distribución normal
y=3*x -12
# Generemos un vector con números aleatorios distribuidos normalmente con media 0 y desviacion típica 20
z=rnorm(100,mean=0,sd=20)
# Añadimos el error aleatorio a la variable y
y=y+z

# Representemos la variable y frente a la x en un gráfico de dispersión
plot(x,y)
# Algo más bonito
plot(x,y,pch=19,col="red",cex=0.5,main="x vs y")

# Construimos una tabla de datos (data.frame) que contenga a x e y
df=data.frame(x,y)
# Mostremos su contenido
View(df)
# Añadimos una nueva columna al data.frame que represente una categoría a la que pertenecen las observaciones
# Hacemos un muestreo aleatorio de 100 extracciones de dos posibles bolas "a" y "b"
z=sample(c("a","b"),100,replace = TRUE)
z
#Añado la columna
df=cbind(df,z)
# Plot coloreado según la categoría de los puntos
plot(df$x,df$y,pch=19,col=df$z,cex=0.5,main="x vs y")

# Ajustemos los puntos a un modelo lineal Y=a + b*X + eps
# Los coeficientes ajustados a y b deberian corresponder con los que hemos usado en la generación de los puntos
# a=-12 ; b=3
mod = lm(y~x,data=df)
?lm
mod
summary(mod)
str(mod)

# dibujamos la recta ajustada
plot(df$x,df$y,pch=19,col="red",cex=0.5)
abline(a=mod$coefficients[1],b=mod$coefficients[2])

```


### Paquetes 

Para instalar un paquete se usa el comando `install.packages`

Instalemos algunos paquetes que vamos a usar a lo largo de este curso

```{r,eval=FALSE}
install.packages("ggplot2")
# Pueden cargarse varios paquetes a la vez
install.packages("reshape2","dplyr","ISLR")
```

Para cargar paquetes usamos el comando `library`

```{r,eval=FALSE}
# R contiene muchos conjuntos de datos disponibles como mtcars
# Si quieres saber que contiene exactamente este conjunto teclea ?mtcars
# ggplot2 es paquete para la realización de gráficos de alta calidad

library(ggplot2)
ggplot(mtcars) + geom_point(aes(wt, mpg,color=factor(cyl)))
ggplot(mtcars) + geom_point(aes(wt, mpg,color=factor(cyl))) + facet_wrap(~am)
```

También pueden llamarse a funciones contenidas en un paquete sin necesidad de cargarlo, anteponiendo al nombre de la función `nombredelpaquete::`

```{r,eval=FALSE}
# Calculo la media de la variable wt (peso en miles de libras) del conjunto de datos mtcars
dplyr::summarise(mtcars,wtm=mean(wt))
```

Podemos ver que paquetes están cargados con 

```{r}
search()
```

y con más detalle con

```{r,eval=FALSE}
sessionInfo()
```

### Ayuda en R

Desde la consola de R existen muchas formas de pedir ayuda sobre una función

La más sencilla es anteponiendo el símbolo de interrogación al nombre de la función sobre la cual se quiere pedir ayuda

```{r}
?read.table
```

Esto es equivalente a 
  
```{r}
help(read.table)
```
  
Para buscar de forma más amplia, no solo por nombre de función usamos *??*

```{r,eval=FALSE}
??plot
```

o equivalentemente

```{r,eval=FALSE}
help.search("plot")
```

**Autotexto Pista o similar**
Si al principio no entiendes muy bien el contenido de la ayuda, no desesperes, ve al final donde encontraras ejemplos de uso de la función que seguramente te ayudarán a entender mejor el funcionamiento de la función.  
**Fin autotexto**

Muchas funciones, además de la documentación sobre su uso, incorporan demostraciones con varios ejemplos de uso. 
Estos pueden verse mediante el comando example

```{r,eval=FALSE}
example(plot)
```

Desde Rstudio también se puede pedir ayuda utilizando el panel lateral *"Help"*

![](img/help_rstudio.png)


### Explorar la sesión

Es posible explorar los objetos almacenados en la sesión de R mediante

```{r, eval=FALSE}
ls()
```

Para borrar un objeto

```{r}
x <- 3
rm(x)
```

En Rstudio es posible explorar los objetos almacenados en la sesión de R mediante el panel lateral *"Enviroment"*

### Directorio de trabajo

Un concepto importante en R, es el directorio de trabajo (working directory). Cuando se inicia R, el directorio de trabajo se establece según el valor configurado por defecto. Habitualmente es el directorio de usuario por defecto del sistema operativo, en Windows sería típicamente `c:\\Users\\nombredeusuario` y en Linux o Mac `/home/nombredeusuario`. Aunque estemos trabajando en Windows, R entiende las rutas al estilo Linux, lo cual recomendamos por simplicidad. 

El directorio de trabajo, será el origen por defecto de las rutas de archivos que utilicemos para leer y escribir ficheros.

Desde la consola de R, los comandos para conocer y cambiar el directorio de trabajo son `getwd` y `setwd` respectivamente.

```{r, eval=FALSE}  
setwd("c:/masterd/CursoBigData/02_introduccionR/")
getwd()
```

Desde RStudio podemos conocer el directorio de trabajo, leyendo la barra superior del área de consola. Es posible cambiar su ubicación a partir de la pestaña Files de los paneles laterales. 


### Guardar

Es posible guardar, en formato binario, el contenido un objeto mediante el comando `save` 

```{r}
save(iris,file="iris.Rdata")
```

Este objeto puede cargarse en la misma u en otra sesión de R, incluso en una maquina distinta mediante el comando `load`

```{r,eval=FALSE}
load("iris.Rdata")
```

Es posible guardar la sesión entera mediante

```{r}
save.image("sesion.RData")
```

R al terminar una sesión pregunta si quiere guardar la sesión. En caso afirmativo, lo hace en un fichero llamado `.RData` en el directorio de trabajo.

R al iniciarse carga automáticamente (si no se le dice lo contrario) el fichero `.RData`, si existe, en el directorio desde el que se lanza, por defecto el directorio de usuario del sistema operativo. 

Es igualmente posible cargar una sesión mediante el comando load, de la misma manera que se carga un objeto.

```{r,eval=FALSE}
load("sesion.Rdata")
```


# Objetos en R

En las secciones anteriores hemos creado estructuras de datos (objetos). Lo hemos hecho mediante el operador de asignación `<-`
(Se puede usar igualmente el operador = para la asignación, aunque se recomienda el uso de `<-` ). 

```{r,eval=FALSE}
x <-1
z <-c(2,3,4)
mod <- lm(y~x,data=df) 
```

En esta sección vamos a ver con más detalle el tipo de objetos existen en R.

## Vectores

La clase de objeto básico en R es el **vector (atómico)**. Se trata de un conjunto de datos del mismo tipo. Existen 5 tipos distintos de vectores: 

- character (texto)
- numeric (números reales)
- integer (números enteros)
- logical (TRUE/FLASE)
- complex (números complejos)

<br>
```{r}
# vector tipo character de longitud 1
x1<- "Hola"  
typeof(x1)
length(x1)
# vector tipo character de longitud 4
x2<- c("Juan","Pablo","Enrique","Javier")
typeof(x2)
length(x2)
# Vector numeric/double
x3 <- c(1.2,5,3/4)
typeof(x3)
length(x3)
# Vector integer
x4<- 1L
typeof(x4)
length(x4)
# En cambio sin la L es numeric 
x5<-1
typeof(x5)
length(x5)

# Vector lógico
x6<- c(TRUE,FALSE,FALSE,FALSE)
typeof(x6)
length(x6)

```

La función `c` que hemos usado para crear vectores es el operador concatenación y su cometido es poner cosas juntas


```{r}
# Sirve para definir vectores a partir de elementos
x <- c(1,5,6)
y <- c(7,1,3)
# Pero tambien concatena objetos ya existentes
z<- c(x,y)
z
```

Es destacable que en *R* no existen variables escalares (un único valor), son un caso particular de vectores con longitud 1. 

- El comando `typeof` nos dice el tipo de datos que contiene el vector y 
- El comando `length` nos dice el número de elementos del vector


## Generación de vectores

Veamos  diferentes formas de generar vectores en R 

```{r}
x <- 3; x
```

Secuencia ordenada

```{r}
y <- 1:5; y
```

Secuencia Lineal

```{r}
z <- seq(0,5,by=0.5); z
```

Concatenación

```{r}
t <- c(x,y,z); t
```

### Vectores Aleatorios

A lo largo del curso, para poner ejemplos o para explicar algunos conceptos estadísticos necesitaremos poder generar vectores a partir de secuencias de números pseudo-aleatorios, o tomar muestras aleatorias de datos ya existentes. 

**Autotexto +info**  
En ciencia de datos es habitual realizar simulaciones de procesos probabilísticos en las cuales es fundamental la generación de números aleatorios. En realidad no se trata de números aleatorios reales, son cadenas de números pseudo-aleatorios que se generan mediante una fórmula matemática precisa pero, sin embargo los números generados, tienen propiedades matemáticas muy similares a los números aleatorios reales 
**Fin autotexto**  


```{r}
# Generación de vectores aleatorios
# Uniformes en el intervalo (0,1)
(x<- runif(10,0,1))
# Normalmente distribuidos
(y<- rnorm(10,mean=0,sd=1))
# Muestreos
(z1<- sample(1:20,10,replace=TRUE))
(z2<- sample(1:20,10,replace=FALSE))
```

Observad que poniendo una expresión de R, que sea una asignación de un valor a una variable entre paréntesis, además de realizar la asignación escribe el valor asignado en la consola. 

### Vectores de Texto

Veamos algunas formas de generar vectores de texto

```{r}
# Concatenación de cadenas - La función paste es muy importante
str1 <- paste("A", 1:6, sep= "")
str1
# Repetición
str2 <- rep("b", 5)
str2
str3 <- rep(letters[1:5], 3)
str3
# Muestreo
sample(letters, 5, replace=TRUE)
# Observad que cada vez se generan nuevos valores
sample(letters, 5, replace=TRUE)
```

### Operaciones Básicas con Vectores

Las operaciones vectoriales en R se realizan elemento a elemento

```{r}
  x <- 1:4
  x + 1
  x^2
  y <- 1:8
  # Ojo al reciclaje. x se convierte en c(1:4,1:4) para poder sumar con y
  x + y
  x * y
  x^3 + sqrt(y)

```

### Operaciones a través de funciones

R dispone de un gran número de funciones que nos permiten realizar operaciones con vectores. Algunos ejemplos son 

|Función| Descripción                                                 |
|-------|-------------------------------------------------------------|
|sum(x) |suma de los elementos de x|
|prod(x)|producto de los elementos de x|
|max(x)|valor máximo en el objeto x|
|min(x)|valor mínimo en el objeto x
|which.max(x)| devuelve el índice (posición) del elemento máximo de x|
|which.min(x)|devuelve el índice (posición) del elemento mínimo de x|
|range(x)|rango de x o c(min(x), max(x))|
|length(x)|número de elementos en x|
|mean(x)|promedio de los elementos de x|
|median(x)|mediana de los elementos de x|
|round(x, n)|redondea los elementos de x a n cifras decimales|
|rev(x)|invierte el orden de los elementos en x
|sort(x)|ordena los elementos de x en orden ascendente|
|rank(x)|devuelve un vector con la posicion de los elementos de  x en un vector ordenado|
|log(x, base)|calcula el logaritmo de x en base "base"|
|pmin(x,y,...)|un vector en el que el iesimo elemento es el mínimo de x[i], y[i], . . |
|pmax(x,y,...)|igual pero con el máximo|
|cumsum(x)|suma acumulada: un vector en el que el iesimo elemento es la suma desde x[1] a x[i]|
|cumprod(x)|lo mismo pero con el producto|
|cummin(x)|lo mismo pero con el mínimo|
|cummax(x)|lo mismo pero con el máximo|
|choose(n, k)|combinaciones de n elementos tomados de k en k|

Veamos algunos ejemplos de uso

```{r}
# Genero 20 números aleatorios entre 0 y 100
set.seed(123)
x=runif(20,0,100)
x
sum(x)
mean(x)
length(x)
min(x)
max(x)
round(max(x),1)
log(x[1])
sort(x)
median(x)
sort(x)[10:11]
mean(sort(x)[10:11])
cumsum(x)
pmin(c(1,5,7),c(2,3,6))
which(x<20)
which.max(x)
x[which.max(x)]
```


### Atributos

Los objetos de R además de valores, tienen atributos, que son como metadatos del objeto. 
Estos metadatos pueden ser muy útiles en algunos casos que veremos. Algunos ejemplos de atributos son:

- nombres
- dimensiones (por ejemplo en matrices y arrays)
- longitud
- niveles de una variable tipo factor

Los atributos de un objeto se pueden ver mediante el comando `attributes`. No todos los objetos de R contienen atributos, en este caso la función `attributes` devolvería NULL


## Otros objetos en R

Los vectores son el tipo básico de objeto en R, pero existen más tipos de datos como 

-  Las **matrices (matrix)**  o, de forma general, los arrays  son generalizaciones
multidimensionales de los vectores.
- Los **factores** que sirven para representar datos categóricos.
- Las **listas** son una forma generalizada de vector en las cuales los elementos no tienen
por qué ser del mismo tipo. A menudo los elementos de una lista son a su vez vectores u otras listas. A partir de listas y atributos podemos construir objetos todo lo complicados que queramos.
- Los  **data frames** son estructuras similares a una matriz, en la que cada
columna puede ser de un tipo distinto a las otras. Los data frames son apropiados
para describir "matrices de datos" donde cada fila representa a una observación y cada
columna una variable, pudiendo ser estas de diferente tipo: numéricas o categóricas.
- Las **funciones** son también objetos de R.

Las estructuras de datos en R se pueden clasificar según su dimensión (1d, 2d, o más dimensiones (nd))  y por si son homogéneas (todos los contenidos deben ser del mismo tipo) o heterogéneas (los contenidos pueden ser de distinto tipo) de la siguiente manera.


```{r, echo=FALSE,eval=TRUE}

library(knitr)
kable(data.frame('Homogéneos'=c("(Atomic) vector","Matrix","Array"),
                 'Heterogéneos' =c("List","Data Frame",""),row.names = c("1d","2d","nd")))

```

### Factores

Un *factor* es un tipo de dato que se usa para representar variables categóricas. Los factores son necesarios para la definición de "modelos estadísticos" con variables categóricas. 

En realidad es un dato tipo entero con un atributo llamado *levels* que hace una correspondencia entre las posibles categorías y números enteros. Los factores parecen, y a veces se comportan como vectores de texto,pero en realidad son números enteros.

Veamos un ejemplo 

```{r}
# genero un vector de longitud 10, donde cada elemento puede ser a,b o c
str<- sample(letters[1:3],10,replace=TRUE); str
# Convierto el vector de texto str a factor
fac <- factor(str); fac
levels(fac)
# Clase del objeto
class(fac)
# Tipo de dato (es integer)
typeof(fac)
# Si queremos verlo como numérico
as.numeric(fac)
```

Los objetos en R pueden tener un atributo llamado `class`, como es el caso de los factores. En este caso cuando una función genérica (llamémosle `fun`) se aplica a un un objeto perteneciente a una clase `miclase`, el sistema busca una función `fun.miclase` y si la encuentra se la aplica al objeto. 



### Listas

Colección de objetos de distinto tipo y  de diferente longitud. 

```{r}

mylist <- list(a = c(10,20,30),
              b = c('do', 're', 'mi', 'fa', 'sol','la','si'),
              c = matrix(1:9,nrow=3))
mylist

class(mylist)
length(mylist)
```

La  lista del ejemplo contiene un vector numérico, un vector tipo character y  una matriz, cada uno de un tamaño distinto. 

Una lista puede estar compuesta por otras listas 

```{r}
mylist1 <- list(a = c(10,20,30),
              b = c('do', 're', 'mi', 'fa', 'sol','la','si'))
mylist2 <- list(x=1:10,y=11:25)
# Lista de listas
mylist3= list(mylist1,mylist2)
mylist3
```

### Matrices

Una matriz es un vector pero indexado en dos dimensiones (filas y columnas). Veamos un ejemplo

```{r}
z <- 1:12
M <- matrix(z, nrow=3)
M

#clase
class(M)
#dimensión  
dim(M)
#resumen
summary(M)
```

La función summary es una función genérica que proporciona un resumen de los datos. En función del tipo de dato realiza acciones diferentes. En el caso de una matriz ejecuta realmente la función summary.matrix

### Creando matrices a partir de vectores rbind y cbind

Puedo crear matrices concatenando vectores por filas o por columnas

```{r}
x <- 1:5
y <- 6:10
z <- 11:15
# Concatenación por columnas
M <- cbind(x, y, z)
M
# Concatenación por filas
M <- rbind(x, y, z)
M

```


### Operaciones con matrices

Veamos algún ejemplo de operaciones con matrices

```{r}
# Traspuesta (cambio filas por columnas)
TM <- t(M)
TM
# Suma
B=T + TM
# Producto de matrices
C=M %*% TM
C
# Matrices de texto
M[1,1] <- "un poco de texto" 
M
```


### Data Frames

Colección de objetos de igual longitud. Una fila por observación, una columna por variable. Las columnas pueden ser de distintos tipos: character,factor, integer, numeric, etc ...

Los data frames son el  modo más habitual de almacenar datos en R y se usan continuamente.

```{r}
df <- data.frame(a=letters[1:5], x = 1:5,
                   y = rnorm(5), z = 0)
df
length(df)
dim(df)
summary(df)
```

## Funciones

### Definición y aplicación

Las funciones en R nos permiten personalizar el código y ampliar sus capacidades.

Para definir una función usamos el comando *function*

```{r}
# Definición
CircleArea <- function(r) { 
  pi*r^2
}

# La función es un objeto
CircleArea
class(CircleArea)

# Aplicación

CircleArea(5)

```


Las funciones por defecto operan vectorialmente

```{r}
x=runif(10, min=0, max=5)
x
CircleArea(x)
```



y pueden devolver cualquier tipo de objeto

```{r}
foo  <-  function(x, y){
  mx <- mean(x)
  sdx <- sd(x)
  lmod=lm(y~x)
  list(media=mx, std_Dev=sdx, modelo= lmod)
  }

foo(1:10,1:10 + rnorm(10))
```

### Argumentos

Una función identifica sus argumentos por su nombre y por su orden (sin nombre)

```{r}
potencia <- function(x, exp)
{
    x^exp
}
# orden
potencia(1:10, 2)
# nombre
potencia(x=1:10, exp=2)
potencia(exp=2, x=1:10)

```

R permite  asignar un valor por defecto a los argumentos

```{r}
potencia <- function(x, exp = 2)
{
    x ^ exp
}
potencia(5)
potencia(5,2)
```

También es posible definir funciones sin argumentos

```{r}
hola <- function()
{
    print('Hola fondo norte')
    print('Hola fondo sur')
}

hola()

```


Existen un tipo especial de argumentos denotados por `...` que suelen emplearse para permitir pasar argumentos opcionales a otras funciones a las que está llamando nuestra función  

```{r}
sqrootSum <- function(x, ...)
{
    sum(x ^ (1/2), ...)
}

x <- seq(-5,5,1)
sqrootSum(x)
sqrootSum(x,na.rm=TRUE)

```

Le estamos pasando a sqrootSum el argumento `na.rm=TRUE` que a su vez le está pasando a la función de R `sum`. Con esto conseguimos que no tenga en cuenta los valores erroneos en el cálculo que queremos que haga.

En R no es necesario pasarle todos los argumentos a una función , es capaz de gestionar posibles argumentos ausentes con el comando  `missing`


```{r,fig.height=4,fig.align='center'}
myplot <- function(x, y) {
                if(missing(y)) {
                        y <- x
                        x <- 1:length(y)
                }
                plot(x, y)
}
myplot(rnorm(10))
```



# Indexado de datos

El indexado de datos, nos permite obtener subconjuntos de estos mediante condiciones.
Los operadores para el  indexado de datos en R son muy potentes y es una de las principales virtudes del lenguaje. 

## Indexado numérico

Podemos seleccionar subconjuntos en los datos indicando las posiciones de los elementos que queremos seleccionar

```{r}
# Genero un vector x
x <- seq(1, 100, 2)
x
# Seleciono el 3er elemento
x[3]
# Seleciono los cinco primeros elementos
x[1:5]
# Seleciono los elementos de la quinta a la décima posición en orden inverso
x[10:5]
```

## Condiciones lógicas

También es posible seleccionar datos mediante condiciones lógicas. 
Primero veamos como funcionan los operadores lógicos

```{r}
x <- sample(c(-2,-1,0,1,2),20,replace=TRUE)
x
# x menor que 0. Devuelve TRUE en las posiciones que cumplen la condicion
# y FALSE en las que no
x < 0
# x mayor o igual que 0
x >= 0
# x igual a 0
x == 0
# x distinto de 0
x != 0
```

## Condiciones  lógicas múltiples

Podemos realizar operaciones lógicas múltiples mediante los operadores lógicos: 

- `&`  (y): se cumplen ambas condiciones
- ` |`  (o): Se cumple alguna condición

```{r}
cond  <-  (x > 0) & (x <=1)
cond

cond  <-  (x >1) | (x < -1)
cond

```


## Indexado con condiciones lógicas

R permite el indexado mediante una condición lógica:

- R realiza la operación lógica obteniendo un vector lógico con TRUEs y FALSEs
- Selecciona las posiciones del dato que tienen TRUE en el resultado de aplicar la condición

```{r}
x <- 1:50
# Dame los valores de x que sean distintos de 9
x[x != 9]
# Dame los valores de x que sean mayores de 20
x[x > 20]
y <- round(runif(10,1,100))
y
# Dame los valores de x que tambien estén en y
x[x %in% y]
# Dame los valores de x en las posiciones en las que y es mayor que 50
x[y>50]

```

Con condiciones múltiples el funcionamiento es equivalente

```{r}
z <- seq(-5, 5, by = .5)
z
z[z < -3 | z > 3]

cond <- (z >= 0 & z <= 4)
cond
z[cond]

```

Veamos un ejemplo práctico de uso de los operadores de selección

3 jugadores juegan a los dados, tiran 10 veces cada uno

```{r}
# Fijo la semilla del generador de números aleatorios. De esta forma al ejecutarlo en tu ordenador obtendrás los mismos resultados que en el texto. Si cambias la semilla obtendras otros resultados, perfectamente válidos.
set.seed(153)
# Genero las tiradas: 30 valores aleatorios entre 1 y 6
tirada = sample(1:6,30,replace=TRUE)
tirada
# Asigno las tiradas a los jugadores 1,2 y3. Las 10 primeras al jugador 1, las 10 siguientes al 2 y las últimas al 3
jugador = rep(1:3,each=10)
jugador
#¿Cual es la suma de las tiradas del jugador 1
sum(tirada[jugador==1])
# ¿La suma de los jugadores 2 y 3 juntos?
sum(tirada[jugador>1])
# ¿La suma de los jugadores 1 y 3 juntos?
sum(tirada[jugador==1 | jugador==3])
#¿Qué jugador ha sacado más seises?
t1 <- tirada[jugador==1]
length(t1[t1==6])
t2 <- tirada[jugador==2]
length(t2[t2==6])
t3 <- tirada[jugador==3]
length(t3[t3==6])
# El jugador 1 es el que más 6's,  ha sacado 2
```

## Indexado de matrices

```{r}
M=matrix(1:16,nrow=4)
M
M[1,2]
M[3:4, ]
M[1:2, 2:4]
M[1, c(1, 3)]

```

## Indices negativos

Indican que filas o columnas **no** seleccionar
```{r}
M[-1,]
M[,-c(1, 2)]
# Funciona igual con vectores
x=1:10
x[-(1:2)]
```

## Indexado de listas
Para seleccionar elementos en listas, podemos hacerlo por su nombre

```{r}
mylist1 <- list(a = c(10,20,30),
              b = c('do', 're', 'mi', 'fa', 'sol','la','si'))
mylist$a
```

o por su índice

```{r}
#[] devuelve una lista de un elemento
mylist[1]
#[[]] devuelve el contenido del primer elemento de mylist
mylist[[1]]
```

## Indexado de data frames

Por su nombre (como una lista)
```{r}
df$x

```
Por su índice (como una matriz)
```{r}
df[1:2,]
df[,1]

```

Es posible tomar subconjuntos de data frames mediante condiciones lógicas, pero eso ya lo aprenderemos más adelante en este curso


# Control de flujo

Ya hemos visto que R más que un software estadístico es un lenguaje de programación. La mayor potencia de un lenguaje de programación la encontramos en realizar tareas repetitivas mediante bucles (for, while) o poder controlar la ejecución de un programa mediante condiciones lógicas (if/else).

## Bucles for

Mediante un bucle for es sencillo realizar un cálculo repetitivo como la suma los cuadrados de los 100 primeros números naturales $\sum_{n=1}^{n=100} n^2$. Veamos como hacerlo en R

```{r}
sum=0
for(i in 1:100){
  sum = sum + i^2
}
sum
```
La variable i va tomando todos los valores en la secuencia 1:100 (1,2,3, ..., 100) y en cada paso añade $i^2$ a la variable *sum*.

Notad que la sintaxis del bucle for es distinta a como se hace en los lenguajes  C o Java. Se parece en cambio a la del lenguaje Python.  

A pesar de que la operación anterior se puede realizar mediante un bucle for,  R es mas eficiente operando vectorialmente. La forma óptima de hacer el cálculo anterior en R es

```{r}
sum((1:100)^2)
```

## Funciones  *apply

En R,  suele ser más eficiente que los bucles for, usar  la familia de funciones \*apply para realizar cálculos repetitivos sobre los elementos de un vector, una matriz o una lista. Veamos algunos ejemplos de la familia de funciones `*apply`

`lapply(x,FUN)` aplica la función `FUN` a los diferentes elementos de `x` y devuelve una lista de la misma longitud que `x`.

`sapply` funciona de la misma manera pero devuelve un vector o una matriz, dependiendo del tipo de dato  que sea `x`, en lugar de una lista.

```{r}
# lapply devuelve una lista
lapply(1:5, function(i){sum(1:i)})
# sapply devuelve un vector
sapply(1:5, function(i){sum(1:i)})
```

Comprobemos que las operaciones tipo apply son más eficientes que los bucles. El comando `system.time` mide el tiempo que dura la ejecución de un código determinado. 

Aunque lo realmente rápido es usar funciones vectorizadas, si es posible

```{r,cache=TRUE}
N <- 20000

# Bucle for
system.time({
  x=NULL
  for(i in 1:N){
    x[i]=sum(as.numeric(1:i))
    }
  })
# sapply
x1=NULL
system.time({x1=sapply(1:N, function(i){sum(as.numeric(1:i))})})

# función vectorial
system.time({x2=cumsum(as.numeric(1:N))})
```

## Matrices

Para realizar cálculos repetitivos por filas o columnas de matrices se usa la función `apply` 

```{r}
# suma de las filas
apply(M, 1, sum)
# de otra manera
rowSums(M)
# media de las columnas
apply(M, 2, mean)
colMeans(M)
```

Algunas tareas habituales como la suma de los elementos de una matriz por filas o columnas, tienen su función vectorial correspondiente como  `rowSums`y `colSums` que son más eficientes que `apply`.


## Condiciones if - else

Una condición del tipo 

```{r,eval=FALSE}
if cond1
  expr1
else
  expr2
```

evalúa la condición lógica `cond1` y si es cierta ejecuta el código en `expr1`, en caso contrario ejecuta el código de `expr2`

```{r}
x <- rnorm(5);x
y<- NULL
for(i in 1:5){
  if(x[i]>0){
    y[i]=1
  } else{
    y[i]=-1
  }
}
y
```

El bloque `else`, no es imprescindible

```{r}
x <- rnorm(5);x
y<- rep(0,5)
for(i in 1:5){
  if(x[i]>0){
    y[i]=1
  }
}
y
```

En R, existe una versión vectorial de las condiciones if-else que debe usarse siempre que se pueda. 

```{r}
y=ifelse(x>0,1,-1)
y
```

# Escribiendo un programa en R

R es un lenguaje interpretado, por tanto los programas suelen denominarse *scripts* (guiones).
Podemos crear un programa escribiendo nuestros comandos en un fichero de texto, que grabaremos con la extensión .R y después ejecutarlo desde la consola de R o Rstudio. 

Desde Rstudio, creamos una ventana de script a partir del menú de la aplicación: File-> New  File -> R script
Escribiremos el código en un fichero y lo grabaremos (File->Save) con el nombre, por ejemplo, miguion.R.

Podemos escribir el programa usando cualquier otro editor de texto, pero Rstudio tiene una serie de funcionalidades que lo hacen especialmente apropiado para crear y ejecutar programas en R. 



## Ejecución de programas en R

Para ejecutar un programa de R podemos hacerlo de diversas formas.

### source

Podemos cargar código almacenado en un fichero mediante la función source

```{r,eval=FALSE}
source('../../scripts/calcula_factorial_10.R')
```

Desde Rstudio, es equivalente pulsar el botón Run desde la ventana de edición con el programa que queremos ejecutar abierto.

### Desde terminal de comandos

```{r,eval=FALSE}
Rscript calcula_factorial_10.R

R CMD BATCH  calcula_factorial_10.R

```


# Trabajando con datos en R

En esta sección vamos a comenzar a trabajar con alguna de las funcionalidades que hacen de R uno de los programas más utilizados para el análisis de datos.

## Directorio de trabajo

Antes de comenzar con la lectura de ficheros de datos, es necesario conocer, y cambiar si fuera necesario, nuestro directorio de trabajo

```{r, eval=FALSE}  
# ¿Cual es mi directorio de trabajo?
getwd()
# Lo cambio si es necesario
# setwd("c:/Uers/masterd/CursoBigData/02IntroduccionR/")
```

Desde la consola podemos consultar los ficheros existentes en un un directorio, filtrando por un determinado patrón o dentro de un subdirectorio.

```{r}
dir()
dir(pattern='.Rmd')
dir('Ejercicios')
```

### Lectura de ficheros

El comando básico para la lectura de ficheros de datos estructurados en columnas es *read.table*. Lee los datos de un fichero de texto delimitado y los almacena en un data frame. 

Para los ejemplos de esta sección vamos a utilizar un conjunto de datos con resultados, estadísticas y cuotas de apuestas de los partidos de la primera división de fútbol española. Estos datos son una versión simplificada de los que pueden descargarse de <http://football-data.co.uk/mmz4281/1617/SP1.csv>. El significado de cada una de las columnas podéis encontrarlo en <http://football-data.co.uk/notes.txt>. 

```{r}
# Antes de ejecutar el comando comprobad la ruta completa desde el directorio de trabajo
# El separador de columnas es "," y la primera línea del fichero indica el nombre de las variables 

fdata <- read.table(file="~/Formacion informatica/R/Proyectos/Master BD/3 Ciencia de datos con R/Datasets/SP1_1617_red.csv",
                        sep=",",header = TRUE)

# Primeras líneas
head(fdata)

# Para ver la tabla en Rstudio
View(fdata)

```

## Lectura de ficheros delimitados

En ocasiones es mejor usar  las funciones `read.delim` y `read.csv` que están basadas  en `read.table` pero con diferentes configuraciones por defecto. Consulta la ayuda de estas funciones para descubrir las diferencias. 

```{r,eval=FALSE}
?read.table
read.delim
read.csv
```

Es posible leer los ficheros directamente de Internet

```{r}
sp1  <- read.csv(file="http://football-data.co.uk/mmz4281/1516/SP1.csv")
head(sp1,1)
```


También podemos leer datos tabulares desde el portapapeles del sistema operativo. 
Si seleccionamos un texto en una hoja de cálculo, copiamos al  portapapeles y ejecutamos

```{r,eval=FALSE}
a <- read.table("clipboard", header=T, sep="\t")   
# o
a <- read.delim("clipboard", header=T, sep="\t")   
```

Existen comandos en R para la lectura de ficheros de texto a más bajo nivel como *readLines* o *scan* que no vamos a utilizar en el curso pero pueden ser útiles en algunas situaciones. 

Para la lectura de ficheros muy grandes, se recomienda el uso del paquete `readr`.

## Escritura de datos en un fichero

Para escribir el contenido de un data frame en un fichero delimitado usamos el comando `write.table`.

```{r}
#Por ~ se entiende el directorio de usuario del sistema operativo
write.table(fdata, file = "~/borrar.txt",sep=",")
# Comprobad el fichero que se ha creado en vuestro ordenador
```


## Datos incluidos en paquetes

Para el aprendizaje de R o para practicar con los diferentes modelos y análisis estadísticos, R trae de serie más de 100 conjuntos de datos en el paquete `datasets` y muchos más en otros paquetes.

Podemos ver un resumen de los datos contenidos en el paquete `dataset` o en cualquier otro paquete como `ggplot2` con el comando `data`

```{r}
data()
data(package='ggplot2')
```


## Más conjuntos de datos

Otros conjuntos de datos interesantes se pueden encontrar en los siguientes en enlaces

- Datos proporcionados por RStudio  
<https://blog.rstudio.org/2014/07/23/new-data-packages/>

- Kaggle <https://www.kaggle.com/datasets>

- Fun Data for teaching: <https://bartomeuslab.com/2016/01/21/fun-data-for-teaching-r/>



## Lectura de otras fuentes de datos

En R es posible importar datos de numerosos formatos y fuentes distintas. Algunos ejemplos son:

- Excel 
- Web scrapping
- Base de datos
- Google Docs
- APIS
   - Twitter
   - Geocoding

### Lectura de Excel

Es muy habitual encontrar tablas de datos en formato Excel. En R tenemos a disposición varios paquetes para leer datos desde Excel. 
Veamos un ejemplo usando el paquete `readxl`

```{r}
library(readxl)
# Leemos datos del precio del mercado ibérico mayorista de electricidad
precio_md <- read_excel("../Datasets/precio_md.xls")
# El paquete readxl almacena la tabla en un formato más avanzado de objeto llamado tbl_df.
# Lo transformamos a un data.frame "normal"
precio_mdf <-as.data.frame(precio_md)
head(precio_md)
```

También es  posible realizar esta importación de datos usando los menús de Rstudio. 
`File-> Import Dataset -> From Excel`

## Indexado en data frames

El sistema de indexado para data.frames, funciona por los mismos principios que vimos para vectores o matrices, veamos algunos ejemplos.

### Filas y columnas

Filas

```{r}
fdata[1:2,]
```
Filas y columnas

```{r}
fdata[1:3,2:3]
```

Nombres de columnas

```{r}
fdata[1:3,c("Date", "HomeTeam", "AwayTeam")]
```


### Valores Ausentes (NA's)

Un concepto importante cuando se trabaja con datos son los valores ausentes, que siempre existen en casi cualquier recopilación de información. R etiqueta los valores ausentes como NA (not available), y muchas de sus funciones hacen un tratamiento especial de este tipo de datos

Operar con NA siempre da NA 

```{r}
NA +1
```

A veces esto es un problema con el uso de funciones, pero R suele tener previsto que hacer con los NA 

```{r}
# Media de goles del equipo que juega en casa
mean(fdata$FTHG)
mean(fdata$FTHG,na.rm = TRUE)
```

Existe otro tipo especial de dato erróneo que es el NaN (not a number) que se utiliza para representar el resultado de operaciones matemáticas no definidas, como la raíz cuadrada de un número negativo

```{r}
sqrt(-1)
# Sin embargo R si que maneja el concepto de infinito
5/0
```


### Identificar NA's

En R es posible comprobar la existencia de valores ausentes mediante las funciones  `anyNA` e `is.na`

En el conjunto de datos de partidos de fútbol, la última fila tiene algunos valores NA ya que no se ha disputado el encuentro.

```{r}
# Detecta si hay algún NA
anyNA(fdata)
# Devuelve TRUE si el valor es NA y FALSE en caso contrario
tail(is.na(fdata$FTHG))
```

## Indexado condicional

El indexado condicional es muy interesante trabajando con data frames, ya que nos permite seleccionar, filtrar datos con múltiples condiciones y operar sobre estas selecciones.

```{r}
iris[iris$Sepal.Length <4.5,]
iris[iris$Species=="virginica" & iris$Petal.Length > 6.5,]
mean(iris$Petal.Width[iris$Species=="setosa"])
mean(iris$Petal.Width[iris$Species=="virginica"])
```

## Indexado  con NAs

Los NA dan problemas con el indexado de data frames

```{r}
selection <- fdata[fdata$FTHG>5 & 
                         fdata$FTAG<1,]
# Devuelve una fila con NA  
selection

# Es posible evitarlo con la condicion explícita de que no haya NA 
selection <- fdata[!is.na(fdata$FTHG) & fdata$FTHG>5 & 
                          !is.na(fdata$FTAG) & fdata$FTAG<1,]

selection
```
 
Usando la función `which` para indexar se puede evitar tener que hacer las comprobaciones explícitas de la existencia de NAs

```{r}
idx <- which(fdata$FTHG>5 & fdata$FTAG<1)
selection <- fdata[idx,]
selection
```


### Subset

Para facilitar la sintaxis del indexado podemos usar la función `subset`

```{r}
subset(iris, Species=="setosa" &  Sepal.Width > 4)
subset(fdata,FTHG>5 & FTAG<1)
# selecciono los datos de precio de España
precio_esp <- subset(precio_md, geoname == "España")
```

## Manipulación de datos

### Nombres de columnas

Para obtener los nombres de las columnas de un data frame
```{r}
names(precio_md)
```

Cambio de nombre de variables

```{r}
names(precio_md) <- c("id","serie", "geoid","pais","precio_eurMw","datetime")
head(precio_md,1)
```

### Transform

Con la función `transform` podemos modificar de forma sencilla valores de variables existentes o crear nuevas variables en un data frame

```{r}
# Transformacion de unidades en el precio
precio_md <- transform(precio_md, precio_eurKw = precio_eurMw/1000)
# Transformo el campo de tiempo para que tenga un formato de serie temporal apropiado en R 
#  Primero quito los : del indicador de la zona horaria
precio_md <- transform(precio_md, datetime=gsub("\\:00$","00",datetime))
# Convierto el datetime a un formato  del tipo fecha-hora
precio_md <- transform(precio_md,datetime=as.POSIXct(datetime,format="%Y-%m-%dT%H:%M:%S%z"))

head(precio_md)
```

### Categorizar datos

La función `cut` convierte una variable numérica en categórica según una definición de intervalos

```{r}
# Con los datos de precios
precio_md<- transform(precio_md, precio_cat= cut(precio_eurMw,c(0,45,60,75,90,Inf),include.lowest = TRUE))
#Conteo de ocurrencias de variables categóricas
table(precio_md$precio_cat)

```

### Re-etiquetado de variables

Podemos cambiar las etiquetas de los niveles de un factor

```{r}
catvar=precio_md$precio_cat
levels(catvar)
levels(catvar) = c("Muy bajo","Bajo","Medio","Alto","Muy alto")
head(catvar)
head(precio_md$precio_cat)
```


Otra forma de reetiquetar las variables es usando una "lookup table" que consiste en un vector de texto cuyos elementos tiene nombre

```{r}
lut=c("[0,45]"="Muy bajo","(45,60]"="Bajo","60,75]"="Medio","(75,90]"="Alto","(90,Inf]"="Muy alto")
catvar= precio_md$precio_cat
catvar = lut[catvar]
head(catvar)
class(catvar)
levels(factor(catvar))
```

## Cambio de formato de datos

### Datos *wide* y datos *long*

Existen dos formas básicas de organizar los datos:

- datos **wide**: todas las variables se encuentran en diferentes columnas
- datos **long**: los datos tienen unas variables para su identificación y las variables relevantes se almacenan por nombre y valor

Veamos un ejemplo para aclarar el concepto

```{r}
wide=data.frame(id=1:4,var1=c(2,4.2,5.1,6),var2=c(5,4,5.5,6.3)) 
wide
long = data.frame(id=rep(1:4,2),variable=rep(c("var1","var2"), each = 4),
        valor=c(2,4.2,5.1,6,5,4,5.5,6.3))
long
```

### Paquete reshape2

El paquete reshape2 nos permite transformar un conjunto de datos en formato *wide* a formato *long* y viceversa.

```{r}
if(!require(reshape2))
  install.packages('reshape2')
library(reshape2)
```

### Melt

La función `melt` convierte de wide a long

```{r}
melt(wide, id.vars= c("id"),measure.vars=c("var1","var2"),
     variable.name="variable",value.name = "valor")
```

### dcast

La función `dcast` convierte de long a wide

```{r}
dcast(long, id ~ variable , value.var = "valor")
```

###  Agregación de datos con dcast

Si la descomposición de dcast no es única se usan funciones de agregación

```{r}
head(airquality)
aqm <- melt(airquality, id=c("Month", "Day"), na.rm=TRUE)
dcast(aqm, Month ~ variable, fun.aggregate = mean, value.var = "value",
      margins = c("Month", "variable"))
```

## Agregación de datos

Aunque en la próxima sección veremos el paquete dplyr que proporciona un método muy sencillo y potente de realizar agregaciones de datos, veamos primero como hacerlo con las funciones del paquete base.

### apply

Ya vimos que la función `apply` realiza operaciones por filas o columnas de una matriz. Podemos aplicarla a un data frame


```{r}
# media de goles del local y el visitante
apply(fdata[5:6],2,mean,na.rm=TRUE)
```

### aggregate

La función aggregate es la apropiada para hacer agregaciones de variables condicionadas al valor de otras variables. Para especificar las variables que queremos usar para la agregación se una la sintaxis de fórmulas, que se usa en otras funciones de R como los modelos estadísticos (por ejemplo `lm`).

```{r}
# Media de cuotas de apuestas a ganador local en función del resultado real 
# (H: ganador local;D: Empate;A: Ganador visitante)
aggregate(B365H ~ FTR, data=fdata, FUN = mean)
# # Mediana de cuotas de apuestas a ganador local en función la diferencia de goles
aggregate(B365H ~ dif_goles, data=transform(fdata,dif_goles=FTHG - FTAG),FUN = median)
```

La fórmula "B365H ~ FTR" indica que queremos aplicar la función FUN a B365H para los diferentes valores que tome FTR. Los datos en el lado derecho de "~" deben ser de tipo discreto.  

Es posible agregar en función de varias variables:

```{r}
tmp <- aggregate(B365H ~ FTR + HomeTeam  , 
                 data=fdata,FUN = mean)
head(tmp)


```

## Unión de data frames

### merge

Con merge podemos juntar tablas de datos a partir de valores coincidentes. Es el equivalente al JOIN en bases de datos SQL.

Como ejemplo añadamos a los datos de precios, valores de demanda eléctrica

```{r}

demanda <-  read.csv("../Datasets/demanda_ene2017.csv",sep=";")
demanda <- transform(demanda,datetime=as.POSIXct(datetime))
names(demanda)[2] <- "demanda"
precio_esp <- subset(precio_md,pais=="España")
demanda <- merge(demanda,precio_esp,by="datetime")
head(demanda)

```

Y podemos ver que los precios y la demanda están correlacionados

```{r}
aggregate(demanda ~ precio_cat,data=demanda,FUN=mean)
```

# Análisis de datos avanzado con dplyr

Ya hemos visto en las secciones anteriores algunas de las potencialidades de  R para el trabajo con tablas de datos. 
En esta sección vamos a aprender el uso del paquete `dplyr` que nos permitirá realizar complicados análisis de datos con una sintaxis muy sencilla y con gran eficiencia en tiempo de cálculo. 

## Qué es dplyr

El paquete *dplyr* tiene tres objetivos principales:

 - Identifica las operaciones de manipulación de datos más importantes y convierte su uso en algo sencillo desde R. Es intuitivo y fácil de leer, especialmente por uso de sintaxis "encadenada"
 - Es rápido, realizando las tareas en memoria, con las piezas claves de código escritas en C++.
 - Utiliza la misma interfaz para trabajar sin importar si los datos están almacenados en un data data frame, un data table o una base de datos.

`dplyr` implementa de forma eficiente  en R la estrategia  *split-apply-combine*, que consiste en dividir un conjunto grande de datos, aplicar una función a cada trozo y combinar todos los resultados. 


## Funcionalidad

A partir de 5 verbos básicos podemos hacer las manipulaciones más habituales que se hacen en un conjunto de datos: 

  - `filter`
  - `select`
  - `arrange`
  - `mutate`
  - summarise
  - Habitualmente con group_by

Podeis encontrar más información sobre el paquete en: 

- <https://github.com/hadley/dplyr>
- <https://github.com/hadley/dplyr/blob/master/vignettes/>
- <http://genomicsclass.github.io/book/pages/dplyr_tutorial.html>


## Datos: IMDB

Vamos a trabajar con un conjunto de datos de 5000 películas evaluadas en la Internet Movie Data Base (IMDB)

```{r}
imdb <- read.csv("../Datasets/imdb_movie_reduced.csv")

# Trunco los títulos para una mejor legibilidad

imdb$movie_title=substr(imdb$movie_title,1,30)
# output
head(imdb)

# Para ver la tabla completa en Rstudio 
# View(imdb)
```

## Tibble

Un tibble es un tipo más general de data frame que utiliza el paquete dplyr. Las columnas de un tibble pueden ser vectores atómicos como  en un data.frame pero también pueden ser listas que contengan objetos más complicados como modelos, matrices u otros  data.frames 

```{r, message=FALSE, warning=FALSE}
# install.packages(dplyr)
# Te recomendamos instalar el paquete tidyverse que  incluye dplyr, ggplot2 y otros muchos muy utiles que comparten una misma filososfia
library(dplyr)
imdb_tbl <- tbl_df(imdb)
imdb_tbl
```

## Verbos principales

Veamos la funcionalidad de los verbos principales del paquete. 

### filter

La función filter permite seleccionar filas mediante condiciones lógicas. Es equivalente a subset, aunque en filter las condiciones `&` se pueden separar con coma. 

```{r}
# Equivalente a subset(imdb,duration>150 & title_year==2015)
filter(imdb,duration>150,title_year==2015)
```

Condiciones más complejas

```{r}
tmp <- filter(imdb,duration>180 & 
                (title_year==2015 | title_year==2011))
tmp
```

### select

Seleccionar columnas por nombre, como el *select* de SQL

```{r}
tmp <- select(imdb, movie_title, imdb_score) 
head(tmp)
```


o de forma más genérica mediante la función matches

```{r}
# Selecciona las columnas que incluyan en el nombre la cadena "likes"
tmp <- select(imdb, matches("likes"))
head(tmp)
```


### Encadenamiento (Pipelining)

La forma habitual de encadenar operaciones con funciones es mediante el anidado. Sin embargo, en `dplyr` podemos encadenar operaciones con el operador `%>%`, que podemos pronunciarlo como "entonces" (then).
En Rstudio podemos escribir el operador `%>%` tecleando Ctrl+Shift+M

Veamos un ejemplo

```{r}
# Forma anidada
select(filter(imdb,duration>180 & language !="English"),
        movie_title, language,director_name,duration)
# Encadenamiento -- Mucho más legible
imdb %>% filter(duration>180 & language !="English") %>%
    select(movie_title, language,director_name,duration)
```

### Arrange

Cambia el orden de las filas. En el ejemplo ordena según la puntuación de la película (imdb_score)

```{r}
imdb %>% filter(duration>180 & language !="English") %>%
    select(movie_title, language,director_name, imdb_score) %>% 
      arrange(-imdb_score)

# El - es para ordenar de mayor a menor, también puede usarse desc() 

```


### mutate

Añadir nuevas variables o cambiar las ya existentes

```{r}
# Nota. Los select los hacemos unicamente por mejorar la legibilidad de los resultados
# Añadimos nuevas variables
imdb %>% select(movie_title,title_year,duration,imdb_score) %>% 
              mutate(duration_hours=duration/60,
                     stars=cut(imdb_score,c(0,2,4,6,8,10),labels = c("*","**","***","****","*****"))) %>% 
                head(4)
# Cambiamos una columna existente
imdb %>% select(movie_title,title_year,imdb_score) %>% 
              mutate(imdb_score=imdb_score/2) %>%
  head()

```

### summarise

- Convierte las variables a un valor único (media, mediana, suma, etc.)
- Es útil normalmente agrupado con group_by

Veamos algunos ejemplos

```{r}
imdb %>%  
  summarise(dur.m = mean(duration,na.rm=TRUE), imdb_score.m=mean(imdb_score))

imdb %>% group_by(language) %>% 
  summarise(dur.m = mean(duration,na.rm=TRUE), imdb_score.m=mean(imdb_score))

imdb %>% group_by(title_year) %>% 
  summarise(dur.m = mean(duration,na.rm=TRUE), imdb_score.m=mean(imdb_score)) %>% 
  arrange(-imdb_score.m)
# Ojo arrange actúa sobre el resultado de summary
```


En summarise puede utilizarse cualquier función que devuelva un valor único, sea cual sea su origen: función de base, contenida en un paquete o definida por el usuario. 

El paquete dplyr incorpora algunas funciones propias de gran utilidad como 

- `n()` cuenta el número de filas
- `n_distinct(vector)` cuenta el número de elementos únicos en el vector

```{r}
imdb %>% group_by(language) %>% 
  summarise(num_films=n(),
            num_directors= n_distinct(director_name)) %>% 
  arrange(-num_films)
```

Veamos un ejemplo con una función definida por el usuario

```{r}
desviacion_tipica_media <-function(x){
  sd(x,na.rm=TRUE)/sqrt(length(x))
}

imdb %>% group_by(title_year) %>% summarise(num_films= n(), mean_score=mean(imdb_score), mean_dev= desviacion_tipica_media(imdb_score)) %>% 
  arrange(-title_year)
```


### summarise_each 

`summarise_each` permite aplicar la misma función  a distintas columnas a la vez

```{r}
imdb %>% group_by(title_year) %>% 
  summarise_each(funs(mean), imdb_score, budget,gross)

```


Se puede aplicar más de una función en `summarisse_each`

```{r}

imdb %>% group_by(language) %>% 
  summarise_each(funs(mean,min,max), imdb_score)

```

## Agrupación

### group_by

Las operaciones de agrupación  pueden realizarse también con otras funciones distintas a summarise. 

Por ejemplo `group_by` con `mutate` y `filter` son útiles en conjunción con  las llamadas "window functions" (como  `rank`, o `min(x) == x`). 

Veamos ejemplos. Para cada año cojo las 3 películas mejor puntuadas

```{r}
imdb %>% filter(title_year>2013) %>% group_by(title_year) %>% 
  mutate(ranks=rank(-imdb_score,ties="first")) %>% 
    filter(ranks<=3) %>%
      select(title_year,movie_title,imdb_score,ranks) %>% 
      arrange(title_year,ranks)

```

Ahora,  para cada año cojo la mejor valorada incluyendo los empates

```{r}
imdb %>% filter(title_year>2013) %>% group_by(title_year) %>% 
    filter(imdb_score==max(imdb_score)) %>%
      select(title_year,movie_title,imdb_score) %>% 
      arrange(title_year)
```

Por último escalo la puntuación para que dentro de cada año las puntuaciones vayan de 0 a 1, siendo 0 el mínimo y 1 el máximo


```{r}
tmp<- imdb %>% filter(title_year>2013) %>% group_by(title_year) %>% 
    mutate(norm_score=(imdb_score-min(imdb_score))/(max(imdb_score) - min(imdb_score)))%>% 
   dplyr::select(title_year,movie_title,imdb_score,norm_score)

tmp

# Veamos con un summary que el código anterior hace lo que queriamos
# No hace falta hacer el group_by(title_year) porque tmp ya es una tabla agrupada

tmp %>% summarise(m=mean(imdb_score),mn=mean(norm_score),minn=min(norm_score),maxn=max(norm_score))

```

### Otras funciones útiles con group_by

Es útil usar funciones de muestreo junto con group_by para tomar muestras que se repartan en los diferentes grupos. 

- `sample_n(n)`: Muestrea n filas de data frame.

- `sample_frac(frac)`: Muestrea una fracción frac del data frame.

Mostremos 3 películas aleatorias en cada idioma

```{r}
imdb %>% filter(language %in% c("English","Spanish","French","German") ) %>% 
  group_by(language) %>% sample_n(3,replace=FALSE) %>% dplyr::select(language,movie_title,title_year)
```


Ahora mostremos un  1% aleatorio de las películas de cada año. Redondea las fracciones al entero más pŕoximo

```{r}
imdb %>% filter(title_year>2013) %>% 
  group_by(title_year) %>% sample_frac(0.01) %>% select(title_year,movie_title)

# Comprobemos que selecciona el número correcto cada año
imdb %>% filter(title_year>2013) %>% 
  group_by(title_year) %>% summarise(num=n(),perc1=round(0.01*num))
```

## Funciones de ventana

Profundicemos un poco más en las funciones de ventana (o window functions)

- Las funciones de agregación (como mean) toman n inputs y devuelven  1 valor
- Una función de ventana toma n inputs y devuelve  n valores, como 
    - Funciones de ordenado o de ranking (como `min_rank`) 
    - Funciones de "Offset"(`lead` y `lag`), o agregados acumulativos (como `cummean` o `cumsum`)

Veamos más ejemplos de funciones de ventana

Presupesto (budget) acumulado por año ordenando de menor a mayor presupuesto 

```{r}
imdb %>% filter(title_year>2014) %>% group_by(title_year) %>% 
  arrange(budget) %>% 
  mutate(cum_budget=cumsum(budget)) %>%
  dplyr::select(title_year,movie_title,budget,cum_budget) %>%
  arrange(title_year,-cum_budget)
```


Media de "Likes"" en facebook de las películas por año y diferencia con el año anterior

```{r}
imdb %>% filter(title_year>=2010) %>% group_by(title_year) %>%
  summarise(num=n(),likes=mean(movie_facebook_likes)) %>%
  arrange(title_year) %>%
  mutate(change_likes=likes-lag(likes,1))

```


## Do


Con la función `do` podemos aplicar cualquier función a los datos sin tener que limitarnos al mutate, summarise, etc.

Además podemos combinar el `do` con el resto de verbos del paquete dplyr: filter, mutate, group_by, etc. La única condición es que `do` debe devolver un data frame.

Dentro del código en el interior del `do`, los valores del data frame sobre el que actúan se referencian mediante "."

Ilustremos su uso mediante un ejemplo. Vamos a ver cuales son los géneros mejor puntuados. Los géneros tienen un número variable para cada película. Por ejemplo Star Wars tiene "Action|Adventure|Fantasy|Sci-Fi" mientras que Titanic tiene "Drama|Romance"


```{r,cache=TRUE,warning=FALSE}
# Multiplico cada fila (cada película) por el número de géneros que posea
tmp<- imdb %>%    
  group_by(title_year,movie_title) %>% do({
  # tomo el vector 
  generos = as.character(.$genres)
  # | es un caracter especial y debo "escaparlo con \\"
  spl = strsplit(generos,split="\\|")
  #strsplit devuelve una lista. Como deberia tener un único elemento, lo convierto a vector
  spl=spl[[1]]
  aux=.
  merge(aux,data.frame(genre=spl),by=NULL)
})

# Veamos el resultado

tmp %>% select(title_year,movie_title,genres,genre) %>% head(10)

## Calculemos las puntuaciones medias por género

tmp %>% group_by(genre) %>% summarise(num=n(),imdb_score=mean(imdb_score)) %>% arrange(-imdb_score)

```


# Gráficos en R

Una de las mayores virtudes de R es su potencia para realizar gráficos y visualizaciones de gran calidad, tanto desde el paquete base como desde paquetes específicos como `lattice` o `ggplot2`. 

El objetivo de esta sección no es hacer una explicación exhaustiva del sistema gráfico de R, sino a partir un conjunto muy amplio de ejemplos explorar una gran parte de su potencial y aprender de forma práctica el uso de los comandos principales. 

Para la documentación precisa, te aconsejamos que consultes la ayuda en línea de R y consultes las referencias que te recomendamos.

Los ejemplos los vamos a construir a partir de un conjunto de datos que contiene datos de demanda eléctrica media diaria en España, junto con algunas variables meteorológicas. 

```{r}
demanda = read.csv("../Datasets/demanda_diaria.csv",sep=";",
                     colClasses = c("Date",rep(NA,6)))

#retoques
demanda$wd = factor(weekdays(demanda$fecha),levels=c("lunes","martes","miércoles","jueves","viernes","sábado","domingo" ))
demanda$mes = factor(format(demanda$fecha,"%Y-%m"))
demanda$lluvia = factor(demanda$lluvia,levels = c("Baja","Media","Alta"))

head(demanda)
```

La descripción de las columnas es la siguiente: 

- *Fecha*
- *temperatura_mean:* temperatura media diaria en España. Promedio de 8 estaciones meteorológicas
- *temperatura_min*: temperatura mínima diaria
- *temperatura_max*: temperatura máxima diaria
- *radiation_mean*: radiación solar media recibida
- *demanda_mwh*: demanda eléctrica Media (Mw)
- *lluvia*: variable categórica indicando la cantidad de lluvia diaria caída en España: Baja, Media, Alta 
- *wd*: día de la semana
- *mes*: mes del año

## Gráficos base

Para una introducción más detallada al sistema de gráficos base te remitimos a la documentación oficial que encontrarás en  <https://cran.r-project.org/doc/manuals/r-release/R-intro.html#Graphics>

Puedes hacerte una primera idea de la variedad de gráficos que permite escribiendo el comando 
`demo(graphics)` en la consola de R.

### Función  plot

`plot` es la función básica del sistema gráfico. Se trata de una función genérica que realiza diferentes gráficos en función de como son los datos de entrada. 

Admite como entrada gran variedad de datos diferentes: vectores, modelos lineales, densidades de frecuencia estimadas, etc. Veamos mediante ejemplos los diferentes tipos de gráficos que realiza en función de la naturaleza de los datos de entrada

### 1 variable numérica

El argumento es un vector numérico

```{r}
plot(demanda$temperatura_mean)
```

### 1 variable categórica

El argumento es un vector categórico (factor). Muestra un diagrama de frecuencias

```{r}
plot(demanda$lluvia)
```

### 2 variables numéricas

Los  argumentos son dos vectores numéricos. Muestra un gráfico de dispersión

```{r}
plot(demanda$temperatura_mean,demanda$demanda)
```

## 2 variables categóricas

Los  argumentos son dos vectores categóricos. Muestra una tabla de contingencias o diagrama de frecuencia bidimensional

```{r}
demanda$demcat <- cut(demanda$demanda,c(20000,25000,27500,30000,35000),labels = c("Baja","Media","Alta","Muy Alta"))
plot(demanda$mes,demanda$demcat)
#plot(demanda$mes,demanda$demanda)

```


### Histogramas

Para dibujar un histograma en R

```{r}
hist(demanda$demanda)
```

Un histograma es el equivalente a una tabla de frecuencias para una variable continua. Primero se divide el rango de valores de la variable en intervalos (bins), normalmente de anchura fija, y cuenta el número de ocurrencias de la variable en cada intervalo

Podemos especificar de forma explícita el número de intervalos a usar. En caso contrario R elige automáticamente la división en intervalos. 

```{r}
hist(demanda$demanda_mwh,breaks = 50)
```

### Boxplot

Un gráfico estadístico que proporciona mucha información sobre la relación entre una variable continua y otra categórica es el boxplot o diagrama de cajas

```{r}
boxplot(demanda$demanda_mwh ~ demanda$wd)
```

Dibuja, para una variable continua, un resumen de su distribución condicionada a una variable categórica.

Muestra para cada valor de la variable categórica:

- Mediana: línea central
- Caja
    - Cuantil 0.25
    - Cuantil 0.75
- Mínimo (excepto "outliers")
- Máximo (excepto "outliers")
- Outliers (excepcionalidades): 
    - si $q_{0.25}-x > 1.5(q_{0.75} - q_{0.25})$
    - o  $x- q_{0.75} > 1.5(q_{0.75} - q_{0.25})$

Si no conoces o no  te acuerdas de estos conceptos estadísticos, no te preocupes que los definiremos de forma precisa  en la unidad 3 de este curso. 

### Pairs

Dibuja un gráfico de correlación entre múltiples variables numéricas.

```{r,cache=TRUE,fig.height=5}
pairs(demanda[c("demanda_mwh","temperatura_mean","radiation_mean","temperatura_min","temperatura_max")])
```

### Añadir elementos a un gráfico

Una vez realizado el gráfico, es posible añadir diferentes elementos al gráfico, tal y como muestra el siguiente ejemplo


```{r,fig.height=3.7}
plot(demanda$radiation_mean,demanda$temperatura_mean,
     ylim=c(-5,40),
     ylab="Temperatura Media diaria",xlab="Radiación media diaria")
# Ajuste lineal de los datos
lmod=lm(temperatura_mean~radiation_mean,data=demanda)
# Recta de ajuste y=a + b*x
a=lmod$coefficients[1]; 
b=lmod$coefficients[2]
abline(a,b,col=2)
# Otros puntos
points(demanda$radiation_mean,demanda$temperatura_max,col=4,cex=0.5)
# Anotaciones textuales
text(150,32,"T máxima",col=4)
# Leyenda
legend("topleft", c("T mean","Tmax","fit"), col = c(1,4,2),
       text.col = "grey20", lty = c(-1,-1,1), pch = c(1,1,NA),cex=0.8)
```


## Personalización de gráficos base

Veamos dos versiones de un mismo gráfico: 

La básica 

```{r}
plot(demanda$temperatura_mean,demanda$demanda_mwh)
```

y la retocada, modificando algunos  parámetros 

- colores (col)
- tamaño de puntos (cex)
- título (main) 
- etiquetas de ejes (xlab, ylab)

```{r,fig.height=4.5}
plot(demanda$temperatura_mean,demanda$demanda_mwh, col=demanda$wd, 
     cex=0.5,
     xlab="Demanda eléctrica media diaria en España (Mwh)",
     ylab="Temperatura Media diaria",
     main="Dependencia Demanda eléctrica y temperatura"
     )
```

Veamos un ejemplo que modifica aún más parámetros gráficos

```{r,echo=TRUE}
x=seq(0,20,2)
condensador=data.frame(t=x,V=10*(1-exp(-x/5)))
plot(condensador$t,condensador$V, 
     col="orange", # color
     cex=1, # tamaño del punto
     pch=4, # símbolo del punto
     lty = 2, # tipo del línea
     type="o", # tipo de gráfico: puntos(p), lineas(l), puntos y lineas(o) 
     ylab="Diferencia de Potencial (V)", #titulo eje y
     xlab="tiempo (s)", #titulo eje x
     main="Proceso de carga de un condensador", # título principal
     col.main = "darkgray", # color título
     cex.axis = 0.6 # Tamaño de etiquetas ejes
)
```


Los posibles **símbolos** para representar puntos son: 

```{r,fig.height=4.5}
tmp <- cbind(expand.grid(x=1:4,y=1:4),s=1:16)
plot(tmp$x,tmp$y,pch=tmp$s,xlim=c(0,5),ylim=c(0,5))
```


Y los diferentes tipos de  línea

```{r,fig.height=4}
plot(x=0:1,rep(1,2),type="l",lty=1,ylim=c(0,7),xlab="",ylab="lty")
for(i in 2:6){
  lines(0:1,rep(i,2),lty=i)  
}
```

### Parámetros gráficos

Los parámetros gráficos se pueden cambiar de forma permanente con la función `par`.
Una llamada a `par()` sin argumentos muestra los parámetros definidos por defecto

```{r, eval=FALSE}
par()
```

```{r, echo=FALSE}
par() %>% head()
```


Puede cambiarse el valor de los parámetros pasando argumentos a la función `par` 

```{r,warning=FALSE,fig.height=4.5}
oldpar <- par()
par(col="blue",pch=5)
plot(rnorm(10),runif(10))
```

y también con ella podemos restaurar los parámetros originales

```{r,warning=FALSE,fig.height=4.5}
par(oldpar)
plot(rnorm(10),runif(10))
```

### Gráficos múltiples

Un parámetro muy interesante que nos permite realizar gráficos múltiples en un mismo panel es 
`mfrow`

```{r,echo=FALSE,warning=FALSE}
oldpar<- par(mfrow=c(2,2))
plot(demanda$fecha,demanda$demanda_mwh,type="l")
plot(demanda$temperatura_max,demanda$demanda_mwh)
hist(demanda$demanda_mwh,main="")
boxplot(demanda$demanda_mwh ~ demanda$lluvia)
par(oldpar)
```


## Exportar un gráfico

Por supuesto, es posible exportar los gráficos realizados en R a un fichero. Veamos dos ejemplos

En png

```{r,eval=FALSE}
png("figura_ejemplo.png")
boxplot(demanda$demanda_mwh ~ demanda$mes)
dev.off()
```

o en pdf

```{r,eval=FALSE}
pdf("figura_ejemplo.pdf")
boxplot(demanda$demanda_mwh ~ demanda$mes)
dev.off()
```


# Gráficos con ggplot2

El paquete ggplot2, creado por Hadley Wickham, es una alternativa muy popular a los gráficos base. 
Es una implementación de las ideas del libro **The Grammar of Graphics** (Leland Wilkison), cuyo objetivo es establecer una serie de principios comunes para la visualización de datos.

La filosofía de aprendizaje que emplearemos es la misma que hemos usado con los gráficos base: ir descubriendo la filosofía y la sintaxis del paquete a partir de ejemplos. Para una referencia más rigurosa te remitimos a las siguientes referencias:

- <http://docs.ggplot2.org/current/>
- <http://ggplot2.org/>
- Libro: ["ggplot2: Elegant Graphics for Data Analysis"](http://amzn.com/0387981403?tag=ggplot2-20)


## Instalación

Lo primero de todo es instalar el paquete si no lo tenemos todavía instalado. 

```{r,eval=FALSE}
install.packages("ggplot2")
```

Después cargamos el paquete

```{r}
library(ggplot2)
```

El paquete `ggplot2` dispone de una función genérica análoga al `plot` de la distribución base 
llamada `qplot`

```{r, message=FALSE,warning=FALSE,fig.height=5}
qplot(temperatura_mean, demanda_mwh, data = demanda, color = wd )
```

Sin embargo no la vamos a usar, sino que vamos a aprender a construir los gráficos de ggplot desde la base. 

## Conceptos básicos 

Siempre se empieza con una llamada a `ggplot`

```{r}
p <- ggplot(demanda) 
```
o
```{r}
p <- ggplot(demanda,aes(temperatura_mean, demanda_mwh)) 
```


- El resultado es un objeto de la clase ggplot  
- De momento no hay plot (gráfico), por dos razones
      - El gráfico no se dibuja hasta que se imprime el objeto `p`
      - El gráfico contiene 0 capas(layers)


## Capas (layers) y estéticas (aesthetics)

- Se añaden cosas al gráfico mediante el operador +
- Se puede hacer de forma repetida añadiendo múltiples capas al gráfico
- La función `aes` ("aesthetics") se usa para hacer corresponder las variables de los datos a atributos gráficos. 
- El primer argumento, o los dos primeros, de `aes` especifican las variables que vamos a representar, según sea la dimensión del gráfico: 1d (un histograma por ejemplo) o 2d (un gráfico de dispersión).
- Los siguientes argumentos son pares nombre/valor donde se especifican  otros atributos como la variable que controla el color de los puntos o la forma de los puntos, etc.

Veamos un ejemplo

```{r,message=FALSE,warning=FALSE,fig.height=4.5}
p <- ggplot(demanda,aes(temperatura_mean, demanda_mwh)) +
  geom_point() + 
  geom_smooth()
p
```

- Mediante `ggplot(demanda,aes(temperatura_mean, demanda_mwh))` decimos que vamos a usar los datos contenidos en el data frame demanda. Como variable `x` usaremos la columna `temperatura_mean` y como variable `y` la columna `demanda_mwh`.

- Con `geom_point`: Insertamos una capa (layer) con un gráfico de dispersión de la variable `x` (*temperatura_mean*) frente la variable `y` (*demanda_mwh*).

- Con `geom_smooth`: Insertamos una capa (layer) con un gráfico que muestra el ajuste a una curva suave de la variable  `y` (*demanda_mwh*) frente a la variable `x` (*temperatura_mean*).

### Estéticas

Se pueden establecer atributos a varios niveles:

- Podemos establecer atributos para todo el gráfico llamando a `aes` dentro de `ggplot()`
- O podemos establecer los atributos específicos para cada una de las capas

Veamos ejemplos:


```{r,message=FALSE,warning=FALSE,fig.height=4.5}
p <- ggplot(demanda,aes(temperatura_mean, demanda_mwh)) +
  geom_point(aes(color=wd)) + 
  geom_smooth()
p
```


```{r,message=FALSE,warning=FALSE,fig.height=4.5}
p <- ggplot(demanda,aes(temperatura_mean, demanda_mwh)) +
  geom_point(aes(color=wd),size=1,alpha=0.5) + 
  geom_smooth(aes(color=wd),se=FALSE)
p
```


Los aesthetics pueden variar en  cada capa individual 

```{r,message=FALSE,warning=FALSE,fig.height=4.5}
ggplot(demanda) +
  geom_line(aes(fecha,temperatura_min),color="blue") + 
  geom_line(aes(fecha,temperatura_mean),color="grey") +
  geom_line(aes(fecha,temperatura_max),color="red")   
```

### Capas

Hay varios tipos de objetos que se pueden añadir a los gráficos:

- **geoms** (“geometrics”): los gráficos que queremos mostrar, puntos, líneas, barras, polígonos, texto, etc
- **position**: ajustes de posición. Por ejemplo en un gráfico de barras controla donde deben situarse las barras, si una al lado de otra o apiladas. 
- **facets**: pueden dibujarse varios paneles en un mismo gráfico que dividen los datos según el valor de ciertas variables categóricas
- **scales**: controlan las características de escala de las variables. Por ejemplo una escala logarítmica para una variable continua o para escoger la paleta de colores para una variable categórica. 
- **themes**: ¿No te gusta el fondo gris? ¿Quieres personalizar el etiquetado? Pueden definirse estas características de forma individual o usar temas predefinidos.


## Ejemplos de gráficos 

Veamos ahora una galería de los diferentes tipos de gráficos que podemos elaborar combinando diferentes capas. 

**Autotexto video**

Para mayores explicaciones de los diferentes gráficos tenéis a disposición un vídeo en campus donde se explican uno a uno. 

**Fin  Autotexto video**


### Gráfico de puntos

```{r,message=FALSE,warning=FALSE,fig.height=4.5}
ggplot(demanda) + 
  geom_point(aes(radiation_mean,demanda_mwh, color=wd)) 
```


### Gráfico de puntos y líneas

```{r,message=FALSE,warning=FALSE,fig.height=4.5}
library(dplyr)
ggplot(demanda %>% filter(mes=="2014-01")) + 
  geom_point(aes(fecha,demanda_mwh)) + 
  geom_line(aes(fecha,demanda_mwh))
```

### Ajuste suave

```{r,message=FALSE,warning=FALSE,fig.height=4.5}
ggplot(demanda %>% filter(mes=="2014-01"),aes(fecha,demanda_mwh)) + 
  geom_point() + 
  geom_line() + 
  geom_smooth()
```

### Ajuste lineal

```{r,message=FALSE,warning=FALSE,fig.height=4.5}
ggplot(demanda %>% filter(mes=="2014-01"),aes(fecha,demanda_mwh)) + 
  geom_point() + 
  geom_line() + 
  geom_smooth(method="lm")
```

### Histogramas - stack

```{r,message=FALSE,warning=FALSE,fig.height=4.5}
# Ojo: Cambio los datos de wide a long para el plot
library(reshape2)
tmp <- melt(demanda,id.vars ="fecha", 
            measure.vars =c("temperatura_min","temperatura_max"))
ggplot(tmp) + geom_histogram(aes(value,fill=variable))
```

### Histogramas - dodge

```{r,message=FALSE,warning=FALSE,fig.height=4.5}

ggplot(tmp) + geom_histogram(aes(value,fill=variable),
                             binwidth = 5, position = "dodge")
```

### Histogramas - identity

```{r,message=FALSE,warning=FALSE,fig.height=4.5}

ggplot(tmp) + geom_histogram(aes(value,fill=variable), alpha=0.5,
                             binwidth = 5, position = "identity")
```


### Histogramas - facets

Una funcionalidad muy interesante en ggplot son los facets. Es posible dividir un gŕafico en varios paneles en función de los valores de los datos. Para ello usamos las funciones `facet_wrap`
y 'facet_grid`

```{r,message=FALSE,warning=FALSE,fig.height=4.5}
ggplot(tmp) + geom_histogram(aes(value,fill=variable),binwidth = 2) + 
  facet_wrap(~variable)
```

### Histogramas - facet grid

```{r,message=FALSE,warning=FALSE,fig.height=4.5}
ggplot(demanda) + geom_histogram(aes(demanda_mwh),binwidth = 1000) + 
  facet_grid(lluvia~wd)
```


### Histogramas - density

```{r,message=FALSE,warning=FALSE,fig.height=4.5}
tmp <- melt(demanda,id.vars ="fecha", 
            measure.vars =c("temperatura_min","temperatura_max"))
ggplot(tmp) + geom_density(aes(value,color=variable))
```

### Gráficos de barras

Primero agrupamos datos

```{r}
tmp <- demanda %>% group_by(mes,wd) %>% summarise(demanda=sum(demanda_mwh))
```

### Gráficos de barras - dodge

```{r,message=FALSE,warning=FALSE,fig.height=4.5}
ggplot(tmp, aes(mes,demanda,fill=wd)) + 
  geom_bar(position="dodge", stat="identity") + 
  theme(axis.text.x = element_text(angle=60))
```

### Gráficos de barras - stack

```{r,message=FALSE,warning=FALSE,fig.height=4}
ggplot(tmp, aes(mes,demanda,fill=wd)) + 
  geom_bar(position="stack", stat="identity") + 
  theme(axis.text.x = element_text(angle=60))
```

### Gráfico de area

```{r,message=FALSE,warning=FALSE,fig.height=4}
# La variable x debe ser una variable continua
# Debo convertir la variable mes a fecha 

ggplot(tmp,aes(as.Date(paste(mes,"-01",sep="")),demanda,fill=wd)) + 
  geom_area() + xlab("mes")
```

### Boxplot

```{r,message=FALSE,warning=FALSE,fig.height=5}
ggplot(demanda,aes(wd,demanda_mwh)) +
    geom_boxplot()
```

### Boxplot y puntos "jitter"

```{r,message=FALSE,warning=FALSE,fig.height=5}

ggplot(demanda,aes(wd,demanda_mwh)) + geom_boxplot() + 
  geom_point(position="jitter",size=1,color="blue")  
    
```

### Boxplot variable continua

```{r,message=FALSE,warning=FALSE,fig.height=5}

ggplot(demanda) +
  geom_boxplot(aes(temperatura_mean,demanda_mwh, group=cut_width(temperatura_mean,width=5))) + 
    geom_point(aes(temperatura_mean,demanda_mwh),size=1,alpha=0.7,color="orange") 
    
```

### Cambio de datos 

Vemos una forma sencilla de cambiar los datos sobre los cuales se realiza el gráfico

```{r,message=FALSE,warning=FALSE,fig.height=5}
p<- ggplot(demanda) + 
  geom_point(aes(temperatura_max,demanda_mwh,color=radiation_mean))
p
```


Restrinjo los datos al fin de semana

```{r,message=FALSE,warning=FALSE,fig.height=4.5}
p %+% subset(demanda,wd %in% c("sábado","domingo"))
```


## Personalización de gráficos

Veamos por último algunas formas de personalizar nuestros gráficos de ggplot. En la unidad 4 del curso profundizaremos más en este aspecto. 

### Etiquetas de ejes

```{r,message=FALSE,warning=FALSE,fig.height=4}
  p <- ggplot(demanda) + 
  geom_point(aes(temperatura_max,demanda_mwh,color=radiation_mean)) +
  xlab("Wind Speed M1") + ylab("Wind Speed M2")  
  p
```


### Escalas de ejes

```{r,message=FALSE,warning=FALSE,fig.height=4.5}
  p +
  scale_y_continuous(breaks=seq(20000,35000,1000))
```


### Escalas de color

```{r,message=FALSE,warning=FALSE,fig.height=5}
  p + scale_color_continuous("Temperatura", low="yellow",high = "red")
```


### Parámetros generales

Detalles generales del gráfico se controlan con `theme`

```{r, message=FALSE,warning=FALSE,fig.height=4.5}
p + 
  theme_bw() + 
  theme(legend.position ="bottom", axis.text.x=element_text(angle=90, color="red")) 

```

## Exportar gráficos

Podemos exportar los gráficos de ggplot mediante el método que usamos para los gráficos base

```{r,eval=FALSE}
p<- ggplot(mtcars)  + geom_point(aes(mpg,wt,color=factor(gear)))
png("grafico.png")
p
dev.off()
```


O mediante la función `ggsave`

```{r,eval=FALSE}
ggsave("grafico.pdf",p)
```

Si se ejecuta sin el segundo argumento, guarda el último gráfico realizado

```{r,eval=FALSE}
ggsave("grafico.pdf")
```


# ¿Qué has aprendido?

Con lo visto hasta ahora, ya puedes empezar a hacer tus propios programas en R y empezar a transformar los datos en conocimiento. Aunque al principio te esté resultando difícil, no retrocedas en tu empeño y ya verás como en poco tiempo R se convertirá en un aliado muy útil a la hora de trabajar con datos.


# Bibliografía 

## Manuales generales de R

- Introducction to R: <https://cran.r-project.org/doc/manuals/r-release/R-intro.html>.
En Español (<https://cran.r-project.org/doc/contrib/R-intro-1.1.0-espanol.1.pdf>)
- R para Principiantes, Emmanuel Paradis  <http://cran.r-project.org/doc/contrib/rdebuts_es.pdf> 
- R Programming for Data Science, Roger D. Peng <small>  <http://leanpub.com/rprogramming> 
- icebreakeR: <small> <http://www.ms.unimelb.edu.au/~andrewpr/r-users/icebreakeR.pdf>
- Introduction to Probability and Statistics Using R, G. Jay Kerns. Se obtiene en formato pdf descargando el paquete "IPSUR" desde R.
- R for Data Science, Garrett Grolemund, Hadley Wickham <http://r4ds.had.co.nz/>

## Paquete dplyr

- <https://github.com/hadley/dplyr>
- <https://github.com/hadley/dplyr/blob/master/vignettes/>
- <http://genomicsclass.github.io/book/pages/dplyr_tutorial.html>
- Manipulación de datos con dplyr <https://www.gitbook.com/book/rsanchezs/dplyr>


## Gráficos en R

- Capítulo 12 Introducction to R: <https://cran.r-project.org/doc/manuals/r-release/R-intro.html#Graphics>
- <http://docs.ggplot2.org/current/>
- <http://ggplot2.org/>
- ggplot2: Elegant Graphics for Data Analysis, Hadley Wickham


## Otros créditos

Los datos de resultados de fútbol proceden de <http://football-data.co.uk>

Los datos de valoración de películas proceden de la plataforma Kaggle <https://www.kaggle.com/datasets>

Los datos de demanda eléctrica han sido eleborados a partir de información procedente de Esios (Red Eléctrica de España) <https://www.esios.ree.es>
y datos meteorológicos de Weather Underground <https://www.wunderground.com/>

La gráfica de evolución del número de paquetes de R se ha realizado con un código inspirado en <http://blog.revolutionanalytics.com/2016/04/cran-package-growth.html>


```{r setup, include=FALSE}

library(reshape2)
library(dplyr)
library(ggplot2)
#install.packages(contrib.url)
```

# Tu reto en esta unidad

En muchas situaciones cotidianas nos encontramos con información en forma de medidas estadísticas. Veamos algún ejemplo e intenta responder a las preguntas:

- Leemos en la prensa: la tasa de paro se sitúa en el 18.5 % según la última encuesta de población activa. ¿Cómo se calcula exactamente esa tasa?
- El bebé come muy bien, está en el percentil 90 de peso. ¿Qué significa eso?
- La desigualdad en la distribución de la riqueza crece año tras año. ¿Cómo se mide cuantitativamente la desigualdad?
- El salario medio en España es de 24000 Euros anuales  mientras que el salario mediano se sitúa en 16000 euros anuales. ¿Qué diferencia hay entre el salario medio y el salario mediano? ¿Por qué son tan distintos?

No te preocupes si no has sabido responder correctamente a las preguntas. Al final de la unidad serás capaz de responderlas y explicárselo a cualquiera que tenga dudas. 

# Introducción

La definición de estadística según el diccionario de la Real Academia es:

1. Estudio de los datos cuantitativos de la población, de los recursos naturales e industriales, del tráfico o de cualquier otra manifestación de las sociedades humanas.

2. Rama de la matemática que utiliza grandes conjuntos de datos numéricos para obtener inferencias basadas en el cálculo de probabilidades.

Las dos definiciones se corresponden con las dos ramas principales de la estadística: la estadística descriptiva y la estadística inferencial. En esta unidad nos vamos a centrar en la primera.



![Áreas de trabajo de la estadística](img/workflow_statistics.png)

## Estadística Descriptiva y Análisis Exploratorio de Datos

La *estadística descriptiva* tiene por misión recolectar, presentar y caracterizar un conjunto de datos con el fin de describir apropiadamente las diversas características de ese conjunto.

Aprenderemos un conjunto de técnicas que permiten:

- Obtener, tabular, presentar, resumir y deducir propiedades del conjunto de datos en estudio
- Representarlos gráficamente de forma adecuada para descubrir sus propiedades y relaciones.


El conjunto de técnicas para explorar los datos de forma sistemática es lo que los estadísticos llaman 
**Análisis Exploratorio de datos** (EDA por sus siglas en inglés). El análisis exploratorio no sigue unas reglas fijas, en realidad
consiste en hacerse preguntas y buscar respuestas mediante visualizaciones, transformaciones, métricas y pequeños modelos.  
Normalmente estas respuestas nos hacen refinar las preguntas o hacernos nuevas preguntas.

El EDA es una parte importante del análisis de datos, incluso cuando las preguntas están claras desde el principio, ya que siempre va a ser necesario para explorar la calidad de los datos y proceder a su limpieza. 

En resumen, el análisis exploratorio de datos es el primer paso, y el más crucial, a la hora de analizar unos datos.  Antes de hacer inferencias/predicciones es esencial conocer y examinar nuestras  variables para entre otras cosas:

- Encontrar errores
- Encontrar valores anómalos
- Ver patrones en los datos
- Generar hipótesis
- Encontrar violaciones a las suposiciones estadísticas 
- ...  y porque si no luego tendremos problemas 


## Algunas definiciones básicas


- **Población** es el conjunto de elementos, individuos o entes del que se pretende estudiar una serie de características o comportamientos. 
- **Individuo** es cada uno de los elementos de la población y en Estadística el término puede referirse a cosas tan diversas como personas, provincias, empresas, edificios, etc. También se denomina observación.  
- **Censo** es la información recogida para el estudio de una característica en todos los individuos de una población.
- **Muestra** es un subconjunto de elementos de la población, seleccionado para llevar a cabo un estudio estadístico sobre una o varias variables.  Motivos tales como la  economía, rapidez, calidad, imposibilidad o la observación destructiva hacen habitual el estudio de las características de una población a través de muestras.


## Tipos de Dato

Una **variable** es una característica de los individuos de una población que nos interesa estudiar. Por ejemplo si estamos estudiando una población de personas: la altura, la edad, el color de piel, el peso, la presión arterial, o el grupo sanguíneo. Si nuestra población es el conjunto de hipotecas concedidas por un banco, las variables serán el capital del préstamo, el tipo de interés, el tipo de inmueble hipotecado, etc.  

**Dato** es el conjunto de valores en bruto que toma variable. 

Podemos clasificar las variables según el tipo de datos que recogen como:

- **Cualitativas**: son aquellas en la que los valores posibles no expresan una cantidad sino la clasificación del dato en una categoría. Por ejemplo: color de ojos, el lugar de nacimiento, el género, el código postal (aunque sea un número) o una respuesta binaria a una pregunta tipo Si/No o Verdadero/Falso. 
Las variables cualitativas pueden diferenciarse en:
    - *Variable cualitativa ordinal*: La variable puede tomar distintos valores ordenados siguiendo una escala establecida, aunque no es necesario que el intervalo entre mediciones sea uniforme, por ejemplo: bajo, medio, alto.
    - *Variable cualitativa nominal*: En esta variable los valores no pueden ser sometidos a un criterio de orden, como por ejemplo el color de pelo.

- **Cuantitativas**: aquellas cuyo resultado es un número. A su vez, las hay de dos tipos:
    - *Cuantitativas discretas*: cuando se toman valores aislados, normalmente resultados de conteos. Por ejemplo número de hermanos, número de empleados de una empresa o los puntos anotados por un jugador de baloncesto.  
    - *Cuantitativas continuas*: cuando, entre dos valores cualesquiera, puede haber valores intermedios. Es decir, se toman todos los valores de un determinado intervalo. Por ejemplo: la altura de una persona, la temperatura en un instante temporal o el precio de un producto. 

    
![](img/tiposdevariables.png)


Las características a estudiar en un conjunto de datos pueden ser muy distintas y es importante conocer qué tipo de variables estamos manejando porque las técnicas estadísticas a utilizar dependerán de ello y por tanto la validez de las conclusiones que se extraigan de su análisis.


## Datos ejemplos

Como siempre vamos a ilustrar todos los conceptos a partir de ejemplos basados en datos reales. En esta unidad vamos a utilizar datos de:

- Encuesta Estructura Salarial (Instituto Nacional de Estadística)
- EPA: Encuesta de Población Activa (Instituto Nacional de Estadística)
- Datos antropométricos: altura, peso, perímetro torácico, perímetro de muñeca, etc.

### Encuesta de Población Activa (EPA)

Fuente: Instituto Nacional de Estadística <http://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736176918&menu=resultados&idp=1254735976595>

En R podemos cargar los microdatos de la encuesta, es decir las respuestas detalladas de cada participante, usando el paquete MicroDatosEs.

```{r,cache=TRUE}
# Si no tienes instalado el paquete instalalo 
#install.packages('MicroDatosEs')

epa <- MicroDatosEs::epa2005("~/Formacion informatica/R/Proyectos/Master BD/3 Ciencia de datos con R/Datasets/ine/EPAWEBT0416")
epa <- as.data.frame(epa)

#ccaa, prov, edad, sexo, nac, nforma, aoi, factorel
# Para simplificar, recodificamos la ocupacion a o="ocupado", p="parado" e i="inactivo"
epa$AOI= factor(epa$AOI)
epa$ocupacion <- epa$AOI
lev=levels(epa$AOI)

# Se ha cambiado la codificación de AOI. Ahora los niverles se definen así. 
# levels(epa$ocupacion) = list(o =  lev[1:2], p = lev[3:4], i = lev[5:7])
levels(epa$ocupacion) = list(i =  lev[1:3], p = lev[5:6], o = lev[c(4,7)])

# epa$ocupacion[grepl("^Inactivos", epa$ocupacion)] <- "i"
# epa$ocupacion[grepl("[O-o]cupados", epa$ocupacion)] <- "o"
# epa$ocupacion[grepl("^Parados", epa$ocupacion)] <- "p"


```

### Encuesta de Estructura Salarial

Fuente: Instituto Nacional de Estadística 
<http://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736177025&menu=resultados&idp=1254735976596>

Cargamos en R los microdatos 

```{r,cache=TRUE}
ees= MicroDatosEs::ees2010("~/Formacion informatica/R/Proyectos/Master BD/3 Ciencia de datos con R/Datasets/ine/EES14_WEB")
ees=as.data.frame(ees)
```

### Antropométricos

Proporcionados por la revista Journal of Statistics Education, Volume 11, Number 2 (July 2003).
<http://ww2.amstat.org/publications/jse/v11n2/datasets.heinz.html>

```{r}
url="http://ww2.amstat.org/publications/jse/datasets/body.dat.txt"
body <- read.table(url)
BodyMeasurements <- c("Biacromial_diameter","Biiliac_diameter","Bitrochanteric_diameter","Chest_depth","Chest_diameter","Elbow_diameter","Wrist_diameter","Knee_diameter","Ankle_diameter","Shoulder_girth","Chest_girth","Waist_girth","Navel_girth","Hip_girth","Thigh_girth","Bicep_girth","Forearm_girth","Knee_girth","Calf_max_girth","Ankle_min_girth","Wrist_min_girth","Age","Weight","Height","Gender")    
names(body) <- BodyMeasurements 
```


# Distribución de  Frecuencias

La forma más sencilla de visualizar y simplificar los datos sin apenas perder la información que contienen es mediante la distribución de frecuencias. Consiste en contar el número de veces que ocurre cada valor en unos datos. Debemos distinguir entre variables categóricas y numéricas. 


## Variables categóricas

**Frecuencias absolutas** (conteo) y **frecuencias relativas** (porcentaje de observaciones respecto al total)

```{r}
# Tabla de frecuencias de la variable aoi (tipo de ocupación) de la EPA
#absolutas
table(epa$ocupacion)
#relativas
prop.table(table(epa$ocupacion))
```

Se define la **Moda** como el valor de la variable con mayor frecuencia. En el ejemplo la categoría moda es:  `r names(table(epa$ocupacion))[which.max(table(epa$ocupacion))]`

De forma gráfica mediante ggplot, construimos un gráfico de frecuencias mediante geom_bar()

```{r,fig.height=5}
ggplot(epa) + geom_bar(aes(ocupacion),fill="grey") 
```
 
 Los NA que aparecen en la gráfica corresponden a encuestados menores de 16 años. 
 El comando table no cuenta los NA a no ser que se lo digamos explícitamente
 
 
```{r}
table(epa$ocupacion,useNA='ifany')
```



Un tipo especial de variable categórica son aquellas que tienen un orden, por ejemplo los grupos de edad

```{r}
table(epa$EDAD)
```

En las representaciones gráficas debemos tener cuidado que las categorías queden ordenadas correctamente


```{r,fig.height=5}
# Hay que convertir la EDAD a factor y reordenar niveles
epa$EDAD=factor(epa$EDAD)
epa$EDAD=factor(epa$EDAD,levels=levels(epa$EDAD)[c(2:14,1)])
ggplot(epa) + geom_bar(aes(EDAD),fill="grey") + 
  theme(axis.text.x=element_text(angle = 90))
```

## Variables categóricas múltiples

Podemos estudiar las distribuciones de frecuencias de múltiples variables mediante tablas de contingencia en varias dimensiones

```{r}
# simplemente pasamos varios argumentos a la función table
t<- table(epa$ocupacion,epa$SEXO); t
```

Las frecuencias marginales, es decir las frecuencias de cada variable considerada de forma individual se obtienen mediante

```{r}
# Por filas (variable 1)
margin.table(t,1)
# Por columnas (variable 2)
margin.table(t,2)
```

Para obtener las frecuencias relativas, tanto absolutas como marginales

```{r}
# Frecuencias relativas
prop.table(t)
# Frecuencias relativas marginales por filas
prop.table(t,1)
# Frecuencias relativas marginales por columnas
prop.table(t,2)
```

Veamos ahora un ejemplo con 3 variables

```{r}
t<- table(epa$ocupacion,epa$SEXO,epa$NFORMA); 
t
```


## Gráficos de tablas multidimensionales

Veamos ahora algún ejemplo de representación gráfica de las tablas de frecuencias multidimensionales

```{r}
t=table(epa$ocupacion,epa$SEXO);
```

Suelen representarse con diagramas de barras, de diferentes tipos:

- **Barras apiladas** (position="stack): la altura de cada barra principal es proporcional a cada una de las frecuencias de la variable 1. Dentro de cada barra los segmentos de cada color son proporcionales a las frecuencias conjuntas de la variable 1 y 2. 


```{r}
ggplot(epa,aes(SEXO,fill=ocupacion)) + geom_bar(position="stack")
```

- **Barras apiladas normalizadas** (position="fill"):  puede observarse la distribución de la variable 2 relativa a la variable 1,  pero no se ve la distribución global de la variable 1

```{r}
ggplot(epa,aes(SEXO,fill=ocupacion)) + geom_bar(position="fill")
```

- **Barras paralelas** (position="dodge"): la atura de cada barra indica la frecuencia conjunta. Esta es la mejor representación para observar las frecuencias conjuntas, pero es difícil visualizar las marginales.

```{r}
ggplot(epa,aes(SEXO,fill=ocupacion)) + geom_bar(position="dodge")
```


## Variables numéricas continuas - Histogramas

Para estudiar la distribución de frecuencias en variables continuas dividimos en intervalos y contamos las frecuencias por intervalos.
La división en intervalos no tiene porqué ser regular, es decir podemos dividir en intervalos de distinta longitud
La representación gráfica asociada de denomina histograma. 
En el caso de intervalos no regulares debe representarse la densidad de frecuencia, que se calcula como la frecuencia relativa dividida por la longitud del intervalo

Consideremos una variable x que toma 50 valores.

Sea x= `r round(sample(c(runif(5,0,10),runif(10,10,20),runif(35,20,40)),50,replace=TRUE),1)`  

Para calcular su histograma manualmente deberiamos rellenar la siguiente tabla


|Intervalo|Frecuencia absoluta|Frecuencia relativa |Longitud intervalo|Densidad absoluta|Densidad relativa |
|---------|---------|--------------------|------------------|----------------|--------------------|
|0-10     |5    | 5/50 = 0.1 |10   |5/10=0.5     |0.1/10=0.01  |
|10-20    |10   | 10/50 = 0.2|10   |10/10=1      |0.2/10=0.02  |
| ...     |$n_i$|$f_i=n_i/N$ |$l_i$|$D_i=n_i/l_i$|$d_i=f_i/l_i$|
|Total    |N=50 |1           |  -   |     -      |      -      |



Veamos algún ejemplo con datos reales, de cómo calcular y representar un histograma con R

```{r,results='hide',fig.height=3.5}
h<- hist(body$Height,xlab="Altura (cm)")
```

La función *hist* calcula más que un gráfico

```{r}
h
```


## Densidad de frecuencia

En el histograma se puede mostrar la densidad de frecuencia en lugar de la frecuencia absoluta. 
La densidad de frecuencia en un histograma se define como la frecuencia relativa dividida por la longitud del intervalo.  

```{r,results='hide',fig.height=4}
h <- hist(body$Height,probability = TRUE, xlab="Altura (cm)")
```

Si los intervalos no son regulares, hist mostrará por defecto la densidad de frecuencia.  

```{r,results='hide',fig.height=4}
h <- hist(body$Height,breaks=c(145,160,165,170,175,180,185,200), xlab="Altura (cm)")
```


### Ajuste de densidad

Los histogramas a veces son ruidosos y su forma depende de la elección de los intervalos.
Muchas veces útil hacer un ajuste suave de la densidad de la distribución de una variable. En R lo hacemos con la función *density*.

```{r,results='hide',fig.height=4}
h <- hist(epa$DCOM,probability = TRUE, xlab="Meses en la empresa")
d <- density(epa$DCOM,na.rm = TRUE)
lines(d,col="red")
```

Realiza el ajuste mediante el método de estimación de densidad Kernel
Consiste en un ajuste de la función de densidad mediante la superposición de unas funciones de un tipo dado (suelen ser gaussianas, rectangulares, cosenos, etc.), que se denominan *funciones kernel*. 

$$\hat{f}(x)=\frac{1}{n\,b_w} \sum_{j=1}^{m} K\left(\frac{x-x_{j}}{b_w} \right) $$

donde el parámetro $b_w$ se denomina ancho de banda y representa la suavidad del ajuste. Muchas veces es necesario elegir el parámetro `bw` manualmente


![Estimación de densidad por funciones kernel](img/histogram_and_KDE.png)


En nuestro caso de ejemplo, veamos los ajustes de densidad obtenidos usando diferentes anchos de banda

```{r,results='hide',fig.height=5}
h <- hist(body$Height,probability = TRUE, xlab="Altura")
d1 <- density(body$Height,bw= 2.5,na.rm = TRUE)
d2 <- density(body$Height,bw= 5,na.rm = TRUE)
lines(d1,col="red")
lines(d2,col="blue")
```

Pueden realizarse los mismos gráficos usando ggplot

```{r, message=FALSE, warning=FALSE}
ggplot(body,aes(x=Height)) + 
  geom_histogram(aes(y=..density..),binwidth=5) + 
  geom_density(color="red",bw=2.5) +
  geom_density(color="blue",bw=5)
```


## Distribución Acumulada

Cuando trabajamos con variables cuantitativas y con cualitativas ordinales es interesante conocer las distribuciones de frecuencias acumuladas.

La distribución de frecuencias acumulada, tiene sentido para variables numéricas y para categóricas ordenadas.
Cuenta el número de eventos con valor igual o menor que cada posible valor de la variable. 

En R, la función `ecdf` calcula una función de distribución acumulada.

```{r,fig.height=4.8}
x=body$Height
fac=ecdf(x)
# ojo, ecdf devuelve una función
plot(fac, verticals = TRUE, do.points = FALSE)
```

# Medidas numéricas

Aunque la distribución de frecuencias nos da mucha información sobre nuestros datos, es deseable poder caracterizar los aspectos principales de una distribución de frecuencias mediante unas medidas numéricas que nos permitan resumir las características principales.

De esta manera podemos comparar diferentes conjuntos de datos o distribuciones mediante el análisis de las medidas numéricas adecuadas. Podemos distinguir diferentes tipos de medidas: 

- Centralidad
- Posición
- Dispersión
- Forma
- Concentración

que nos serán necesarias en función del fenómeno que queremos analizar. 

Existen una serie de preguntas genéricas que nos debemos hacer para toda medida de síntesis

- ¿Intervienen todos los datos?
- ¿Con qué tipo de datos se puede calcular?
- ¿Es única?
- ¿Es robusta?
- ¿Qué representatividad tiene?
- ¿Cómo se interpreta?
- ¿Cómo se comporta al transformar los datos originales?


# Medidas de centralidad/posición

Son las medidas que buscan situar dónde se encuentra situada la distribución de frecuencias, bien sean sus valores más representativos o centrales, bien sean sus zonas intermedias o sus extremas.

## Media

Dado un conjunto de datos $\{x_1,x_2, ..., x_n\}$, la media se define como

$$ \bar{x}=\frac{1}{n} \sum_{i=1}^{n} x_i $$

El mayor problema de la media es que es una medida muy sensible a "outliers" o valores anómalos. Eso quiere decir que la media no es una medida robusta.

Por ejemplo si miramos los salarios de la Encuesta de Estructura Salarial

```{r}
mean(ees$SALBRUTO)
summary(ees$SALBRUTO)
```

Se observa que el valor máximo es muy superior a la media. Si tomamos una muestra que contiene el máximo o no, obtenemos valores muy distintos para la media muestral. 

```{r,fig.height=5,echo=FALSE}
set.seed(1)
# Tomo una muestra que contiene al máximo salario
salarios = c(sample(ees$SALBRUTO,100),max(ees$SALBRUTO,100))
hist(salarios, xlim=c(0,1e5), breaks = c(seq(0,1e5,5000),max(salarios)),main="Medias muestra de tamaño 100 ")
lines(density(salarios,bw=5e3),col="grey")
# Media sin contar el máximo salario
mt=mean(salarios[salarios<max(salarios)])
# Media contando todos los puntos
m=mean(salarios)
abline(v=mean(salarios),col="red",lwd=2)
abline(v=mean(salarios[salarios<max(salarios)]),col="blue",lwd=2)
text(1e4,2e-5,paste("Trimmed Mean=",round(mt,1)),col="blue",cex=0.8)
text(3.8e4,3e-5,paste("Mean=",round(m,1)), col="red", cex=1)
```

## Otros tipos de media

- **Media Truncada (trimmed mean)**: Eliminamos una fracción $\alpha$ por arriba y abajo de los datos ordenados. Esto la hace poco sensible a los valores anómalos.

```{r}
mean(salarios,trim = 0.01)
mean(salarios,trim = 0.1)
```


- **Media Ponderada**: Los distintos elementos tienen distinta importancia

$$ \bar{x}=\frac{\sum_{i=1}^{n} w_i \, x_i}{\sum_{i=1}^{n} w_i}$$

```{r}
# La variable FACTOTAL se denomina factor de elevación y se usa para corregir los errores de muestreo de la encuesta
weighted.mean(ees$SALBRUTO,ees$FACTOTAL)
```


- **Geométrica**: Relevante cuando el conjunto de números es interpretable por su producto. 
Por ejemplo tasas de crecimiento ($x_{i+1} = t_1 t_2 ... t_n x_{0}$)
$$ \bar{t}=\left(\prod_{i=1}^{n} t_i \right)^{\frac{1}{n}}$$


## Mediana

La mediana de una serie de datos $\{x_1,x_2,...,x_n\}$ es el valor tal que la mitad de las x's son mayores que él y la otra mitad son menores

- Si n es impar, entonces la mediana  es el valor central $x_{(n+1)/2}$ de la serie ordenada.
- Si n es par, la mediana es el promedio de los dos valores centrales

La mediana es muy poco sensible los valores anómalos, en este sentido es una medida robusta. Para comparar variables que contienen muchos valores anómalos, la mediana es más útil que la media.

En R se calcula mediante la función *median*

```{r}
x=rnorm(100)
# Mediana según definición
x=sort(x)
mean(x[50:51])
# Usando la función apropiada de R
median(x)
```

## Cuantiles

- El cuantil p, que denotamos $q_p$, de una variable, es el valor tal que una fracción p  
de observaciones se encuentran por debajo  y la fracción  (1 − p) se
encuentran por encima de este valor.

- Los percentiles, $p_k$ se definen de forma equivalente expresados en porcentaje en lugar de fracciones

- Los cuartiles $Q_1$, $Q_2$ y $Q_3$ corresponden a los cuantiles 0.25, 0.5 y 0.75 


Veamos gráficamente para una distribución de frecuencias dada, la relación de  los cuartiles con las áreas encerradas por la densidad de frecuencia.

```{r,echo=FALSE}
f=7
e=1
x=seq(0,1.5,0.01)
y=dweibull(x,shape=f,scale=e)

plot(x,y,type="l",xlim=c(0.4,1.4),ylim=c(0,3),ylab="p(x)")
q=qweibull(c(0.25,0.5,0.75,0.99999),shape=f,scale=e)

x=seq(0,q[1],0.001)
y=dweibull(x,shape=f,scale=e)
polygon(c(0,x,q[1]),c(0,y,0),col="skyblue")
text(q[1],2.75,"Q1",col=1,cex=0.8)
text(0.875*q[1],0.5,"25%",cex=0.8)

x=seq(q[1],q[2],0.001)
y=dweibull(x,shape=f,scale=e)
polygon(c(q[1],x,q[2]),c(0,y,0),col="green")
text(q[2], 2.75, "Q2",col=1,cex=0.8)
text((q[1]+q[2])/2, 0.5, "25%", cex=0.8)

x=seq(q[2],q[3],0.001)
y=dweibull(x,shape=f,scale=e)
polygon(c(q[2],x,q[3]),c(0,y,0),col="yellow")
text(q[3], 2.75, "Q3",col=1,cex=0.8)
text((q[2]+q[3])/2, 0.5, "25%", cex=0.8)

x=seq(q[3],q[4],0.001)
y=dweibull(x,shape=f,scale=e)
polygon(c(q[3],x,q[4]),c(0,y,0),col="orange")
text((q[3]+q[4])/2, 0.5, "25%", cex=0.8)

boxplot(rweibull(10000,shape=f,scale=e),horizontal=TRUE,alpha=0.3,add=TRUE,cex=0.5)

```

Para calcular cuantiles en R lo hacemos mediante la función `quantile`

```{r}
x=ees$SALBRUTO
quantile(x,probs = 0.6)
# Si no especificamos probs, por defecto muestra el 0,0.25,0.5,0.75 y 1
quantile(x)
# Podemos especificar un conjunto de probs mediante seq
# Calculo los cuantiles 0,0.1,0.2, ..., 0.9,1
quantile(x,probs=seq(0,1,.1))
```


Los cuantiles son en realidad los inversos de la distribución acumulada de frecuencias


```{r,cache=FALSE,fig.height=4.5}
x=ees$SALBRUTO
plot(ecdf(x),xlim=c(0,1e5))
abline(v=quantile(x,seq(0,1,.25)),lty=2,lwd=0.5,col=2)
abline(h=seq(0,1,.25),lty=2,lwd=0.5,col=2)
```



### Boxplots

A partir de los cuantiles se elaboran los boxplots o diagramas de caja, que son muy útiles para comparar las distribuciones de datos pertenecientes a distintas muestras o categorías. Se trata de representar la distribución de datos mediante una caja y unos segmentos, cuyos límites se corresponden con medidas de posición tal y como se muestra en la siguiente figura.

![Significado de un boxplot](img/Boxplot.png)

Veamos un  ejemplo con los datos de la encuesta de estructura salarial, donde  mostramos los salarios de los encuestados divididos según su nivel de estudios.   

```{r,fig.height=5}
# Reordenamos los niveles por orden creciente de salario mediante reorder(ESTU,SALBRUTO) 
ggplot(ees) + geom_boxplot(aes(reorder(ESTU,SALBRUTO),SALBRUTO)) + theme(axis.text.x = element_text(angle=30)) +
  coord_cartesian(ylim=c(0,1e5)) + xlab("")
```

Podemos hacer el mismo gráfico  distinguiendo además  por sexo y tipo de jornada, gracias al coloreado y los facets

```{r,fig.height=4}
ggplot(ees) +  
geom_boxplot(aes(reorder(ESTU,SALBRUTO),SALBRUTO,fill=SEXO)) + facet_wrap(~TIPOJOR) + 
  theme(axis.text.x = element_text(size=6,angle=30)) + xlab("") +
  coord_cartesian(ylim=c(0,1e5))
```

Podemos realizar también un boxplot a medida, es decir donde los límites de las cajas y de los segmentos signifiquen cosas distintas a lo habitual. 
Por ejemplo hagamos un boxplot con cuantiles 0.05, 0.25, 0.5, 0.75 y 0.95. Además pintamos las medias mediante puntos.
Con el paquete ggplot es posible, haciendo stat="identity" dentro de geom_boxplot y especificando manualmente los límites de cajas y segmentos: ymin, lower, middle e ymax.


```{r}
# En primer lugar escribimos una función que me calcula las métricas
seven_nums <-  function(x,p=c(0,0.05,0.25,0.5,0.75,0.95,1)){
  q=quantile(x,p,na.rm=TRUE)
  res =data.frame(num=length(x),m=mean(x,na.rm=TRUE),sd=sd(x,na.rm=TRUE),t(q))
  names(res)[-(1:3)] =paste0("q",round(p*100))
  res
}
tmp <- ees %>% group_by(ESTU,SEXO,TIPOJOR) %>% do({
  seven_nums(.$SALBRUTO)
})

ggplot(tmp) +  
geom_boxplot(aes(x=reorder(ESTU,m),ymin=q5,lower=q25,middle=q50,
                 upper=q75,ymax=q95,fill=SEXO),alpha=0.3,stat="identity") +
  geom_point(aes(reorder(ESTU,m),m,color=SEXO), position=position_dodge(width=.9)) + 
  facet_wrap(~ TIPOJOR) + 
  theme(axis.text.x = element_text(size=6,angle=30))
```

## Stripchart

Cuando tenemos pocos datos, a veces es más útil el stripchart que muestra la distribución de todos los puntos

```{r,echo= FALSE}
opar <- par(mfrow=c(2,2),mex=0.8,mar=c(3,3,2,1)+.1)
colors=rep(c("red","blue","green"),nrow(iris))
stripchart(iris$Sepal.Length~iris$Species,col=colors)
stripchart(iris$Sepal.Length~iris$Species,method="stack",col=colors)
stripchart(iris$Sepal.Length~iris$Species,method="jitter",col=colors)
stripchart(iris$Sepal.Length~iris$Species,method="jitter",jitter=.03,col=colors)
par(opar)
```


# Medidas de dispersión

Es importante completar la información proporcionada por las medidas de posición con medidas de dispersión que midan el grado de variabilidad de las variables.

## Varianza / Desviación típica

La medida de dispersión más habitual es la varianza. Debemos de distinguir entre 

**Varianza poblacional**
$$ \sigma^2 = \frac{1}{n}\sum_{}^{}\left( x_i - \mu \right)^2$$

**Varianza muestral**
$$ s^2 = \frac{1}{n-1}\sum_{}^{}\left( x_i - \bar{x} \right)^2$$

dependiendo de si se aplica a una población con la media conocida o a una muestra de la que conocemos una estimación de la media poblacional, que es la media muestral. 

En las fórmulas $\bar{x}$ es la media de la muestra y $\mu$ la media de la población.

La  **Desviación típica ** es simplemente la raíz cuadrada de la varianza, y tiene las mismas unidades que los datos.

$\sigma=\sqrt{\sigma^2}$

La varianza, al igual que la media, está muy influenciada por los valores anómalos.


**Autotexto:**
**¿Por qué n-1 ?**

Una pregunta muy habitual es porque en la varianza muestral dividimos por n-1 y no por n. Respondamos de un modo práctico, a partir de los resultados de una simulación. Este es un enfoque que vamos a utilizar habitualmente a lo largo del curso. No vamos a hacer demostraciones matemáticas complicadas, pero vamos a hacer comprobaciones mediante experimentos con R. Este enfoque se denomina por muchos autores "Estadística Moderna". 

Genero 10000 muestras de una población normal con media 0 y $\sigma=1$

Calculo $\sum_{i}\left( x_i - \bar{x} \right)^2$ 

```{r}
varn <- function(x){sum((x-mean(x))^2)}
res=NULL
# Genero 10000 muestras de tamaño 10 y calculo la suma de cuadrados
n=10
s<- replicate(10000,varn(rnorm(n)))
mean(s)
# Para una muestra de tamaño 100
n=100
s<- replicate(10000,varn(rnorm(n)))
mean(s)

```

Se observa que el valor medio de $\sum_{i}\left( x_i - \bar{x} \right)^2$ es aproximadamente 9 o 99, en lugar de 10 o 100. Por eso en la definición de la varianza muestral se divide por n-1

La diferencia radica en que en el caso de la población la media real es conocida y para la muestra tenemos una estimación de esta calculada a partir de los datos. 

**Fin autotexto**


## Coeficiente de variación

Se define como

$$ CV = \frac{\sigma}{|\bar{x}|} $$


Simplemente escala la desviación típica a la magnitud de $\bar{x}$. Es útil para comparar la dispersión en variables con diferentes órdenes de magnitud, por ejemplo porque han sido expresadas en unidades distintas.

## Intervalo intercuartílico 

$$
IQR = Q_3 -Q_1 = q_{0.75} - q_{0.25} = P_{75} - P_{25}
$$

Es una medida robusta, a la que afectan poco los valores anómalos.

En él se basa el criterio de detección de valores anómalos que usan los boxplots: $x_i$ es anómalamente grande si $x_i \ge q_{0.75} + 1.5 IQR$ y anómalamente pequeño si  $x_i \le q_{0.25} - 1.5 IQR$. Este criterio se basa en las propiedades de la distribución normal y según él si los valores están normalmente distribuidos solo el 0.70 % serían clasificados como anómalos y se usa el IQR para que los propios valores anómalos no influyan en la medida de la dispersión. 

![IQR y valores anómalos](img//criterio_outliers.png)

# Medidas de Forma

Las medidas de posición y dispersión son las más habituales a la hora de sintetizar la información contenida en unos datos. Sin embargo en algunas ocasiones es conveniente obtener más información sobre la forma de las distribuciones. 

## Asimetría (skewness)

Las distribuciones de frecuencia pueden clasificarse según su simetría de la siguiente manera: 

![Tipos de simetría/asimetría en las distribuciones de frecuencia](img/asimetria.png)


El *skewness* o coeficiente de asimetría es un indicador numérico que clasifica la simetría de una distribución de frecuencias según su valor.

$$Sk = \frac{1}{n} \sum_{i=1}^{n} \left(\frac{ x_i - \mu}{\sigma} \right)^3 $$

- Si Sk<0 --> Asimétrica a izquierda
- Si Sk=0 --> Simétrica
- Si Sk>0 --> Asimétrica a derecha

En R podemos calcularlo usando la función `skewness` contenida en el paquete e1071.

En el ejemplo siguiente, generamos 3 distribuciones de números aleatorios con diferentes propiedades de simetría y calculamos su skewness. 

La distribución de Weibull depende de dos parámetros: forma(shape) y escala(scale) y en función de sus valores genera números distribuidos con diferentes propiedades de simetría. En cambio la distribución normal genera números simétricamente distribuidos.


```{r,fig.height=3.5,message=FALSE}
library(e1071)
df=rbind(data.frame(asim="right",x=rweibull(1000,shape = 1.3,scale= 1)),
          data.frame(asim="left",x=rweibull(1000,shape = 7,scale=3)-2),
            data.frame(asim="norm",x=rnorm(10000,mean=1,sd=1)))
skew= df %>% group_by(asim) %>% summarise(sk=skewness(x))  
ggplot(df) + geom_density(aes(x,color=asim)) + 
  geom_text(aes(1.2*(as.numeric(asim)-1.5),c(0.5,1,0.4),label=paste0("Sk=",round(sk,2)), color=asim),data=skew) +
  scale_color_hue(guide=FALSE)
```

## Curtosis

Mide si una distribución es más "picuda" que una normal o más plana

$$ K = \frac{1}{n} \sum_{i=1}^{n} \left(\frac{ x_i - \mu}{\sigma} \right)^4 -3$$

- Si K>0 --> Leptocúrtica (más apuntada)
- Si K=0 --> Mesocúrtica
- Si K<0 --> Platicúrtica (más plana)

En realidad los valores que más afectan a la curtosis son los valores extremos (las colas de la distribución) :

- Platicúrtica(K<0)--> colas pequeñas
- Leptocúrtica (K>0) --> colas largas
- Una curtosis grande indica la presencia de valores anómalos, respecto a una distribución normal

En R calculamos la curtosis mediante la función `kurtosis` del paquete e1071.

```{r}
# Uniforme  - platicúrtica
# Es intervalo ha elegido para que tenga varianza unidad 
xu=runif(100000,min = -1.73,max = 1.73)
kurtosis(xu)
# t-student - leptocúrtica  
xt=rt(100000,df=4)
# Escalo para que tenga varianza unidad
xt=xt/sd(xt)
kurtosis(xt)

```

Gráficamente

```{r, message=FALSE, warning=FALSE}
df=rbind(data.frame(K="t-student - lepto",x=xt),
          data.frame(K="Uniforme - plati",x=xu),
            data.frame(K="Normal",x=rnorm(100000,mean=0,sd=1)))
ggplot(df) + geom_density(aes(x,color=K)) + xlim(-5,5)
```


Un truco mnemotécnico nos lo da la siguiente imagen, creada originalmente por el estadístico William Gosset, más conocido por su seudónimo Student, descubridor de la distribución t de Student.

![kurtosis](img//kurtosis.gif)



# Medidas de concentración

## Curva de Lorentz e Índice de Gini

Se pretende medir la distribución entre sus componentes de una determinada variable: por ejemplo riqueza entre personas

![Curva de Lorent e índice de Gini](img//Gini_coefficient.png)

Veamos cómo se calcula  la curva de Lorentz por ejemplo para la distribución de salarios de la encuesta de estructura salarial:

- Se ordena en sentido creciente el vector de salarios de las personas encuestadas. Y se calcula la suma acumulada de salarios. 

- Se muestra en el eje x, la proporción de personas y en el eje y la proporción de salario acumulado.

-  Si la curva de Lorentz pasa por ejemplo por el punto (0.5,0.3) quiere decir que el 50% de las personas más pobres poseen un 30% de la riqueza total.


El **índice de Gini** mide cuanto se aleja la distribución de salarios de la equipartición (recta y=x). Representa el área de la zona sombreada de la figura

```{r,cache==TRUE, fig.height = 3.7 }
x=ees$SALBRUTO
n=length(x)
x=sort(x)
cumx=cumsum(x)
plot((1:n)/n,cumx/cumx[n],type="l",xlab="Prop personas",ylab="Prop salario acumulado")
polygon(c(0,(1:n)/n,0),c(0,cumx/cumx[n],0),col="yellow")
abline(0,1)
```

En R, podemos calcular el índice de Gini y la curva de Lorentz mediante le paquete `ineq`.

```{r,include=FALSE}
if(!require('ineq'))
  install.packages('ineq')
```

```{r,fig.height=3.7}
library('ineq')
# Indice de GIni
Gini(ees$SALBRUTO)
# Curva de Lorentz
plot(Lc(ees$SALBRUTO))
```

Podemos hacernos más preguntas sobre la desigualdad en los salarios. Respondamos a algunas.

¿Están más repartidos los salarios en hombres o mujeres?

```{r}
ees %>% group_by(SEXO) %>% summarise(gini=Gini(SALBRUTO))
```

¿Y en grupos por estudios terminados?

```{r}
ees %>% group_by(ESTU) %>% summarise(gini=Gini(SALBRUTO))
```


# Dos o más variables numéricas


## Covarianza

Para detectar relaciones entre dos variables continuas, las medidas más básicas son la covarianza y la correlación lineal. 

**Covarianza:**

$$
Cov(x,y) = \frac{1}{n-1} \sum_{i=1}^{n} \left( x_i - \bar{x} \right)  
  \left( y_i - \bar{y} \right)
$$


Mide cuanto se espera que una variable cambie cuando cambia la otra, suponiendo que ambas están relacionadas linealmente. 
Si dos variables son independientes su covarianza es 0.  

La tabla siguiente nos indica el signo de las contribuciones de cada observación a la covarianza


| Contribución cov| $y<\bar{y}$| $y>\bar{y}$|
|-----------------|----------|----------|
|$x<\bar{x}$      |+         |-         |
|$x>\bar{x}$      |-         |+         |


En el gráfico a continuación, se muestran los puntos (x,y) de dos casos. El de la izquierda con covarianza positiva y de la derecha con covarianza negativa. Los puntos están coloreados en azul cuando hacen una contribución positiva a la covarianza y en rojo cuando su contribución es negativa. Se observa la diferente proporción de puntos rojos y azules en cada uno de los casos.

```{r,echo=FALSE,fig.width=10}
x=rnorm(100)
y=1+2*x+2*rnorm(100)
mx=mean(x)
my=mean(y)
df=data.frame(x,y)
df=df %>% mutate(difmx=x-mx,difmy=y-my,cov=factor(sign(difmx*difmy)))
y=1-2*x + 2*rnorm(100)
df1=data.frame(x,y)
df1=df1 %>% mutate(difmx=x-mx,difmy=y-my,cov=factor(sign(difmx*difmy)))
df=rbind(cbind(corr="positiva",df),cbind(corr="negativa",df1))

ggplot(df) + geom_point(aes(x,y,color=cov)) + geom_hline(aes(yintercept = my),color="grey60",lty=2) + geom_vline(aes(xintercept = mx),lty=2,color="grey60") + 
  facet_wrap(~corr) + xlab("") + ylab("")


```

## Correlación

Para tener una medida de relación que no dependa de la escala de cada
variable, usamos la correlación lineal.

Se define a la **correlación lineal** o coeficiente de correlación de Pearson 
como:

$$ r(x,y) = \frac{Cov(x,y)}{\sigma_x \sigma_y}$$

La correlación lineal varía entre -1 y 1

- Si r=1 si x e y están perfectamente correlacionadas de forma positiva
- Si r=1 si x e y están perfectamente correlacionadas de forma negativa
- Si r=0 son independientes linealmente. Esto no quiere decir que no pueda existir una relación no lineal entre las variables

Para calcular la covarianza y correlación en R se usan las funciones `cov` y `cor`

```{r}
x=rnorm(100,sd=4); y=2*x + rnorm(100)
cov(x,y)
cor(x,y)
```

Sin embargo la covarianza y la correlación nula no son una condición suficiente para la independencia de dos variables.
Es posible que dos variables estén relacionadas de forma no monótona y en ese caso la correlación lineal podría ser nula.
Veamos un  ejemplo de dos variables  x e y que están relacionadas por una relación cuadrática y la correlación lineal es aproximadamente 0.

```{r,fig.height=3.5,fig.width=4}
set.seed(2)
x=rnorm(200); 
#
y=x^2 + rnorm(200)
cor(x,y)
```


```{r}
plot(x,y,cex=0.7)
```

## Matrices de correlación

Cuando tenemos más de dos variables, entre las cuales quiero calcular la correlación, podemos hablar de la matriz de covarianza o la matriz de correlación. 

En R, se puede pasar como argumento de `cov` o `cor` una matriz o un data frame numérico. 
En ese caso devuelve una matriz con las covarianzas o correlaciones lineales de las diferentes combinaciones de columnas.

$$C_{i,j} = cov(x_i,x_j)$$

```{r,echo=TRUE}
tmp <- body %>%  dplyr::select(Age,Gender,Weight, Height, Chest_diameter,Chest_depth,	                            Bitrochanteric_diameter,Wrist_min_girth, Ankle_min_girth)
tmp$Gender=factor(tmp$Gender)
levels(tmp$Gender)=c("W","M")
```

```{r}
cor(tmp[c(1,3,4,5)])
```

## Gráficos de correlación 

### Gráficos de dispersión 

Con múltiples variables podemos hacer una matriz de gráficos de dispersión

```{r,cache=TRUE,message=FALSE,warning=FALSE,fig.height=5.5}
pairs(tmp,col=as.numeric(tmp$Gender),cex=0.5,alpha=0.3)
```

Con `ggplot`, se hace mediante la función `ggpairs` del paquete `GGally`. Muestra los gráficos de dispersión de las diferentes combinaciones de variables en el triángulo inferior, mientras que utiliza la diagonal para mostrar las distribuciones univariantes y el triángulo superior para mostrar los valores numéricos del coeficiente de correlación. 

```{r,cache=TRUE,message=FALSE,warning=FALSE,fig.height=5.5}
library(GGally)
GGally::ggpairs(tmp, columns=c(1,3:6))

```


Si se elige una variable para agrupar y colorear, calcula también las correlaciones por grupos.

```{r,cache=TRUE,message=FALSE,warning=FALSE,fig.height=5.5}

ggpairs(tmp, mapping=ggplot2::aes(color = Gender),columns=c(1,3:6))
```


# ¿Qué has aprendido?

En esta unidad has aprendido a exprimir la información que está contenida en un conjunto de datos. 

En concreto, has aprendido a:

- Calcular, representar y comparar distribuciones de frecuencias
- Sintetizar los datos mediante medidas de:
    - posición
    - dispersión 
    - forma
    - desigualdad
- Detectar correlaciones entre variables


de forma tanto numérica como gráfica. 

Ahora ya dispones de todo el conocimiento para realizar el análisis exploratorio de datos, que es un elemento fundamental de la ciencia de datos, ya que es a través del cual serás capaz de hacerte las preguntas adecuadas que posteriormente responderás usando las técnicas de inferencia y modelización que vas a aprender en las siguientes unidades. 

Para terminar, vuelve a las preguntas del inicio de la unidad y verás como eres capaz de dar una respuesta para todas ellas.


# Bibliografía 

- Introduction to Probability and Statistics Using R, G. Jay Kerns.
- R for Data Science, Garrett Grolemund, Hadley Wickham <http://r4ds.had.co.nz/>
- Probabilidad y estadística para ingeniería y ciencias / Jay L. Devore


---
title: 'Comunicación'
author: 'MasterD'
output:
  html_document:
    highlight: tango
    theme: united
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
  pdf_document:
    toc: yes
  revealjs::revealjs_presentation:
    center: yes
    css: style.css
    highlight: pygments
    theme: sky
  word_document:
    reference_docx: www/plantillaMasterD_basica5.docx
    toc: yes
---


```{r setup, include=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
options(width=100)
library(knitr)
inline <- function(x = "") paste0("`` `r ", x, "` ``")
```

# Tu reto en esta unidad

Seguro que a lo largo de tu vida  has tenido que repetir algún trabajo que hiciste hace mucho tiempo. Y cuando has ido a recuperar aquello que hiciste años atrás, has olvidado casi completamente como lo hiciste y tienes que empezar prácticamente de cero. 

En esta unidad vas a aprender una herramienta para crear documentación que refleja no solo los resultados que obtuviste, sino que también guarda como llegaste a ellos y además te permite compartir el trabajo con otras personas. Este paradigma de llama "investigación reproducible". 

¿Quieres que tu trabajo sea reproducible con el paso de tiempo, por ti u otras personas? 


# Introducción

Hasta ahora hemos aprendido las herramientas para cargar los datos en R, transformarlos de manera adecuada, calcular métricas estadísticas y a representarlos gráficamente para entender las relaciones que existen entre ellos.  

Sin embargo, no importa lo buenos que sean tus análisis si no eres capaz de comunicar tus resultados a los demás de manera eficaz. 

La comunicación es el tema que vamos a desarrollar en esta unidad, que vamos a dividir en dos secciones principales: 

- **Gráficos para comunicación**: donde aprenderemos como transformar los gráficos que hemos realizado en nuestros análisis exploratorios en gráficos preparados para exponer o publicar. El objetivo de estos gráficos es que  nuestra audiencia los comprenda con facilidad.
- **Documentos dinámicos con R markdown**: donde aprenderemos R markdown, que es una herramienta muy potente que permite integrar texto con código y resultados. Estos documentos pueden generarse en diferentes formatos en función de las necesidades y del público objetivo: podemos generar notebooks para comunicación con nuestros analistas colaboradores, o informes para un público más general en html, pdf, o documentos de Word. 


# Gráficos para comunicación

Hasta ahora hemos aprendido a hacer gráficos orientados al análisis exploratorio de datos. Cuando se hacen gráficos exploratorios, conocemos perfectamente que variables estamos utilizando y realizamos a veces cientos de gráficos, la mayoría de los cuales los desechamos.

Pero cuando hemos llegado a  entender los datos, hemos extraído conclusiones de ellos y queremos comunicar los resultados al resto del mundo, nuestro público objetivo no tendrá el mismo conocimiento de los datos que tenemos nosotros. Deberemos esforzarnos en crear unos gráficos que sean fáciles de comprender por nuestra audiencia.

En este capítulo, aprenderás algunas de las herramientas que ggplot2 proporciona para crear gráficos eficaces, bien diseñados para la difusión y la publicación. 

Vamos a trabajar con los datos del paquete gapminder, que nos proporcionan información de población, riqueza y esperanza de vida de un buen número de países del mundo desde los años 50 del siglo XX. Si no lo tienes instalado

```{r, eval=FALSE}
install.packages("gapminder")
```


## Etiquetas

El sitio más fácil para comenzar a convertir un gráfico exploratorio en un gráfico publicable es mediante un buen  etiquetado. 

Añade etiquetas con la función labs(). Podemos añadir:

- títulos
- subtítulos 
- captions (Explicaciones a pie de imagen)

El siguiente ejemplo muestra para el año 2007 la relación entre la esperanza de vida y el producto interior bruto per cápita.
Le añadimos un título.

```{r, warning=FALSE, message=FALSE, fig.width=8}

library(gapminder)
library(ggplot2)
library(dplyr)

gapminder %>% filter(year==2007) %>% 
ggplot(aes(gdpPercap,lifeExp)) + geom_point(aes(color=continent)) + 
  geom_smooth(se=FALSE) + 
  labs(title="La esperanza de vida crece con la riqueza de los paises")
```

El propósito del título de un gráfico es resumir el hallazgo principal. Debemos evitar, en la medida de lo posible,  títulos que simplemente describan lo que representa  el gráfico, como por ejemplo: "Gráfico de dispersión de la esperanza de vida frente a la renta per cápita".

Si necesitas agregar más texto, hay dos etiquetas útiles que puede utilizar en ggplot2 (a partir de la versión 2.2.0): 

- `subtitle`: el subtítulo agrega detalles adicionales usando una fuente más pequeña debajo del título.
- `caption`: añade texto en la parte inferior de la gráfica.  Se usa a menudo para especificar la fuente de los datos.

```{r,warning=FALSE, message=FALSE, fig.width=8}
gapminder %>% filter(year==2007) %>% 
ggplot(aes(gdpPercap,lifeExp)) + geom_point(aes(color=continent)) + 
  geom_smooth(se=FALSE) + 
  labs(title="La esperanza de vida crece con la riqueza de los paises",
       subtitle="Año 2007",
       caption= "Fuente: Fundación Gapminder https://www.gapminder.org/")

```

También podemos  utilizar labs() para cambiar las etiquetas de los  ejes y los títulos de las leyendas. Es una  buena costumbre reemplazar los  nombres de variables cortas con descripciones más detalladas, e incluir las unidades.

```{r,warning=FALSE, message=FALSE, fig.width=8}

gapminder %>% filter(year==2007) %>% 
  ggplot(aes(gdpPercap,lifeExp)) + geom_point(aes(color=continent)) + 
    geom_smooth(se=FALSE) + 
    labs(title="La esperanza de vida crece con la riqueza de los paises",
       subtitle="Año 2007",
       caption= "Fuente: Fundación Gapminder https://www.gapminder.org/",
       x = "Producto Interior Bruto per cápita (Dólares americanos)",
       y = "Esperanza de vida (Años)",
       color = "Continente")

```


## Anotaciones

Además de etiquetar los componentes principales del gráfico, a menudo es útil etiquetar observaciones individuales o grupos de observaciones. 
La primera herramienta que vamos a ver es `geom_text()`. Es similar a `geom_point()`, pero tiene una estética adicional: `label`. Esto hace posible agregar etiquetas textuales a los gráficos.

Como ejemplo podríamos representar cada país por su nombre en lugar de por un punto

```{r, message=FALSE}
gapminder %>% filter(year==2007) %>% 
ggplot(aes(gdpPercap,lifeExp)) + geom_text(aes(label=country,color=continent)) + 
  geom_smooth(se=FALSE)
```

Aunque resulta un poco difícil de leer debido a la gran cantidad de países representados. Podríamos limitar esta cantidad, por ejemplo escribiendo solo el nombre del país con mayor esperanza de vida en cada continente. 
Para ello necesitamos un poco de dplyr.


```{r, message=FALSE}
gap2007 <- gapminder %>% filter(year==2007)

paises_longevos <- gap2007 %>%  group_by(continent) %>% filter(row_number(desc(lifeExp)) == 1)

ggplot(gap2007, aes(gdpPercap,lifeExp)) + geom_point(aes(color=continent)) +
  geom_text(aes(label=country,color=continent),data=paises_longevos) + 
  geom_smooth(se=FALSE)
```

Aun escribiendo solo 5 etiquetas, no acaban de leerse bien los nombres de los países ya que se solapan unos con otros. 
Mediante `geom_label()` podemos añadir texto rodeado por un recuadro. Esto mejora algo la legibilidad

```{r, warning=FALSE, message=FALSE}
# Ponemos la transparencia de las etiquetas a 0.5 para ver debajo de lo solapado
ggplot(gap2007, aes(gdpPercap,lifeExp)) + geom_point(aes(color=continent)) +
  geom_label(aes(label=country,color=continent),data=paises_longevos, alpha=0.5) + 
  geom_smooth(se=FALSE)
```

Pero seguimos sin evitar el problema de las etiquetas solapadas. Para mejorar este aspecto podemos usar el paquete ggrepel (si no lo tienes instalado ya sabes como hacerlo) que tiene una función que ajusta la posición de las etiquetas para que no se solapen


```{r, warning=FALSE, message=FALSE}
ggplot(gap2007, aes(gdpPercap,lifeExp)) + geom_point(aes(color=continent)) +
  geom_point(aes(color=continent),size=3, shape = 1, data=paises_longevos) +
  ggrepel::geom_label_repel(aes(label=country,color=continent),data=paises_longevos,alpha=0.7) +
  geom_smooth(se=FALSE)
```

Date cuanta que además hemos añadido una capa nueva al gráfico, que muestra los puntos de los países seleccionados de manera resaltada.


A veces simplemente queremos añadir un texto explicativo en algún lugar del interior del gráfico, pero cuyo texto no está contenido en los datos.  Para ello tenemos que crear una estructura de datos para esta anotación. Veamos un ejemplo

```{r, fig.width=8}
label <- gap2007 %>%
  summarise(
    lifeExp = 0.85*max(lifeExp),
    gdpPercap = max(gdpPercap),
    label = "Cuando los paises son suficientemente ricos\n hay otros factores que influyen más en la salud que el PIB"
  )

ggplot(gap2007, aes(gdpPercap,lifeExp)) + 
  geom_vline(aes(xintercept=30000),size=3,color="white") +
  geom_point(aes(color=continent)) +
  geom_point(aes(color=continent),size=3, shape = 1, data=paises_longevos) +
  geom_smooth(se=FALSE) + 
#  geom_vline(aes(xintercept=30000),linetype=2,size=0.5) +
  geom_text(aes(label = label), data = label, vjust = "top", hjust = "right")
```

Hagamos algunas observaciones al último gráfico: 

- Hemos creado el data.frame label para posicionar la anotación
- Para justificar, vertical y horizontalmente,  el texto correctamente hemos usado los argumentos `vjust = "top"`, `hjust = "right"` en `geom_text()`.
- Hemos añadido un salto manual de linea a la etiqueta mediante "\n"
- Hemos añadido otro elemento de anotación en el gráfico: una linea vertical mediante `geom_vline()`

Además de `geom_text()` y `geom_label()` tenemos a disposición  otras "geoms" en ggplot2 útiles para  anotar nuestros gráficos. Algunas ideas:

- Utiliza `geom_hline()` y `geom_vline()` para agregar líneas de referencia. Deben ser discretas para interferir lo menos posible con los elementos principales del gráfico. En el ejemplo he usado una linea blanca gruesa en la primera capa del gráfico. Pueden usarse tambien lineas finas y punteadas. 

- Utiliza `geom_rect()` para dibujar un rectángulo alrededor de los puntos de interés. Los límites del rectángulo están definidos por la `aes(xmin, xmax, ymin, ymax)`

- Utiliza `geom_segment()` con el argumento arrow para llamar la atención de un punto con una flecha. Utilice la estética x e y para definir la posición inicial y `xend` e `yend` para definir la ubicación de la punta.


```{r}
ggplot(NULL) + 
  geom_vline(aes(xintercept=0),color="red") + 
  geom_hline(aes(yintercept=0),color="red",linetype=2,size=0.25) +
  geom_rect(aes(xmin=-0.25,xmax=0.25,ymin=-0.25,ymax=0.25),fill=NA,color="blue") + 
  geom_segment(aes(x=-0.15,y=-0.15,xend=0,yend=0), arrow =arrow(length = unit(0.05,"npc")) , color="grey50") 
  
```

## Escalas

Otra manera de mejorar los gráficos de cara a  la comunicación es ajustar las escalas. 
Las escalas controlan el mapeo de valores de datos a elementos que podemos percibir. Normalmente, ggplot2 agrega automáticamente las escalas. 
Por ejemplo, cuando escribimos:

```{r, eval=FALSE}
ggplot(gap2007, aes(gdpPercap,lifeExp)) + 
  geom_point(aes(color=continent))
```

ggplot2 agrega automáticamente las escalas predeterminadas silenciosamente. Añade una escala para cada estética que añadimos al gráfico, en este caso `x`,`y` y `color`. El gráfico anterior es equivalente a:

```{r, eval=FALSE}
ggplot(gap2007, aes(gdpPercap,lifeExp)) + 
  geom_point(aes(color=continent)) +
  scale_x_continuous() +
  scale_y_continuous() +
  scale_colour_discrete()

```

Observa el esquema de nomenclatura para las funciones que controlan las escalas: `scale_` seguido del nombre de la estética, seguido de `_`, y del  nombre de la escala. 

Las escalas predeterminadas se nombran según el tipo de variable con el que se asocian: continuo, discreto, fecha o fecha-hora.
Hay un montón de escalas no predeterminadas que veremos a continuación.

Las escalas predeterminadas han sido cuidadosamente seleccionadas para funcionar bien con un rango amplio de datos de entrada. 
Sin embargo, es posible que, en ocasiones, queramos reemplazar los valores predeterminados por dos razones:

- Es posible que queramos  modificar algunos de los parámetros de la escala predeterminada. Esto nos permite hacer cosas como cambiar las marcas en los ejes, o las etiquetas de la leyenda.

- Es posible que queramos sustituir la escala por completo, ya que en algunas ocasiones nos puede interesar una escala diferente a la por defecto. 

### Ejes

Podemos controlar las marcas y etiquetas de los ejes mediante los argumentos `breaks` y `labels` de la función `scale_` correspondiente. Por ejemplo:

- Marcas en eje y cada 5 años y en el eje x cada 5000 \$

```{r,message=FALSE,warning=FALSE}
ggplot(gap2007, aes(gdpPercap,lifeExp)) + 
  geom_point(aes(color=continent)) +
  scale_x_continuous(breaks = seq(0, 50000, by = 5000)) +
  scale_y_continuous(breaks = seq(35, 85, by = 5))
```

- Si quisiéramos ocultar completamente las marcas de los ejes, por ejemplo por motivos de confidencialidad

```{r}
ggplot(gap2007, aes(gdpPercap,lifeExp)) + 
  geom_point(aes(color=continent)) +
  scale_x_continuous(labels = NULL) +
  scale_y_continuous(labels = NULL)


```


Otro uso de breaks es cuando tenemos pocos puntos de datos  y queremos resaltar exactamente donde se producen las observaciones. 
Por ejemplo, hagamos un gráfico que muestra cuando cada presidente de EE.UU. comenzó y terminó su mandato. Modificamos las marcas del eje x para que coincidan con el inicio del mandato de cada presidente.
El conjunto de datos `presidential` viene incluido en el paquete ggplot2. 


```{r}
presidential %>%
  mutate(id = 33 + row_number()) %>% # Creo una variable nueva para posicionar los presidentes en el eje y
  ggplot(aes(start, id)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    geom_text(aes(y=id+0.3,label=name),size=3) +
    scale_x_date(NULL, breaks = presidential$start, date_labels = "'%y")
```

En este caso, es interesante observar como la especificación  de los  breaks y  labels para variables de fecha o fecha-hora es algo diferente a las variables numéricas:

- `date_labels`: toma una especificación de formato del estilo de `format.Date` o `format.POSIXct` (teclea en la consola  `?strptime` para más información)

- `date_breaks`: (no la hemos usado) admite una cadena de texto como  “2 days” o “1 month”.

### Leyendas

Los argumentos `breaks` y  `labels` que hemos visto para ajustar los ejes  también funcionan con las leyendas, aunque con estas últimas  hay  otras modificaciones que son más habituales de utilizar.

Para controlar la posición de la leyenda, debemos usar una configuración de `theme()`. Veremos con más detalle los temas al final del capítulo, pero en resumen, controlan las partes del gráfico que no tienen que ver con los  datos. El argumento  `legend.position` controla donde se dibuja la leyenda:

```{r fig.align = "default", fig.width = 7, fig.asp = 0.5, fig.show='hold'}
base <- ggplot(gap2007, aes(gdpPercap,lifeExp)) + 
  geom_point(aes(color=continent)) 

base + theme(legend.position = "left")
base + theme(legend.position = "top")
base + theme(legend.position = "bottom")
base + theme(legend.position = "right") 
```


Puedes emplear `legend.position = "none"` para suprimir la leyenda del gráfico.

Para controlar los detalles visuales de cada leyenda, usaremos `guides()` junto con  `guide_legend()` o `guide_colourbar()`. 

El siguiente ejemplo controla dos aspectos importantes en la leyenda: primero el número de filas en la leyenda mediante nrow y por último el tamaño de los puntos y su transparencia mediante override.aes. Esto es útil cuando hagamos gráficos con un tamaño de punto muy pequeño o con transparencia alta.

```{r}
# Prueba a hacer el gráfico comentando la última linea
ggplot(gap2007, aes(gdpPercap,lifeExp)) + 
  geom_point(aes(color=continent),size=1,alpha=0.5) + 
  theme(legend.position = "bottom") +
  guides(color = guide_legend(nrow = 2, override.aes = list(size = 3,alpha=1)))

```

### Reemplazar la escala

En lugar de ajustar los detalles de una escala, es posible cambiarla por completo. 

Cuando alguna variable toma valores en ordenes de magnitud muy diferentes es útil hacer una transformación logarítmica de ellos. Esto lo podemos hacer transformando directamente los datos. 


```{r}
ggplot(gap2007, aes(pop,gdpPercap)) + 
  geom_point(aes(color=continent)) 

ggplot(gap2007, aes(log10(pop),log10(gdpPercap))) + 
  geom_point(aes(color=continent)) 
```

Sin embargo, la desventaja de esta transformación es que los ejes están ahora marcados con los valores transformados, lo que hace difícil interpretar el gráfico. En lugar de realizar la transformación en los datos, podemos hacerlo con la escala. Esto es visualmente idéntico, excepto que los ejes están etiquetados en la escala de datos original.


```{r}
ggplot(gap2007, aes(pop,gdpPercap)) + 
  geom_point(aes(color=continent)) +
    scale_x_log10() + 
  scale_y_log10()

```

### Escalas de color

Otra escala que habitualmente se personaliza  es el color. 

Cuando se colorea según los valores de una variable categórica, la escala de color discreta predeterminada selecciona los colores  uniformemente espaciados alrededor de la rueda de color. A veces esta escala es difícil de distinguir por personas con problemas de daltonismo.
Un alternativa útil son las escalas que proporciona el paquete RColorBrewer que han sido afinadas manualmente de cara a mejorar la distinción de los colores. Veamos un ejemplo

```{r, fig.show='hold',fig.width=6}
ggplot(gap2007, aes(gdpPercap,lifeExp)) + 
    geom_point(aes(color=continent)) 

ggplot(gap2007, aes(gdpPercap,lifeExp)) + 
  geom_point(aes(color=continent)) +
  scale_colour_brewer(palette = "Set1")
  
  

```


Y no olvidemos las técnicas más sencillas. Si solo hay unas pocas categorías podemos usar como estética de forma redundante la forma de los puntos. Esto asegura que el gráfico se puede interpretar en una impresión en blanco y negro

```{r}
ggplot(gap2007, aes(gdpPercap,lifeExp)) + 
  geom_point(aes(color=continent,shape=continent),size=2.5) +
  scale_colour_brewer(palette = "Set1")
```


Las escalas ColorBrewer están documentadas en <http://colorbrewer2.org/> y puedes consultar las paletas disponibles mediante el comando

```{r, include=FALSE}
RColorBrewer::display.brewer.all()
```

Cuando tengamos una relación predefinida entre valores y colores, utilizaremos `scale_colour_manual()`. Por ejemplo, si asignamos al partido presidencial un color, queremos usar el mapeo estándar de rojo para republicanos y azul para demócratas:


```{r,fig.width=8}
presidential %>%
  mutate(id = 33 + row_number()) %>%
  ggplot(aes(start, id, colour = party)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    geom_text(aes(y=id+0.3,label=name),size=3) +
    scale_colour_manual(values = c(Republican = "red", Democratic = "blue"))
    
```

Cuando el coloreado se realiza usando una variable continua, es buena idea usar `scale_colour_gradient2()`.
Eso te permite, por ejemplo, asignar  diferentes colores para valores positivos y negativos de la variable, o si deseas distinguir puntos por encima o por debajo de la media.


```{r}

ggplot(gap2007, aes(pop,lifeExp)) + 
  geom_point(aes(color=gdpPercap)) +
  scale_x_log10() + 
  scale_colour_gradient2(low="green",mid="orange",high="red4",midpoint =20000)

```



## Zoom

Hay 3 maneras de controlar los límites de un gráfico

- Ajuste manual de los datos que se representan
- Establecer los límites en cada escala
- Configuración de xlim y ylim en `coord_cartesian()`

Para ampliar una región del gráfico, generalmente es mejor utilizar `coord_cartesian()`. Compara las siguientes gráficos:

```{r fig.asp = 1, fig.align = "default", fig.width = 2.8, fig.show='hold'}

ggplot(gap2007, aes(gdpPercap,lifeExp)) + 
  geom_point() + geom_smooth() + 
  coord_cartesian(xlim = c(0, 5000))

ggplot(gap2007, aes(gdpPercap,lifeExp)) + 
  geom_point() + geom_smooth() + 
  scale_x_continuous(limits = c(0, 5000))

gap2007 %>% filter(gdpPercap<5000) %>% 
ggplot(aes(gdpPercap,lifeExp)) + 
  geom_point() + geom_smooth() 
```



La diferencia entre ellos está en:

- el primero, que usa `coord_catesian()`,  hace el ajuste sobre la nube completa de puntos y posteriormente hace el zoom en la región especificada
- los dos últimos seleccionan los puntos primero y hacen el ajuste después, difieren ligeramente en los límites precisos de los valores máximos y mínimos que muestran los ejes. 


## Temas

Finalmente, podemos personalizar los elementos del gráfico no relacionados con los datos mediante un tema. Por ejemplo el siguiente gráfico usa el tema bw, que sustituye el habitual fondo gris por uno blanco.

```{r, warning=FALSE,message=FALSE}
ggplot(gap2007, aes(gdpPercap,lifeExp)) + 
  geom_point(aes(color=continent)) + geom_smooth(se=FALSE) +
  theme_bw()

```

ggplot2 incluye ocho temas de forma predeterminada. Muchos más se incluyen en paquetes adicionales como ggthemes

![Temas incluidos en el paquete ggplot2](img/visualization-themes.png)

Hay otras características de apariencia del gráfico que pueden cambiarse mediante argumentos de la función `theme()`
Una modificación muy habitual es cambiar la orientación de las etiquetas del eje x cuando corresponden a una variable categórica


```{r}
gap2007 %>% sample_n(20) %>% 
ggplot() + geom_bar(aes(reorder(country,lifeExp),lifeExp,fill=continent),stat="identity") + 
  theme_bw() + theme(legend.position = 'none', axis.text.x = element_text(angle=90,size=8))+
  labs(x="")
```

# R markdown

R Markdown es un entorno de de edición de texto que permite combinar código, resultados y texto con formato.

Este tipo de documentos se emplean habitualmente con tres propósitos diferentes

1. Para comunicar resultados de manera formal, ya sea para un informe/presentación puntual como para generar documentación automatizada de forma periódica. En este caso, se prioriza presentar los resultados y las conclusiones y no el código que hay detrás de los análisis.

1. Para colaborar con otros analistas o científicos de datos, que están interesados en las conclusiones y como se llego a ellas, es decir el código

1. Como cuaderno de laboratorio del científico de datos, en que capturemos no solo lo que hicimos sino también lo que pensábamos, los análisis exitosos o los menos satisfactorios.  

**Autotexto Nota**
Para ti R markdown no es algo nuevo, la documentación de este curso está desarrollada en R markdown y a estas alturas de curso ya habrás abierto varios documentos .Rmd para ejecutar el código que acompaña a nuestras explicaciones. Ahora vas a aprender a crear este tipo de documentos desde cero. 
**Fin Autotexto**
 

## Fundamentos de R markdown

Para empezar a generar documentos con R markdown solo necesitas el paquete rmarkdown. Aunque desde Rstudio, no es necesario que lo instales, ni lo cargues explícitamente, él se encarga de hacerlo cuando lo necesites.

Un fichero de R Markdown es un fichero de texto plano con la  extensión .Rmd. Veamos un ejemplo

```{r, echo = FALSE, comment = ""}
cat(readr::read_file("Ejemplos/ej_rmarkdown.Rmd"))
```

Contiene tres tipos distintos de contenido: 

- Una cabecera YAML (opcional) delimitada por`---` .
- Bloques (chunks) de código delimitados por ```` ``` ````.
- Texto que mezcla texto simple con texto formateado como `# Título` y `__negrita__`.

Cuando abres un fichero .Rmd en Rstudio lo que ves es una interfaz tipo notebook en donde el código y el texto están entrelazados, tal y como muestra la siguiente figura

![](img/captura_rmd.png)

<br>

Puedes ejecutar los chunks de código pulsando el icono de Run que tienes encima de cada chunk (el que parece un botón de play) o  tecleando `Ctrl + Shift + Enter`. Rstudio ejecuta el código y mostrará los resultados justo debajo.



Para producir un informe completo que contenga todo el texto, código y resultados, haz clic en "Knit" o presiona `Ctrl + Mayús + K`. También puede ejecutarse en la consola escribiendo ` rmarkdown::render("tufichero.Rmd") `. 

Esto mostrará el informe en un visor y creará fichero HTML (o el formato especificado) que puedes abrir en tu navegador y compartir con otras personas.


![](img/Captura_rmarkdown_viewer.png)

<br>

Cuando se procesa el documento R Markdown envía el archivo .Rmd a knitr (<http://yihui.name/knitr/>), que ejecuta todos los fragmentos de código y crea un nuevo documento de markdown (.md) que incluye el código y su salida. El archivo de markdown generado por knitr es procesado por pandoc ( <http://pandoc.org/>), que es el responsable de crear el archivo finalizado. La ventaja de este flujo de trabajo en dos pasos es que se puede crear una amplia gama de formatos de salida para un mismo documento, como veremos más adelante

![Fuente:R for Data Science, Garrett Grolemund y Hadley Wickham](img/RMarkdownFlow.png)
<br>

Puedes comenzar por tu cuenta a crear un fichero .Rmd. Selecciona `File -> New File -> R Markdown`  en la barra de menús. 
RStudio creara un documento plantilla con contenido útil que le recuerda cómo funcionan las características principales de R Markdown.

![](img/plantilla_rmarkdown.png)

## Texto en Markdown 

El texto  en archivos .Rmd está escrito en Markdown, es un lenguaje de marcado ligero que proporciona un conjunto pequeño de instrucciones para dar formato al texto desde un fichero de texto plano. Markdown está pensado para que se pueda leer y escribir con facilidad. A continuación te mostramos una pequeña guía de como usar Markdown

```
Formato de texto 
------------------------------------------------------------
*cursiva*  or _cursiva_
**negrita**   __así también en negrita__
`código`
superíndice^2^ and subscript~2~


Títulos
------------------------------------------------------------

# Título Nivel 1

## Título Nivel 2

### Título Nivel 3


Listas
------------------------------------------------------------

*   Lista con viñetas -  Elemento  1

*   Elemento 2

    * Elemento 2a

    * Elemento 2b


1.  Lista numerada -  Elemento  1

1.  Elemento 2. Los números se incrementan automaticamente en la salida.


Links and images
------------------------------------------------------------

<http://ejemplo.es>

[Texto del hipervínculo](http://ejemplo.es)

![Texto al pie de imagen, opcional](path/to/img.png)


Tablas
------------------------------------------------------------

Cabecera 1    | Cabecera 2
------------- | -------------
aaaa          | 1.23
bbb           | 5.67

```

Puedes obtener ayuda sobre los comando básicos de Markdow en Rstudio mediante el menú Help -> Markdown Quick Reference


**Autotexto Reto **
Crea un nuevo fichero de Markdown e introduce todos los elementos de Markdown mostrados arriba. Compílalo pulsando el botón de Knit y observa el documento resultante. Ten cuidado si insertas imágenes que se encuentren en la ruta especificada
**Fin autotexto **

## Fragmentos de código (Code chunks)

Para ejecutar código dentro de un documento de Markdown de R, debes insertar un fragmento de código(chunk). 
Hay tres formas de hacerlo:

- El atajo de teclado `Ctrl + Alt + I`

- El botón "Insert" en la barra de herramientas

- Escribiendo manualmente ` ```{r} ` y  ` ``` `.

Puedes ejecutar el código del chunk, comando a comando, mediante el atajo de teclado que ya conoces `Ctrl + Enter`.
Para ejecutar el bloque completo debes usar `Ctrl + Shift + Enter` o el icono parecido al "Play"" situado en la parte superior derecha del chunk.

El contenido de cada fragmento debe estar centrado en una única tarea. Si quieres hacer más de una cosa inserta diferentes chunks, así estará mejor organizado tu documento. 


### Nombre del chunk

Es posible dar un nombre (opcional) a los chunks mediante: ```` ```{r nombrechunk}``` ````. Esto tiene dos ventajas aparentes:

- Es más fácil navegar sobre los chunks mediante el desplegable de la parte inferior izquierda del editor
- Los gráficos que producen los chunks tienen nombres de fichero fáciles de identificar para poder utilizarlos en otra parte.


### Opciones del chunk

La salida de los chunks se puede personalizar mediante  opciones, es decir mediante argumentos suministrados en la cabecera de chunk. 

Knitr proporciona casi 60 opciones que pueden utilizar para personalizar los chunks de código. 

Aquí  vamos a ver las opciones más importantes, las que controlan si se ejecuta el bloque de código y qué resultados se insertan en el documento final:


- `eval = FALSE` evita que se evalúe el código. (Y, obviamente, si el código no se ejecuta, no se generarán resultados). Esto es útil para mostrar código de ejemplo.

- `include = FALSE` ejecuta el código, pero no muestra el código o los resultados en el documento final. Utilízalo para el código de configuración o pre-cálculos necesarios para el informe.

- `echo = FALSE` no muestra  el código, pero si los resultados. Utilízalo  cuando redactes informes dirigidos a personas que no deseen ver el código R subyacente.

- `message = FALSE` o `warning = FALSE` impide que los mensajes o warnings  provocados en la ejecución  del código R aparezcan en el documento  final..

- `results = 'hide'` oculta la salida impresa y `fig.show = 'hide'` no muestra  los gráficos.

- `error = TRUE` hace que el procesado continúe aunque el código ejecutado devuelva un error. Esto no es lo deseable en la versión final de tu informe, pero puede ser muy útil si necesitas depurar exactamente lo que está pasando dentro de tu .Rmd.

También se controlan mediante argumentos de los chunks las características de los gráficos que genera el código. A esto dedicamos una sección del texto más adelante.

Puedes ver la lista completa de opciones en  <http://yihui.name/knitr/options/>

### Tablas 

De forma predeterminada, R Markdown imprime los data frames y las matrices como los verías en la consola:

```{r}
mtcars[1:5, ]
```

Si prefieres que los datos se muestren con mejor formato, utilizaremos la función `knitr :: kable`. 
El código siguiente genera la siguiente tabla:

```{r kable}
knitr::kable(mtcars [1:5,1:9], caption = "tabla formateada por  kable")
```

Si lees la ayuda  `?knitr::kable` podrás ver muchas opciones en las que puedes personalizar la tabla.


### Gráficos

Cuando un chunk de código devuelve una figura se mostrará en el documento final. Sin embargo controlar el tamaño y la alineación final para que se muestre de la manera deseada puede ser a veces difícil.

Si no nos gustan los resultados por defecto, podemos controlar el tamaño de la figura mediante 5 parámetros del chunk que podemos utilizar: `fig.width, fig.height, fig.asp, out.width` y `out.height`. Pero no es necesario modificar todos a la vez:

- Hay que distinguir entre el tamaño de la figura que crea R (`fig.width` y `fig.height`) y el tamaño en que la inserta en el documento final (`out.width` y `out.height`). `fig.width` y `fig.height` toman un valor numérico y representa el tamaño en pulgadas. Por defecto vale 7. Los parámetros `out.width` y `out.height` son más flexibles, pueden ser un porcentaje del ancho del documento, `out.width="100%"` o un valor numérico con unidades `out.height="300 px"`

- `fig.asp` controla el ratio entre la altura y la anchura de la gráfica. Si esta fijado la `fig.heigth` de calcula como `fig.width*fig.asp`.

- `out.width` y `out.height` funcionan para documentos html o pdf, pero no para  documentos word o .odt
 
- Si fijamos `out.width ="90%"`, entonces cambiar los valores de fig.width cambia el tamaño del texto y los elementos del gráfico respecto al marco global. 
 
Otras opciones importantes a tener en cuenta en los chunks que devuelven imágenes son:

- `fig.align`: controla la alineación de la figura en el documento. `fig.align="center"` mostrará la figura centrada.

- `fig.show='hold'` permite en un chunk que muestra varios gráficos que estos se muestren después del código y no intercalados.

- `fig.cap = "texto"` : Permite insertar un caption a la figura, es decir texto explicativo en la parte inferior. 

- `dev` : especificamos el formato de la imagen del gráfico generada. Por ejemplo si la salida es html, `dev="png"` por defecto. Sin embargo si la salida es un documento PDF `dev="pdf"` por defecto. Si un gráfico contiene muchos puntos, en formato pdf ocupará mucho y tardará mucho tiempo en cargarse, en ese caso es recomendable cambiar en el chunk el formato de salida haciendo dev="png". 

- Es una buena idea nombrar chunks de código que producen gráficos, incluso si no  etiquetamos rutinariamente otros chunks. La etiqueta del chunk se utiliza para generar el nombre de archivo del gráfico en el disco, así que de esta manera es más fácil identificar los ficheros del gráfico y reutilizarlos en otro contexto, como un email o un tweet.



### Opciones globales

A medida que trabajes más con knitr, descubrirás que algunas de las opciones predeterminadas de los chunks no se adaptan a tus necesidades y querrás cambiarlas. Puedes hacerlo  mediante `knitr::opts_chunk$set()` en un fragmento de código. 

Por ejemplo,  si estás preparando un informe y quieres ocultar todo el código, puedes establecer:

```{r eval = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

Esto ocultará el código de forma predeterminada, por lo que solo mostrará los bloques de código que deliberadamente elijas mostrar (con echo = TRUE). 

### Código en linea

Hay otra forma de insertar código R en un documento de R Markdown: directamente en el texto, con: `r inline('codigo')`. 
En el ejemplo que pusimos al principio del capítulo, decía:

> El __pais más poblado__ en el año 2007 es `r inline('data2007$country[which.max(data2007$pop)]')` 
> con una población de  `r inline('max(data2007$pop)') `  habitantes.

que después de procesarse queda en: 

> El pais más poblado en el año 2007 es China 
> con una población de 1318683096 habitantes

R realiza los cálculos e incrusta los resultados en linea con el texto. 

Cuando introduzcamos cálculos en linea que puedan dar como resultado números con decimales debemos usar format o round para controlar el número de decimales que se imprimen en pantalla.  Veamos la diferencia

> El número pi es `r inline('pi')`, se redondea con tres cifras decimales a  `r inline('round(pi,3)')` 
> y si queremos limitar las cifras significativas a dos `r inline('format(pi,digits=2')`

al procesarlo resulta 

> El número pi es `r pi`, se redondea con tres cifras decimales a  `r round(pi,3)` 
> y si queremos limitar las cifras significativas a dos `r format(pi,digits=2)`


### Ecuaciones matemáticas 

**(Esta sección podría ir en autotexto para alumnos top)**

Podemos escribir ecuaciones matemáticas usando la sintaxis del sistema de edición de textos científicos Latex (<https://en.wikibooks.org/wiki/LaTeX/Mathematics> te puede servir como referencia rápida para escribir ecuaciones en este sistema)

Podemos escribir ecuaciones en linea ` $\frac{1}{n}\sum_{i}x^{2}_{i}$ ` o como objeto propio.


`$$
f(x) = \frac{A_0}{\sqrt{1+x^3}}
$$ `

Que una vez procesado el documento se visualizaría como:

La ecuación en linea se ve así  $\frac{1}{n}\sum_{i}x^{2}_{i}$ y la independiente:

$$
f(x) = \frac{A_0}{\sqrt{1+x^3}}
$$


## Formatos de salida

Para controlar algunos detalles del documento de salida usaremos la cabecera YAML.
YAML significa "yet another markup language"" (otro lenguaje de marcado más), que está diseñado para representar datos jerárquicos de una manera fácil de leer y escribir. 

Hasta ahora los ejemplos de documentos de R Markdown que hemos dado creaban un documento de salida en html. Hay dos maneras de cambiar el formato de salida:

1.  Permanentemente, modificando la  cabecera YAML: 
    
    ```yaml
    title: "Demo R markdown"
    output: pdf_document
    ```
    
1.  De forma transitoria ejecutando  `rmarkdown::render()` a mano en la consola:
    
    ```{r eval = FALSE}
    rmarkdown::render("ej_rmarkdown.Rmd", output_format = "word_document")
    ```
    
    Esta forma es útil cuando queremos, mediante un programa, generar documentos en múltiples formatos.


El botón Knit de Rstudio procesa el fichero al primer formato especificado en el campo 'output' de la cabecera. Pero es posible procesar a otros formatos mediante el menú desplegable al lado del botón Knit

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("img/boton_knit.png")
```

Es posible cambiar la configuración por defecto de los documentos  de salida mediante un campo `output` expandido. 
Por ejemplo, si quieres  crear un `html_document` con una tabla de índice flotante, utiliza:

```yaml
output:
  html_document:
    toc: true
    toc_float: true
```

también puedes especificar opciones para formatos múltiples:

```yaml
output:
  html_document:
    toc: true
    toc_float: true
  pdf_document: default
```
Fíjate en la sintaxis especial si no quieres modificar los parámetros por defecto en tipo de documento, una vez que hemos expandido el campo output. 

### Documentos
 
Los principales formatos de salida son: 

*   `html_document` crea un documento html. Es la opción por defecto.

*   `pdf_document` crea un documento  PDF mediante LaTeX (es un sistema para creación de documentación científica), que necesitarás instalar si no lo has hecho anteriormente. RStudio te avisará en caso de necesitarlo.
  
*   `word_document` para documentos de  Microsoft Word (`.docx`).
  
*   `odt_document` para documentos en formato OpenDocument Text  (`.odt`).
  
*   `rtf_document` para documentos de texto enriquecido (`.rtf`).
  

Recuerda que si quieres esconder todo el código para el documento final lo puedes hacer cambiando una opción global de knitr mediante un chunk de código


```{r, eval = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

Si la salida es en html, hay otra opción que es hacer que los chunks de código estén ocultos inicialmente pero puedan hacerse visibles con un click.

```yaml
output:
  html_document:
    code_folding: hide
```

### Notebooks

Un notebook, `html_notebook`, es una variante de  `html_document`. La salida procesada es muy parecida pero el propósito es diferente. 
Un `html_document` se centra en la comunicación con el público general,  mientras que un notebook  se centra en la colaboración con otros científicos de datos. El documento de salida del notebook es un documento  `.nb.html` que aparentemente es igual al del documento html pero que se puede utilizar de dos formas:

1. Lo puedes abrir en un explorador web y ver la salida procesada. Pero, al contrario que un 
   `html_document`, este procesado siempre incluye una copia del código que ha generado el documento
1. Puedes abrirlo y editarlo desde  RStudio. Cuando abres un fichero  `.nb.html`, RStudio crea el fichero .Rmd que lo generó.

### Presentaciones

También se puede usar R Markdown para crear presentaciones. 
Se obtiene menos control visual que con una herramienta como PowerPoint, pero insertar  automáticamente  los resultados de tu código R en una presentación te puede ahorrar una gran cantidad de tiempo. 

Las presentaciones funcionan dividiendo su contenido en diapositivas, con una nueva diapositiva comenzando en cada primer (`#`) o segundo (`##`) encabezado de nivel. 

R Markdown viene con tres formatos de presentaciones incorporados:

1. `ioslides_presentation` - Presentación HTML con ioslides

1. `slidy_presentation` - Presentación HTML con W3C Slidy

1. `beamer_presentation` - Presentación en PDF con LaTeX Beamer.

Otros formatos populares se proporcionan a través de  paquetes, como por ejemplo:

* `revealjs :: revealjs_presentation` - Presentación en HTML con reveal.js.
    Requiere el paquete __revealjs__. Es el usamos para las presentaciones en este curso

Puedes crear un fichero de presentación mediante File -> New File -> R Markdown, eligiendo como tipo de formato "Presentation" y el motor preferido. Para una presentación usando reveal.js debes elegir como  formato "From Template" y elegir la plantilla de reveal.js, siempre tengamos el paquete instalado.  

### Otros formatos

Es posible generar otros formatos de salida como: 

- Cuadros de mando (Dashboards)
- Libros
- Artículos de revistas científicas
- Sitios web

Y puedes añadir elementos interactivos mediante shiny <http://shiny.rstudio.com/>.

Esto no está contenido en este curso por razones de tiempo pero te animamos a que los explores. Puedes empezar leyendo el capitulo dedicado en el libro R for Data Science <http://r4ds.had.co.nz/r-markdown-formats.html>

# ¿Qué has aprendido?

En esta unidad has aprendido a elaborar documentos y graficos para comunicar de forma efectiva y vistosa tus resultados. 

La forma más facil de transmitir resultados cuantitativos es mediante buenos gráficos, y a ello hemos dedicado gran parte de esta unidad. 

Aquí te hemos proporcionado una introdución a algunas herramientas y te hemos dado buenos consejos que te van a ayudar a comunicar mejor los resultados que obtegas en proyectos de ciencia de datos, pero la maestría la alcanzarás experimentando por tu cuenta y mediante la experiencia.  

Además recuerda que has aprendido el manejo de herramientas que permiten: 

- Que tu  trabajo sea reproducible,  por ti mismos pasado el tiempo, o por otras personas.
- Automatizar tareas: podrás programar informes, dashboards o sitios web que se actualicen automáticamente a partir de una única plantilla.



# Bibliografía

- R for Data Science, Garrett Grolemund, Hadley Wickham. O’Reilly (2016)
  Disponible online en  <http://r4ds.had.co.nz>

- Authoring Books and Technical Documents with R Markdown, Yihui Xie: <https://bookdown.org/yihui/bookdown/>

- Documentación oficial de R markdown: <http://rmarkdown.rstudio.com/>

- Documentación oficial Knitr: <https://yihui.name/knitr/>
  
- R Markdown Cheat Sheet: <https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf> 



---
title: "Inferencia Estadistica"
author: "MasterD"
output:
  pdf_document:
    toc: yes
  html_document:
    highlight: tango
    theme: united
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
  revealjs::revealjs_presentation:
    center: yes
    css: style.css
    highlight: pygments
    theme: sky
  word_document:
    reference_docx: www/plantillaMasterD_basica5.docx
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
options(width=100)

library(reshape2)
library(dplyr)
library(ggplot2)

```

# Tu reto en esta unidad

Si has cursado alguna vez un curso de Estadística a nivel superior, seguramente habrás aprendido a hacer inferencia estadística aplicando una serie de recetas, basadas en una teoría matemática bastante complicada, que se aplican sin entender muy bien el significado de lo que se está haciendo.

En esta unidad vas a aprender a hacer inferencia estadística de una manera intuitiva, a partir de la simulación mediante un ordenador de procesos de muestreo y remuestreo. Este es el enfoque apropiado para el siglo XXI en el que podemos aprovecharnos de las herramientas de computación que no disponían los padres de la estadística clásica en siglos anteriores. 


# Introducción

El objetivo  de la inferencia estadística es obtener conclusiones sobre las características de una población basadas en medidas realizadas sobre una muestra (frecuentemente pequeña) de esta. 

La forma más sencilla de entender la problemática es mediante algún ejemplo: 

- Un conjunto de 50 pacientes con tasas altas de colesterol reciben dos tratamientos distintos. El objetivo es determinar si la variación de la concentración de colesterol en sangre es mayor en un grupo que en otro, mas allá de las posibles diferencias que pueden aparecer al dividir a los 50 pacientes, al azar en dos grupos de 25.

- A partir de una encuesta de intención de voto a 1000 personas, determinar unas predicciones fiables del resultado de las elecciones. El objetivo es determinar unas tasas de error a las predicciones debido a la elección de esas 1000 personas y no de otras 1000 diferentes. 

- Determinar si los resultados de una determinada estrategia en un juego  son superiores a los que se esperaría por puro azar. Por ejemplo, un amigo afirma que es capaz de adivinar el color de una carta de una  baraja francesa antes de voltear la carta. Se realiza un experimento en el que saca 10  cartas de las que adivina 8 veces el color. ¿Es posible que nuestro amigo tenga facultades adivinatorias? Para ello debemos estimar como es de probable obtener ese resultado eligiendo el color al azar.

![](img/workflow_statistics.png)

Tradicionalmente estos problemas se han resuelto aplicando la teoría de la probabilidad al problema del muestreo y elaborando una serie de recetas que constituyen lo que se denomina Contrastes de Hipótesis. En este caso:

- Para el problema del tratamiento para el colesterol aplicaríamos un t-test
- Para la estimación de los errores en la encuesta usaríamos la receta para calcular intervalos de confianza en proporciones
- Para la adivinación de cartas aplicaríamos un test $\chi^2$

Este tratamiento tradicional presenta varias desventajas:

- Es necesario hacer asunciones acerca de la distribución de los datos y los tamaños de las muestras. 
- Las demostraciones teóricas de los resultados son difíciles
- Se aprenden como una serie de recetas mecánicas que se aplican perdiendo en muchas ocasiones las intuiciones acerca del problema.
- En ocasiones  se aplican, incluso en literatura científica de muy alto nivel, de manera errónea produciendo resultados incorrectos y no reproducibles

En el enfoque moderno de la estadística inferencial, se sustituyen la teoría de la probabilidad y las derivaciones teóricas por simulaciones de re-muestreo y reordenación de los datos que nos permiten llegar a resultados equivalentes de una forma más intuitiva.
Evidentemente, a finales del siglo XIX y principios del siglo XX, cuando se desarrollaron los principios de la inferencia estadística, los científicos no disponían de los ordenadores que tenemos ahora, así que vamos a aprovechar esta circunstancia.


# Muestreos

La teoría de la inferencia, se basa en el siguiente esquema

- Si partimos de una población con características conocidas. Por ejemplo disponemos del censo de alturas de personas de un país.
- Estudiamos como varía una magnitud, por ejemplo la media de las alturas, a lo largo de diferentes muestras aleatorias de tamaño finito. Es decir tomamos un muestra de 50 personas y registramos su altura media, digamos 169 centímetros. Tomamos otra muestra, y registramos la altura media (167 cm) y así sucesivamente un número muy elevado de veces.
- A partir de lo anterior conocemos la distribución de muestral de la magnitud a estudiar (en este caso la media muestral) y podemos hacer inferencias para una única muestra como por ejemplo determinar un intervalo de confianza. 

Para ello debemos tener claro lo que son los procesos de muestreo, re-muestreo y reordenación, que es lo que vamos a hacer en esta sección.


## Cocinando un potaje

Una forma sencilla y visual de entender un proceso de muestreo e inferencia es compararlo con probar una sopa o un potaje que estamos cocinando. 
Imaginemos que hemos invitado a unos amigos a comer y estamos cocinando una nueva receta de  potaje de garbanzos, con verduras, chorizo y morcilla. A partir de probar una cucharada queremos responder a algunas preguntas como:

- ¿Están bien cocidos los garbanzos?
- ¿Están demasiado pasadas la verduras?
- ¿Tiene sabor picante?
- ¿Es muy graso?
- ¿Se ha oscurecido el caldo debido a la morcilla desecha?

¿Cómo podemos responder a estas preguntas? ¿Importa de dónde cogemos la cucharada? 
¿Hay algo que deberíamos hacer con el potaje antes de probar? ¿Es suficiente con probar una sola vez? ¿Importa el tamaño de la cuchara?

Veamos ahora una serie de definiciones de conceptos relevantes en los procesos de muestreo e inferencia y veamos como aplican al ejemplo de la cata del potaje. 

- **Población**: la población es el conjunto de todas las observaciones que nos interesan. 
- **Muestra**: Es una colección de observaciones más pequeña que se selecciona de la población. 

- **Muestreo**: se refiere al proceso de selección de observaciones de una población. Se puede hacer de forma  aleatoria o no aleatoria.

- **Muestra representativa**: se dice que una muestra es una muestra representativa si las características las observaciones seleccionadas son una buena aproximación de las características de la población original.

- **Sesgo**: corresponde a la prevalencia de un grupo en una población sobre otro grupo en una muestra

- **Generalizabilidad**: se refiere al grupo más grande en el que tiene sentido hacer inferencias sobre la muestra recogida. Esto está directamente relacionado con cómo se seleccionó la muestra.

- **Parámetro**: es un cálculo basado en una o más variables medidas en la población. Los parámetros se indican casi siempre simbólicamente usando letras griegas tales como $\mu$ para la media , $\sigma$ para la desviación típica o $\pi$ para una proporción.

- **Estadístico**: es un cálculo basado en una o más variables medidas en la muestra. Los estadísticos se denotan generalmente por letras minúsculas latinas. Por ejemplo $\bar{x}$ para la media , $s$ para la desviación típica o $p$ para una proporción.

Veamos en el ejemplo de la prueba del potaje a que se corresponden estos conceptos: 

- **Población**: el puchero completo en el  que hemos cocinado el potaje.

- **Muestra**:  cualquier porción menor de guiso recogido que no es el recipiente entero.  Podríamos decir que cada cucharada representa una muestra. Un plato sería una muestra de mayor tamaño. 

- **Muestreo:** el proceso de selección de cucharadas del recipiente de potaje.

- **Muestra representativa**:  una muestra que seleccionemos sólo será representativa si sabe a lo que  sabe el potaje en general. 
Si seleccionamos una cucharada sin verdura o con trozo de chorizo más picante que el resto, no tendríamos una muestra representativa.

- **Sesgo**: como hemos observado en el punto anterior, podemos seleccionar una muestra que no sea representativa. Si tú cocinas con frecuencia, seguramente sabrás remover bien el potaje antes de probarlo. 

- **Generalizabilidad**: si removemos bien el potaje  antes de probar una cucharada (y nos aseguramos de no solo escoger nuestro ingrediente favorito), los resultados de nuestra muestra se pueden generalizar al guiso completo. Seguramente después de un par de cucharadas, podemos estar bastante seguros de cuál es el sabor de cada plato de potaje que se coman nuestros amigos. 

- **Parámetro**: un ejemplo aquí podría ser la proporción de pimentón que entró en la olla entera, medir la salinidad del guiso en promedio, o el punto de dureza medio de los los garbanzos. 

- **Estadístico**: para convertir un parámetro a un estadístico, solo tienes que  pensar en la misma medida aplicada a una cucharada.


## Visualización de muestras

Cargamos los datos *body* con los que hemos trabajado en unidades anteriores

```{r}
url="http://ww2.amstat.org/publications/jse/datasets/body.dat.txt"
body <- read.table(url)
BodyMeasurements <- c("Biacromial_diameter","Biiliac_diameter","Bitrochanteric_diameter","Chest_depth","Chest_diameter","Elbow_diameter","Wrist_diameter","Knee_diameter","Ankle_diameter","Shoulder_girth","Chest_girth","Waist_girth","Navel_girth","Hip_girth","Thigh_girth","Bicep_girth","Forearm_girth","Knee_girth","Calf_max_girth","Ankle_min_girth","Wrist_min_girth","Age","Weight","Height","Gender")    
names(body) <- BodyMeasurements 
```

Veamos la distribución de alturas de la población mediante un histograma

```{r}
ggplot(body,aes(Height)) + geom_histogram(color="white",binwidth = 5)
```

Tomemos ahora dos muestras aleatorias de 50 elementos y representemos su histograma junto con el de la población y veamos si las muestras representan las características de la población

```{r}
set.seed(1234)
body_sample1 <- body %>% sample_n(50) 
body_sample2 <- body %>% sample_n(50) 
tmp <- rbind(cbind(set="sample1",body_sample1),
             cbind(set="sample2",body_sample2),
             cbind(set="pop",body))

ggplot(tmp,aes(Height, y=..density..,fill=set)) + geom_histogram(binwidth = 5,color="white") + facet_wrap(~set) + 
  theme(legend.position = "none")

```


Está claro que una muestra nunca puede emular completamente las características de la población, pero en este caso se observa que la forma de las distribuciones de alturas de ambas muestras recuerdan bastante la forma de la distribución de la población

Ahora vamos a elegir una muestra sesgada, tomamos 50 individuos de sexo femenino. Veamos como en este caso el histograma es claramente distinto al de la población que contiene tanto hombres como mujeres.

```{r}
set.seed(123)
fem_sample1 <- body %>% filter(Gender==0) %>% sample_n(50) 
tmp <- rbind(cbind(set="female sample",fem_sample1),
             cbind(set="population",body))

ggplot(tmp,aes(Height, y=..density..,fill=set)) + geom_histogram(binwidth = 5,color="white",position="identity" , alpha=0.5)  + 
  theme(legend.position = "right")
```

Este es un ejemplo claro de una muestra no representativa y sesgada. 

## Distribución muestral

Supongamos ahora que estamos interesados en estimar la altura media de la población a partir de los datos de una muestra.

Para las 3 muestras que hemos obtenido en la sección anterior tenemos 

```{r}
tmp <- rbind(cbind(set="sample1",body_sample1),
             cbind(set="sample2",body_sample2),
             cbind(set="fem sample",fem_sample1),
             cbind(set="pop",body))

tmp %>% group_by(set) %>% summarise(mean=mean(Height))
```

Observamos que las medias de las dos primeras muestras son razonablemente parecidas a la de la población mientras que la de la muestra de mujeres está claramente por debajo. 

Debemos darnos cuenta que incluso cuando trabajamos con muestras aleatorias, existe una variabilidad en los estadísticos que podemos medir en ellas (en este caso la media). Está variabilidad entre muestras de denomina **distribución muestral**.

Vamos a hacer un experimento para obtener la distribución muestral de la media de alturas para diferentes tamaños de muestra

```{r, cache=TRUE}
sample_sizes=c(10,20,50,100)
meansample_distr <- NULL
set.seed(1234)
for(n in sample_sizes){ # Bucle a los diferentes tamaños de muestra elegidos
  # Genero 1000 muestras de tamaño n y calculo la media
  msampl <- mosaic::do(1000) * 
    body %>% sample_n(n) %>% summarise(mean=mean(Height))
  # Almaceno en un data frame para todos los tamaños de muestra, especificando el tamaño de la muestra
  meansample_distr <- rbind(meansample_distr,cbind(n=n,msampl))
}

ggplot(meansample_distr%>% mutate(n=round(n)),aes(mean, fill =n )) +
  geom_histogram(binwidth=1 , alpha=0.5, position="identity" , data=meansample_distr%>%filter(n==10)) +
  geom_histogram(binwidth=1 , alpha=0.5, position="identity" , data=meansample_distr%>%filter(n==20))+
  geom_histogram(binwidth=1 , alpha=0.5, position="identity" , data=meansample_distr%>%filter(n==50)) +
  geom_histogram(binwidth=1 , alpha=0.5, position="identity" , data=meansample_distr%>%filter(n==100))

```

Nota que hemos utilizado la función `do` del paquete mosaic (si no lo tienes seguro que ya sabes cómo instalarlo) para repetir 1000 veces la tarea de obtener una muestra y calcular su media. Fíjate en la notación particular de esta función 

```
mosaic::do(1000) * 
    (body %>% sample_n(n) %>% summarise(mean=mean(Height)))
```
do recibe como argumento el número de veces que queremos repetir el cálculo y repite ese número de veces la expresión situada a la derecha del operador *. Finalmente concatena los resultados obtenidos.


En cuanto a las distribuciones de la media muestral observamos que: 

- Las distribuciones son simétricas y tienen forma de campana (lo que formalmente se denomina distribución Normal o Gaussiana). 
- Independientemente del tamaño de la muestra todas está centradas aproximadamente en el mismo valor (la media de la población)
- La dispersión de la distribución decrece al aumentar el tamaño de la muestra


Para confirmar los dos últimos puntos, veamos de forma numérica las medias y las desviaciones típicas de las distribuciones muestrales

```{r, results='asis'}
summ_sampldistr <- meansample_distr %>% group_by(n) %>% summarise(m=mean(mean),sd=sd(mean)) 
knitr::kable(summ_sampldistr)
```


Los resultados que hemos observado se pueden demostrar rigurosamente mediante el **Teorema del Límite Central**, uno de los resultados más importantes de la estadística inferencial. En este curso no lo vamos a demostrar aunque ya lo hemos comprobado mediante el resultado del experimento

El Teorema del Límite Central establece que:

- La distribución muestral de la media de una población (con media $\mu$ y varianza $\sigma$ finitas) se aproxima a una distribución normal, para tamaños muestrales suficientemente grandes.

- La media de la distribución de la media muestral será $\mu_{\bar{x}}=\mu$ y la desviación típica  $\sigma_{\bar{x}}=\frac{\sigma}{\sqrt{n}}$


Observad que no se dice nada sobre la distribución de la población, solo se pide que tenga media y varianza finita. Este punto es muy importante, aunque la distribución de la población sea muy asimétrica, para tamaños de muestra suficientemente grandes la distribución de la media muestral seguirá una distribución Normal. 


Para una descripción más precisa de la distribución Normal y sobre distribuciones de probabilidad en general te recomendamos que leas el apéndice I de esta unidad.

Veamos gráficamente, que en nuestro experimento se cumple la relación entre la desviación típica de la media muestral, la desviación típica de la población y el tamaño de la muestra $\sigma_{\bar{x}} = \frac{\sigma}{n}$.

```{r}
ggplot(summ_sampldistr,aes(1/sqrt(n),sd)) + geom_point() + geom_smooth(method="lm",se=FALSE)
```

# Contraste de Hipótesis

De manera formal los métodos de inferencia estadística se pueden resumir en:

- Contraste de Hipótesis
- Estimación de Intervalos de Confianza

El objetivo de ambas técnicas, en esencia, es poder confrontar el valor de un estadístico obtenido para una muestra con una hipótesis sobre su valor en la población y determinar si las diferencias observadas son compatibles con el azar debido al proceso de muestreo o si sin embargo este azar no es capaz de explicar la diferencia. 

Existen un amplio número de métodos formales que seguramente habrás estudiado alguna vez o al menos oído hablar de ellos como son el t-test, z-test, test chi cuadrado, anova, etc. 

En este curso, en lugar de presentarte un montón de diferentes fórmulas, recetas y escenarios, queremos  elaborar una manera de pensar común  para  las pruebas de hipótesis. Posteriormente, podrás adaptarla a diferentes escenarios según sea necesario en función de las diferentes situaciones.


## Fundamentos del Contraste de Hipótesis

En una contraste de hipótesis, vamos a utilizar los datos de una muestra para ayudarnos a decidir entre dos hipótesis competitivas sobre el valor de un parámetro de una población (pensemos por ejemplo en la media de una variable).
Nos referimos a las afirmaciones competitivas sobre la población como:

- Hipótesis nula, denotada por $H_0$
- Hipótesis alternativa, denotada por $H_a$. 

Los roles de estas dos hipótesis NO son intercambiables:

- La afirmación para la que buscamos pruebas significativas se asigna a la hipótesis alternativa. La alternativa es generalmente lo investigador quiere establecer o encontrar evidencia a favor

- Por lo general, la hipótesis nula es una afirmación de que realmente no hay efecto o no hay diferencia. En muchos casos, la hipótesis nula representa el "status quo" o que nada interesante está sucediendo.

- Evaluamos la fuerza de la evidencia asumiendo que la hipótesis nula es verdadera y determinando cuan improbable sería ver resultados muestrales tan  extremos (o más) como los de la muestra original.

El uso de las pruebas de hipótesis ha provocado y provoca  muchas  interpretaciones incorrectas en la comunidad científica y en la sociedad en general. Una de las razones de esto es que se ha realizado tradicionalmente como un conjunto  mágico de algoritmos y procedimientos para llegar a unas conclusiones de aceptación o rechazo de la hipótesis.


Con el enfoque que vamos a aplicar en este curso verás que no necesitamos aprender toda esta series de supuestos y procedimientos para llevar a cabo un contraste hipótesis. 
Estos métodos se introdujeron en un momento en que los ordenadores no existían o eran muy poco potentes, pero ahora tu teléfono móvil  es más potente  que los ordenadores que enviaron a los astronautas de la NASA a la luna.

En realidad  TODAS las pruebas de hipótesis pueden explicarse mediante  el siguiente gráfico creado  por Allen Downey (<http://allendowney.blogspot.com.es/2016/06/there-is-still-only-one-test.html>) 


![](img/hipotesis_test_general.png)

<br>

Podemos pensar en un test o contraste de hipótesis como si fuera un juicio penal:
 
 Test de Hipótesis                  | Juicio                              
------------------------------------|------------------------------------
El parámetro de la población debe cumplir una de la dos hipótesis| El acusado debe ser declarado culpable o inocente |
Inicialmente asumimos que $H_0$ es verdadera | El código penal asume inicialmente que el acusado es inocente (presunción de inocencia)
La hipótesis nula $H_0$ será rechazada (a favor de $H_a$) sólo si la evidencia de la muestra sugiere fuertemente que $H_0$ es falsa. Si la muestra no proporciona dicha evidencia,  $H_0$ no será rechazada. | Solo si hay evidencias claras a favor de la culpabilidad la presunción de inocencia se desestima a favor de la culpabilidad
La analogía con "más allá de una duda razonable" en las pruebas de hipótesis es lo que se conoce como el nivel de significancia. Esto se establecerá antes de realizar la prueba de hipótesis y se denotará como $\alpha$. Los valores comunes para $\alpha$ son 0.1, 0.05 0 0.01 | La frase "más allá de una duda razonable" se utiliza a menudo para establecer el valor de corte según el cual se han demostrado suficientes evidencias para poder condenar al acusado


Por lo tanto, tenemos dos posibles conclusiones del contraste de  hipótesis:

- Rechazar $H_0$
- No es posible rechazar $H_0$

Instintivamente traducimos  "No rechazar $H_0$" en "Aceptar $H_0$", pero esto es técnicamente incorrecto. Aceptar $H_0$ es lo mismo que decir que una persona es inocente. No podemos demostrar que una persona es inocente, solo podemos decir que no había suficientes pruebas sustanciales para encontrar a la persona culpable.

Cuando ejecutas una prueba de hipótesis, tú eres el jurado del juicio y debes decidir si hay suficiente evidencia para convencerse de que $H_a$ es verdadera ("la persona es culpable") o que no hay evidencia suficiente para convencerse de que $H_a$ es verdadera ("la persona no es manifiestamente culpable"). Debes convencerte (usando argumentos estadísticos) cuál es la hipótesis correcta dada la información de la muestra.   Por lo tanto, no digas nunca "Aceptar $H_0$" cuando realices una prueba de hipótesis. En su lugar di "No es posible rechazar $H_0$"

## Tipos de Errores en el Contraste de Hipótesis

Desafortunadamente, al igual que un jurado o un juez puede tomar una decisión incorrecta con respecto a un juicio penal, existe alguna posibilidad de que lleguemos a una conclusión errónea a través de un contraste de hipótesis sobre un parámetro de una población. Al igual que con los juicios penales, esto viene del hecho de que no tenemos información completa, sino una muestra a partir de la cual tratamos de inferir acerca de una población.

Las posibles conclusiones erróneas de un juicio penal son

- Una persona inocente es declarada culpable
- Una persona culpable es puesta en libertad (no declarada culpable)

Los posibles errores en una prueba de hipótesis son

- Rechazar $H_0$ cuando $H_0$ es verdadero (a esto se le denomina Error de Tipo I)
- Fallar en rechazar $H_0$ cuando en realidad $H_0$ es falsa (Error Tipo II).

El riesgo de cometer un error es el precio que los investigadores pagan por basar una inferencia sobre una población en una muestra. Con cualquier procedimiento razonable basado en la muestra, existe alguna posibilidad de que se produzca un error de tipo I y de que se produzca un error de tipo II.

Para ayudar a entender los conceptos de error de Tipo I y error de Tipo II, observa la siguiente tabla:

![](img/errores_hipotesis.png)



La probabilidad de que ocurra un error de tipo I se denota por $\alpha$ y se denomina **nivel de significación del contraste de hipótesis** mientras que a probabilidad de un error de tipo II se denomina $\beta$ y se denomina **potencia**:

- $\alpha$ corresponde a la probabilidad de rechazar $H_0$ cuando, de hecho, $H_0$ es verdadera
- $\beta$ corresponde a la probabilidad de no rechazar $H_0$ cuando, en realidad, $H_0$ es falsa

Idealmente, queremos que $\alpha=0$ y $\beta = 0$, lo que significaría que no existiría la posibilidad de cometer un error. Pero cuando tenemos que usar información incompleta (datos de una muestra), no es posible tener tanto $\alpha=0$ y $\beta = 0$. 

Normalmente, lo que se hace es que $\alpha$ se establezca antes de que se realice la prueba de hipótesis y luego la evidencia se juzga en función de ese nivel de significación. Los valores comunes para $\alpha$ son 0.05, 0.01 y 0.10. 
Si usamos el valor más habitual $\alpha=0.05$  estamos usando un procedimiento de contraste que, usado repetidamente con diferentes muestras, rechaza una hipótesis nula verdadera el cinco por ciento de las ocasiones.

Entonces, si podemos establecer $\alpha$ como  queramos, ¿por qué elegir 0.05 en lugar de 0.01 o incluso mejor 0.0000000000000001? 

Bueno, un  pequeño $\alpha$ significa que el procedimiento de prueba requiere que la evidencia contra $H_0$ sea muy fuerte antes de poder rechazar $H_0$. Esto significa que casi nunca rechazaremos $H_0$ si $\alpha$ es muy pequeño. Si casi nunca rechazamos $H_0$, ¡ la probabilidad de un Error de Tipo II - no rechazar $H_0$ cuando deberíamos - aumentará! Por lo tanto, a medida que $\alpha$ disminuye, $\beta$ aumenta y si $\alpha$ aumenta, $\beta$ disminuye. Por lo tanto, necesitamos encontrar un equilibrio en $\alpha$ y  $\beta$ y los valores comunes de $\alpha$ 0.05, 0.01 y 0.10 llevan generalmente a un buen equilibrio.


El tamaño de la muestra influye en el compromiso entre el error de tipo I y de tipo II. Con tamaños de muestra grandes podemos exigir un $\alpha$ más pequeño sin comprometer en exceso la posibilidad de no rechazar la hipótesis nula cuando el efecto sea real. 

## Ejemplo 1: El adivino

Se trata de un ejemplo de Contraste de Hipótesis para eventos discretos o para proporciones

Supongamos que tenemos un amigo que nos asegura que es capaz de adivinar el color (rojo o negro) de una carta de una baraja francesa bien barajada antes de sacarla del mazo. Este experimento es equivalente a adivinar el resultado de lanzar una moneda al aire. 

Realizamos un experimento en el que sacamos 10 cartas distintas de la baraja y nuestro supuesto adivino acierta 8 de los colores de esas cartas. ¿Tiene poderes paranormales? ¿Está la baraja marcada? 

Recordemos de nuevo es esquema general de un contraste de hipótesis

![](img/hipotesis_test_general.png)

En nuestro caso:

- **Datos**: Nuestro amigo ha sacado 10 cartas y el resultado de adivinación ha sido: Correcto/Correcto/Correcto/Fallo/Correcto/Fallo/Correcto/Correcto/Correcto/Correcto

- **Estadistíco de test** $\delta$: el número de cartas adivinadas de 10 intentos

- **Efecto observado** $\delta^*$ : En nuestro caso $\delta^* = 8$

- **Modelo para la Hipótesis nula $H_0$**: la probabilidad de adivinar el color de una carta es  0.5

### Datos simulados 

Como el modelo de la hipótesis nula es sencillo, es posible obtener resultados del experimento simulado suponiendo que $H_0$ es cierta. La simulación consiste en emular tandas de 10 lanzamientos de una moneda con igual probabilidad de obtener cara que cruz. Es equivalente adivinar el color de una carta que el resultado de lanzar una moneda.

Para emular el lanzamiento de una moneda vamos a usar la función `rflip` del paquete mosaic.
El resultado de  tres tandas de lanzamientos sería el siguiente (H=cara (head), T=cruz (toss))

```{r}
set.seed(123)
s1=mosaic::rflip(10)
s2=mosaic::rflip(10)
s3=mosaic::rflip(10)
tmp=data.frame("Simulacion 1"= attr(s1,"sequence"),
               "Simulacion 2"= attr(s2,"sequence"),
               "Simulacion 3"= attr(s3,"sequence"),check.names = FALSE) 
knitr::kable(tmp)
```

donde el número de aciertos (H) es respectivamente `r as.integer(s1)`, `r as.integer(s2)`, `r as.integer(s3)`.  

Ahora repitamos 5000 tandas de lanzamientos y mostremos una tabla de frecuencias para el número de aciertos en cada tanda

```{r,cache=TRUE}
set.seed(321)
simFlips <- mosaic::do(5000) * mosaic::rflip(10)
```
 
El resultado es:
 
```{r}
res<- simFlips %>% 
  group_by(heads) %>%
  summarize(count = n())
knitr::kable(res)
```

### p-valor


**Definición: p-valor**

El p-valor es la probabilidad de observar un estadístico muestral, suponiendo que la hipótesis nula es verdadera,  igual o más extremo que el observado en la muestra real.



En la simulación, supuesta cierta la hipótesis nula (el acierto y el fallo son equiprobables), solo en `r sum(res$count[res$heads>=8])` de 5000 casos, se han acertado 8 o más cartas, es decir un   `r round(100*sum(res$count[res$heads>=8])/5000,2)` % de las ocasiones. Es decir el p-valor será

```{r}
(p_valor <- simFlips %>%
  filter(heads >= 8) %>%
  nrow() / nrow(simFlips))
```

Gráficamente se entiende mejor el concepto

```{r}
ggplot(simFlips, aes(x=factor(heads))) +
  geom_bar(aes(fill=(heads>=8))) + geom_vline(aes(xintercept = 8.5)) + theme(legend.position = 'none') +
  xlab("Aciertos")
```

Podemos ver que el estadístico observado de 8 aciertos no es un resultado muy probable suponiendo que la hipótesis nula es verdadera pero tampoco es completamente descartable. Alrededor de 5% de los resultados de nuestras 5000 simulaciones consiguen 8 o más éxitos.

A un nivel de significancia de $\alpha=0.05$ rechazaríamos (por los pelos) la hipótesis nula. Tenemos evidencia que apoya la conclusión de que la persona es realmente mejor adivinando cartas que la adivinación al azar.

Esta idea de un p-valor puede extenderse a los métodos más tradicionales que utilizan  distribuciones normales o  t de Student que se realizan en los cursos tradicionales de introducción a la  estadística, pero de momento no lo necesitamos. Estos métodos tradicionales se utilizaron porque los estadísticos no siempre han sido capaces de hacer 5000 simulaciones en el ordenador en cuestión de segundos.


## Contraste de hipótesis para la diferencia entre dos medias

Uno de los escenarios más habituales del contraste de hipótesis es para determinar si la media de una variable es diferente en distintos grupos de una población.

Vamos a ilustrarlo a través de un ejemplo: basándonos en una muestra de nuestros datos atropométricos, queremos determinar si la media del perímetro de cadera (Hip_girth) en hombres es diferente a la media del  perímetro de cadera en mujeres


Denotamos la media de población usando el símbolo griego $\mu$ (pronunciado "mu"). Nuestra hipótesis nula será

$$H_0: \mu_1 = \mu_2$$

que también puede escribirse como 

$$H_0: \mu_1 - \mu_2 = 0$$

donde $\mu_1$ representa la media del perímetro de cadera del grupo 1 (mujeres) y $\mu_2$ la media del perímetro de cadera del grupo 2 (hombres). 


Nuestra hipótesis alternativa será 

$$H_a: \mu_1 - \mu_2 \neq 0 $$

Como ya hemos visto, la simulación es una herramienta muy valiosa cuando queremos hacer inferencia sobre una población.
En este caso vamos a utilizar una técnica denominada aleatorización o permutación para simular los resultados posibles que se derivan de la hipótesis nula.


En nuestro caso, la población es un conjunto de `r nrow(body)` de medidas de partes del cuerpo de hombres y mujeres. En realidad podemos considerar que nuestro conjunto es una muestra de la población de personas del mundo o de un país.  De todas formas vamos a considerar una muestra de menor tamaño para demostrar que la inferencia puede realizarse con muestras relativamente pequeñas.

Para empezar consideremos una muestra de 50 hombres y 50 mujeres

```{r}
set.seed(951)
body_sampl <- body %>% select(Gender, Hip_girth) %>% group_by(Gender) %>% sample_n(50) %>% ungroup()

body_sampl$Gender=factor(body_sampl$Gender)
levels(body_sampl$Gender)=c("Female","Male")
```

Veamos la distribución de alturas mediante un boxplot y sus medias en rojo

```{r}
ggplot(body_sampl,aes(x=Gender,y=Hip_girth)) + geom_boxplot() + stat_summary(aes(x=Gender,y=Hip_girth),fun.y = "mean", geom="point", color="red")  

```

La diferencia entre las medias es aparente. Pero, ¿es esta diferencia estadísticamente significativa? Realicemos un contraste de hipótesis.  


- **Datos**: La muestra que acabamos de obtener

- **Estadistíco de test**: La diferencia entre medias muestrales $\delta = \bar{x}_m -\bar{x}_f$

- **Efecto observado**  $\delta^{*}$

La diferencia entre las medias observadas es 

```{r}
sample_means <- body_sampl %>% group_by(Gender) %>% summarise(mean=mean(Hip_girth))
(obs_diff <- diff(sample_means$mean))
```

- **Modelo para la Hipótesis nula $H_0$**: Las medias poblacionales del perímetro de cadera de hombres y mujeres son iguales 

$$\mu_m - \mu_f ==  0 $$

### Datos simulados 

Podemos reformular la hipótesis nula desde un punto de vista más visual: 

- Consideremos que apuntamos las medidas de los 50 hombres en 50 tarjetas blancas y las colocamos en un mazo. Hacemos los mismo con las 50 mujeres y los colocamos en otro mazo.

- Asumir como cierta $H_0$, es decir que la  media del perímetro de cadera no depende del género, es equivalente a coger las tarjetas, barajarlas y volver a hacer dos nuevos mazos. Podemos calcular la media de cada uno de los nuevos mazos y calcular su diferencia.

- Nuestra simulación asumiendo la hipótesis nula consistirá en repetir un número elevado de veces (5000) el proceso anterior. 


Hagámoslo con R, usando la función shuffle del paquete mosaic, que reordena/permuta al azar los elementos de un vector.

Una sola vez sería

```{r}
shuffled_data <- body_sampl %>% 
     mutate(Hip_girth = mosaic::shuffle(Hip_girth)) %>% 
     group_by(Gender) %>%
     summarize(mean = mean(Hip_girth))
               
diff(shuffled_data$mean)
```


Si lo repetimos 5000 veces

```{r, cache=TRUE, results='hold'}
set.seed(987)
shuffled_means <- mosaic::do(5000)* (
     body_sampl %>% 
     mutate(Hip_girth = mosaic::shuffle(Hip_girth)) %>% 
     group_by(Gender) %>%
     summarize(mean = mean(Hip_girth))
)
# Mostramos la primeras filas
head(shuffled_means)
# View(shuffled_means)
```

Ahora debemos calcular las diferencias entre medias para cada permutación, es decir obtener la distribución bajo $H_0$ del estadístico $\delta = \bar{x}_m -\bar{x}_f$. Podemos hacerlo de la siguiente manera

```{r}
rand_diffmeans <- shuffled_means %>%
  group_by(.index) %>%
  summarize(diffmean = diff(mean))
head(rand_diffmeans, 10)
```


Representemos la distribución de la diferencia de medias obtenida

```{r}
ggplot(rand_diffmeans, aes(x =diffmean)) +
  geom_histogram(color = "white", bins = 20)
```


### p-valor

Ahora queremos ver donde está nuestra diferencia de medias observada $\delta_{*}= `r obs_diff`$ en esta distribución. Veámoslo gráficamente, aumentando el número de intervalos del histograma para una mejor visualización y representando mediante lineas verticales el valor observado para la diferencia de media observada y su negativo.

```{r}
ggplot(rand_diffmeans, aes(x =diffmean)) +
  geom_histogram(aes(fill=(abs(diffmean) >= obs_diff)),color = "white", bins = 100) + 
  geom_vline(xintercept = obs_diff) +
  geom_vline(xintercept = -obs_diff) + theme(legend.position = 'none')
```


Ahora para calcular el p-valor tenemos que calcular la probabilidad de que supuesta cierta la hipótesis nula, obtener un valor del estadístico de test igual o más extremo que el observado. El concepto de igual o más extremo lo hacemos por los dos lados de la distribución. Por tanto consiste en calcular el cociente entre la suma de las barras azules y la suma de de todas las barras. A simple vista lo podemos estimar como algo en torno al  2%, o igual un 5% pero  seguro que menor de un 20%. Calculémoslo numéricamente 

```{r}
(pvalue <- rand_diffmeans %>%
  filter(abs(diffmean) >= obs_diff) %>%
  nrow() / nrow(rand_diffmeans))
```

Si consideramos un nivel de significación de $\alpha=0.05$, como el p-valor es menor de 0.05, decimos que tenemos evidencias suficientes para rechazar la hipótesis nula, es decir que hay evidencia estadística entre de que la media de los perímetros de cadera de hombres y mujeres es distinta.  



## Enfoque tradicional de los Contrastes de Hipótesis

El enfoque tradicional de los contrastes de hipótesis (pensemos en la  media de una variable o la diferencia de medias entre dos grupos),  se basan en el **teorema del límite central**, es decir que la media muestral de una variable se encuentra normalmente distribuida con: 

- media $\mu_{\bar{x}} = \mu$ y 
- $\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}$

donde $\mu$ es la media de la población y $\sigma$ es desviación típica de la población y es finita. Recordamos de nuevo que la distribución de la variable no tiene porque ser normal, aunque el número de elementos de la muestra necesarios para la validez del teorema crece conforme se aleja de la normalidad.

### Contraste de hipótesis para la media muestral

Hagamos un test de hipótesis para la media de las alturas de nuestro conjunto de datos. 

Consideremos como $H_0$: La media de la altura de la población en 170 cm

Consideremos una muestra de 30 individuos y calculemos la media muestral $\bar{x}$

```{r}
set.seed(3)
body_sampl <- body %>% select(Gender, Height)  %>% sample_n(30) %>% ungroup()
(obs_mean <-  mean(body_sampl$Height))
```

#### Test z (Varianza poblacional conocida)

Si la desviación típica de la población es conocida y vale $\sigma$, entonces la distribución de la variable

$$z=\frac{\bar{x}-\mu}{\frac{\sigma}{\sqrt{n}}}$$

está *normalmente* distribuida con media 0 y varianza 1.  $\bar{x}$ es una variable aleatoria que representa la media una  muestra de tamaño n obtenida al azar  de la población. 

Basándonos en el resultado anterior podemos realizar un test z, que consiste en calcular el p-valor basándose en las propiedades de la distribución normal. 

**Autotexto**

Te recordamos de nuevo que leas el apéndice del capitulo para aprender más sobre distribuciones de probabilidad y sobre la distribución normal. 

**Fin Autotexto**

Si consideramos conocida la desviación típica de $\sigma$ de la población

```{r}
(sigma_pob=sd(body$Height))
```

El valor del estadístico z para nuestra muestra, supuesta cierta la hipótesis nula es 

```{r}
(z=(obs_mean-170)/(sigma_pob/sqrt(30)))
```

Por tanto el p-valor asociado a la hipótesis nula es la probabilidad de obtener según una distribución normal de media 0 y desviación típica 1 un valor absoluto mayor o igual que `r z`. De forma gráfica sería el area sombreada de azul claro. 

```{r}
# dnorm es una función que proporciona la función densidad de probabilidad de una distribución normal
ggplot(data=data.frame(x=seq(-4,4,0.1)),aes(x)) + 
    stat_function(fun = dnorm, color="blue", geom="line") + 
    stat_function(xlim=c(-4,-abs(z)),fun = dnorm, fill = "cyan", geom="area") + 
    stat_function(xlim=c(abs(z),4),fun = dnorm, fill = "cyan", geom="area") + 
    geom_vline(aes(xintercept=-z)) + geom_vline(aes(xintercept=z))  
```

De forma numérica el p-valor se calcula como

```{r}
(p_valor <- 2*(1-pnorm(abs(z),0,1)))
```

donde la función pnorm calcula la función de probabilidad acumulada (o simplemente función de distribución), es decir  `pnorm(x,mu,sigma)` devuelve la probabilidad de obtener un valor menor o igual que x bajo una distribución normal de media `mu` y desviación típica `sigma`

Con $p-valor=`r p_valor`$, el resultado del contraste sería que no tenemos evidencia suficiente para rechazar la hipótesis nula. 

#### Test t (El habitual)

En el contraste anterior hemos usado un dato del que normalmente no se dispone, que es la desviación típica de la población. 
Habitualmente solo tenemos datos de una muestra y no sabemos absolutamente nada de la población.

En este caso el mejor estimador que tenemos para la varianza de la población sería la desviación típica de la muestra $s$ y por tanto el estimador para la desviación típica de la media muestral es 

$$ 
SE_{\bar{x}} = \frac{s}{\sqrt{n}}
$$

En este caso, para realizar el contraste de hipótesis usaremos el estadístico t

$$
T=\frac{\bar{x}-\mu}{\frac{s}{\sqrt{n}}}
$$

que ya no sigue una distribución normal sino una distribución t de Student con $n-1$ grados de libertad. 

Para más detalles sobre la distribución t de Student te remitimos al apéndice, de momento solo debemos saber  unas pocas cosas de la distribución t de Student:

- Depende de un único parámetro: los grados de libertad $df$
- Se aproxima a una distribución normal conforme aumentan los grados de libertad
- Para muestras pequeñas, $df$ bajo, los valores extremos son bastante más probables que en una distribución normal

Veamos gráficamente como cambia la distribución t comparada con la distribución normal. Para $df=20$ ya son difícilmente distinguibles a simple vista.

```{r}
ggplot(data.frame(x=seq(-4,4,.1)),aes(x)) + 
  stat_function(aes(color="Normal"),fun=dnorm, geom="line", size=1.5)+
  stat_function(aes(color="t df=3"),fun=dt,args=list(df=3),geom="line")+
  stat_function(aes(color="t df=10"),fun=dt,args=list(df=10),geom="line")+
  stat_function(aes(color="t df=20"),fun=dt,args=list(df=20),geom="line") +
  labs(y="",color="Distribución") + scale_color_brewer(palette = "Set1")

```


Para la muestra tomada en el ejemplo anterior el valor del estadístico t sería

```{r}
n=nrow(body_sampl)
(obs_mean <-  mean(body_sampl$Height))
(obs_sd <-  sd(body_sampl$Height))
(sd_err <- obs_sd/sqrt(n))
(t = (obs_mean - 170) / sd_err)
```

El p-valor asociado al estadístico t de la muestra para la hipótesis nula de que la media de la población es 170 cm se calcula a través de la función `pt` (equivalente a la `pnorm` que hemos usado antes) como: 

```{r}
(p_valor= 2*(1-pt(t,df=n-1)))
```

Los resultados del test t y test z son consistentes. Ambos dan p valores muy similares y ambos coinciden en afirmar que no hay suficiente evidencia para rechazar $H_0$

**Autotexto**
Recuerda que el contraste de hipótesis nunca dice que la hipótesis nula es cierta, el jurado no dice si el acusado es inocente, dice que no evidencias (pruebas) para poder decir que la hipótesis nula sea falsa. 
**Fin Autotexto**

En la práctica, para hacer inferencias sobre las medias de una población se usan los test t, y no los test z, ya que muy raramente se conocen los datos de la población y el único conocimiento que tenemos son muestras. Y además si conociéramos la población qué sentido tendría hacer inferencias. 

En R podemos hacer un test t usando la función `t.test`, que proporciona además del p-valor, un intervalo de confianza para la estimación. En la siguiente sección veremos que son los intervalos de confianza y que relación tienen con los contrastes de hipótesis.

```{r}
t.test(body_sampl$Height,mu = 170)
```


### Test t para la diferencia de medias

Más habitual que contrastar si una media tiene un determinado valor, es contrastar si las medias de dos poblaciones, o dos grupos de una población son iguales. 

Esto ya lo hemos hecho para el perímetro de cadera para hombres y mujeres utilizando técnicas de aleatorización. En este caso hagámoslo utilizando un test t. 

Recordemos que en este caso la hipótesis nula es que la media de la variable es igual en los dos grupos. 

El estadístico t en este caso será

$$
T=\frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}
$$


donde el denominador es un estimador de la desviación típica muestral de la diferencia entre medias y 

- $\bar{x}_1$ es la media de la muestra del grupo 1
- $\bar{x}_2$ es la media de la muestra del grupo 2
- $s_1$ es la desviación típica de la muestra del grupo 1
- $s_2$ es la desviación típica de la muestra del grupo 2
- $n_1$ es el número de elementos de la muestra del grupo 1
- $n_2$ es el número de elementos de la muestra del grupo 2



Realicemos el test t para la diferencia de medias del perímetro de cadera entre hombres y mujeres.


Tomamos las mismas muestras

```{r}
set.seed(951)
body_sampl <- body %>% select(Gender, Hip_girth) %>% group_by(Gender) %>% sample_n(50) %>% ungroup()
body_sampl$Gender=factor(body_sampl$Gender)
levels(body_sampl$Gender)=c("Female","Male")
```

Calculamos t

```{r}
(summary_hip <- body_sampl %>% group_by(Gender) %>% summarise(n=n(),m=mean(Hip_girth),sd=sd(Hip_girth)))

m1 <- summary_hip$m[2]
m2 <- summary_hip$m[1]
s1 <- summary_hip$sd[2]
s2 <- summary_hip$sd[1]
n1 <- summary_hip$n[2]
n2 <- summary_hip$n[1]

(t<- (m1-m2)/sqrt(s1^2/n1 + s2^2/n2))

```

El número de grados de libertad sería $n_1+n_2-2$, por lo que el p-valor será

```{r}
(p_valor = 2*(1-pt(t,df=n1+n2-2)))
```

Podemos comprobar que es un valor muy parecido al obtenido mediante las técnicas de aleatorización o permutación

En R es muy sencillo realizar un t.test para la diferencia de medias

```{r}
samp1=body_sampl %>% filter(Gender=="Male") 
samp2=body_sampl %>% filter(Gender=="Female") 

t.test(samp1$Hip_girth,samp2$Hip_girth)
```
### Condiciones para el test t

Para que los resultados del test t sean válidos, deben cumplirse tres condiciones:

- Observaciones independientes en ambas muestras
- Poblaciones casi normales o tamaños de muestra grandes ($n \geq 30$)
- Muestras seleccionadas independientemente

En el caso analizado se cumplen las tres condiciones

- Condición 1: Esto se cumple, ya que la muestra se obtuvo de forma aleatoria utilizando R sobre nuestra población.

- Condición 2: Si observamos un histograma del perímetro de cadera observamos que la distribución de frecuencias es aparentemente similar a una normal. De todas formas las muestras son de tamaño mayor que 30

- Condición 3: Esto se cumple ya que no hay ningún emparejamiento natural entre los elementos del grupo de hombres y mujeres

Dado que se cumplen las tres condiciones, podemos estar bastante  seguros de que la prueba basada en la teoría coincidirá con los resultados de la prueba basada en la aleatorización utilizando la función `shuffle`.


Para concluir esta sección nos gustaría remarcar  que las pruebas basadas en la teoría pueden conducir a resultados incorrectos si estas suposiciones no se comprueban cuidadosamente.

En cambio la única hipótesis para la aleatorización y los métodos basados en computación es que la muestra se seleccione al azar. Por tanto te recomendamos el uso de estos métodos siempre que puedas. Son más fiables y flexibles.

Pero  de todas formas, es importante también conocer cómo las pruebas basadas métodos teóricos se pueden hacer y utilizarlas como una aproximación para las técnicas computacionales, ya que siguen estando muy arraigados en la comunidad científica.


# Intervalos de confianza 


Un intervalo de confianza proporciona un rango de valores plausibles para el valor de un parámetro de una población a partir de los datos de una muestra. Depende de un nivel de confianza especificado, los niveles de confianza más altos se corresponden con intervalos de confianza más amplios y al contrario niveles de confianza más bajos se corresponden con intervalos de confianza más estrechos. 
Los niveles de confianza habituales suelen ser del  90%, 95% y 99%.

Podemos hacer una analogía de un intervalo de confianza con una red de pescar. En lugar de tratar de capturar un pez con una sola lanza (estimando un parámetro desconocido usando una estimación/estadística puntual), podemos usar una red para tratar de proporcionar una variedad de ubicaciones posibles para los peces (usamos un rango de valores posibles  basados en nuestra muestra para hacer una conjetura plausible sobre la  ubicación real 
del  parámetro).



## Bootstrapping

Tradicionalmente, la forma de construir intervalos de confianza para una media es asumir una distribución normal para la población o invocar el Teorema del Límite Central y obtener los límites de este intervalo a través de las propiedades de la distribución normal o de la distribución de t de Student.

Al igual que discutimos para el contraste de hipótesis, estos métodos a menudo no son intuitivos, especialmente para aquellos que carecen de una buena formación matemática. Además su validez depende de una serie de suposiciones que no siempre se cumplen. 

En esta sección, vamos a introducir el concepto del "bootstrap". Es una herramienta muy útil que nos permitirá estimar la variabilidad de nuestro estadístico de una muestra a otra. Una de las características más destacadas del *bootstrap* es que nos permite aproximar la distribución muestral y estimar la desviación estándar de la distribución utilizando SOLAMENTE la información en la muestra seleccionada (original).


Para ilustrar el concepto, volvamos a utilizar los datos *body* y vamos a estimar un intervalo de confianza para la altura media.

La distribución de alturas de la población y su valor medio se muestran a continuación


```{r, message=FALSE, warning=FALSE}
pob_mean <- mean(body$Height)
ggplot(body) + geom_histogram(aes(Height),bins=20) + geom_vline(aes(xintercept=pob_mean),color="red") + 
  geom_text(aes(pob_mean*1.02,50,label=round(pob_mean,2)),color="red")
```

Tomemos ahora una muestra de tamaño 50 y veamos su media y distribución

```{r, message=FALSE, warning=FALSE}
set.seed(671)
n=50
body_sampl <- body %>% sample_n(n)
samp_mean <- mean(body_sampl$Height)
ggplot(body_sampl) + geom_histogram(aes(Height),bins = 20) + 
  geom_vline(aes(xintercept=samp_mean),color="blue") + 
  geom_text(aes(samp_mean*1.02,5,label=round(samp_mean,2)),color="blue") 

```


Es importante recordar ahora cual es el objetivo: obtener un intervalo de confianza, es decir un rango de valores, a partir de los datos de la muestra que con una cierta confianza podamos afirmar que contiene a la media real de la población. 

Debemos pensar que los datos de la población los tenemos esta vez a modo de comprobación, pero cuando trabajemos en proyectos de análisis de datos reales van a ser desconocidos. 


El método de Bootstrap consiste en obtener remuestreos a partir de la muestra, tomando muestras con remplazo del mismo tamaño que la muestra original. Siguiendo con nuestro ejemplo, si tenemos una muestra con 50 individuos, tomamos 50 tarjetas en blanco y apuntamos las alturas de los 50 individuos de la muestra. Ahora tomamos una tarjeta, apuntamos la altura y la volvemos a meter en el mazo. Barajamos y volvemos a tomar otra carta, apuntamos la altura y la reponemos al mazo. Así sucesivamente continuamos hasta extraer 50 cartas distintas. Seguramente te habrás dado cuenta que la misma carta puede salir varias veces y algunas no saldrán ninguna vez. Esto es normal y necesario para que el proceso de bootstrap funcione. A este conjunto de 50 observaciones le llamamos muestra de bootstrap. Y podemos calcular la media de las alturas sobre ella. 

Si repetimos un número elevado de veces este proceso, podemos obtener una distribución de medias de las muestras de bootstrap. Es posible demostrar que las propiedades de la distribución de medias de bootstrap son muy similares a la distribución de medias muestrales del mismo tamaño obtenidas a partir de la población si tuviéramos acceso a ella.


El siguiente diagrama esquematiza el proceso de obtención del la distribución de bootstrap


![](img/bootstrap_diagram.png)


Por ejemplo, si hacemos 10 procesos de bootstrap y obtenemos la media de las muestras tenemos:

```{r}
mosaic::do(10) * 
  (sample_n(body_sampl,50,replace=TRUE) %>% 
     summarize(mean = mean(Height)))
```

Observemos que para obtener una muestra de tamaño 50 con remplazo usamos la función del paquete dplyr `sample_n(body_sampl,50,replace=TRUE)`.

Ahora repetimos el proceso 5000 veces y observemos la distribución de medias de bootstrap


```{r, cache=TRUE}
boots_means <- mosaic::do(5000) * 
  (sample_n(body_sampl,50,replace=TRUE) %>% 
     summarize(mean = mean(Height)))

ggplot(boots_means) + geom_histogram(aes(mean),bins=20)
```

Como en este caso tenemos la población, podemos comparar la distribución obtenida para las medias de bootstrap con la que obtendríamos generando 5000 muestras aleatorias de tamaño 50 y calculando sus medias


```{r, cache=TRUE}
sampl_means <- mosaic::do(5000) * 
  (sample_n(body,50,replace=FALSE) %>% 
     summarize(mean = mean(Height)))

ggplot(boots_means) + geom_freqpoly(aes(mean,color="bootstrap"),bins=20) +
  geom_freqpoly(aes(mean,color="population"),bins=20,data=sampl_means) +
  labs(color="Sampling")

```


Podemos observar que las formas de las distribuciones de las medias muestrales son muy parecidas, aunque una está centrada en la media de la muestra original y la otra en la media de la población. Pero que queda claro que a partir de la distribución de las medias bootstrap podemos inferir las propiedades de dispersión de la media muestral y por tanto generar un intervalo de confianza.

Ahora, a partir de la distribución de medias de bootstrap podemos calcular el intervalo de confianza. Para un nivel de confianza del 95% lo estimamos mediante el 95% central de la distribución, que se obtiene como:

- Extremo inferior: cuantil 0.025 
- Extremo superior: cuantil 0.975

```{r, results='hold'}
conf=0.95
samp_mean
pob_mean
(ci_lower <- quantile(boots_means$mean,(1-conf)/2))
(ci_upper <- quantile(boots_means$mean,conf + (1-conf)/2))
```

Podemos observar que el intervalo (`r ci_lower`,`r ci_upper`) contiene la media real de la población

**Autotexto Atención**
La interpretación del significado del intervalo de confianza es importante. Decimos que con un 95% de confianza el valor real de la la media poblacional está en algún lugar entre los extremos del intervalo. 
No debemos interpretarlo como que existe un 95 % de probabilidad de que la media real esté entre esos valores. La media real es la que es y tiene un valor determinado no es una variable aleatoria. Lo aleatorio en este caso es la obtención de muestras. Un 95 % de confianza quiere decir que de cada 100 muestras aleatorias que obtuviésemos de la población, en 95 ocasiones de las 100, el intervalo de confianza,  que se calcula únicamente con datos de la muestra, contendrá a la media real de la población
**Fin Autotexto**

Con el fin de comprobar la definición de confianza, vamos a obtener 100 muestras distintas de la población y vamos a calcular los intervalos de confianza para cada una de esas 100 muestras

```{r,cache=TRUE}
set.seed(32457)
cis <- mosaic::do(100)*({
  sampl<-sample_n(body,50)
  boots_means <- mosaic::do(1000) * 
  (sample_n(sampl,50,replace=TRUE) %>% 
     summarize(mean = mean(Height)))
  ci_lower <- quantile(boots_means$mean,(1-conf)/2)
  ci_upper <- quantile(boots_means$mean,conf + (1-conf)/2)
  data.frame(lower=ci_lower,upper=ci_upper)
})

head(cis)
```

Veamos en un gráfico estos 100 intervalos de confianza junto con la media real de la población

```{r}
ggplot(cis) + geom_linerange(aes(x=.index,ymin=lower,ymax=upper,color=(lower<pob_mean & upper>pob_mean))) + 
  geom_hline(aes(yintercept=pob_mean)) + theme(legend.position = 'none')
```

En la simulación, de los 100 intervalos de confianza determinados a partir de 100 muestras, 96 contienen a la media real de la población. Esa es la interpretación correcta del intervalo de confianza. Si la simulación fuera mayor serían exactamente el 5% de los intervalos los que no contendrían a la media de la población. 

Hemos calculado el intervalo de confianza a partir de los cuantiles. Pero dado que la distribución de medias de bootstrap tiene aspecto de distribución normal podríamos hacerlo también con la fórmula tradicional, basada en las propiedades de la distribución normal:

$$\bar{x} \pm (2 \times SE) $$ 

donde $SE$ es la desviación típica de la distribución de medias bootstrap. En nuestro caso

```{r}
se = sd(boots_means$mean)
(cise_lower <- samp_mean - 2*se)
(cise_upper <- samp_mean + 2*se)
```

## Calculo tradicional de los intervalos de confianza

De forma tradicional, el intervalo de confianza se calcula teniendo en cuenta, al igual que en los contrastes de hipótesis, que la distribución de medias muestrales es una distribución normal. Recordemos que la desviación típica de la distribución muestral la estimamos como: 

$$ SE_{\bar{x}} = \frac{s}{\sqrt{n}}$$

Si la muestra es suficientemente grande ($n \ge 30$) calculamos el intervalo de confianza al 95 % como 

$$\bar{x} \pm (2 \times SE_{\bar{x}}) $$
En realidad este es el el intervalo de confianza al 95.4 %, para que fuera al 95 % multiplicaríamos por 1.96, pero es habitual por simplicidad estimar los intervalos de confianza al 95% usando el factor 2.


```{r}
n=nrow(body_sampl)
se = sd(body_sampl$Height)/sqrt(n)
samp_mean - 2*se
samp_mean + 2*se
```


En el caso de muestras más pequeñas, aunque es válido para cualquier tamaño de muestra, debemos usar la distribución t de Student, por el hecho de usar la desviación típica muestral $s$ para estimar $SE$ en lugar de la $\sigma$ de la población (que en principio no conocemos).
En ese caso podemos calcular el intervalo de  confianza al 95 % de la media de una muestra de tamaño $n$  como 

$$
(\bar{x} \pm qt(0.975,df=n-1) \, \frac{s}{ \sqrt{n}}
$$

donde $qt(0.975,df=n-1)$ es el cuantil 0.975 de la distribución t de Student. Si $n$ es suficientemente grande vale 1.96 como hemos dicho antes. Veamos para nuestra muestra de tamaño 50 como sería el intervalo de confianza así calculado


```{r}
n=nrow(body_sampl)
se = sd(body_sampl$Height)/sqrt(n)
f=qt(0.975,n-1)
samp_mean - f*se
samp_mean + f*se
```

Muy parecido  al obtenido anteriormente.

Al termino $qt(0.975,df=n-1)$ que multiplica a $SE$ en el cálculo del intervalo de confianza se le denomina habitualmente factor de cobertura. Como hemos dicho tiende a 1.96 cuando n es grande. Veamos una tabla de valores en función del tamaño de la muestra

```{r, results='asis'}
ns=c(2,3,5,10,15,20,30,50,100,1000)
f=qt(0.975,df=ns-1)
knitr::kable(data.frame(n=ns,qt_975=f),digits = 3)
```

## Relación con el contraste de hipótesis

Existe una relación directa entre el contraste de hipótesis y el intervalo de confianza:

- Consideremos que la  hipótesis nula del contraste de hipótesis es que un cierto parámetro $\delta$ toma un determinado valor $\delta_0$

- Disponemos de una muestra para la cual podemos estimar un intervalo de confianza para $\delta$: $(\delta_{inf},\delta_{sup})$

- Si el intervalo de confianza no contiene a $\delta_0$ entonces rechazaremos $H_0$ con un nivel de significación $\alpha$ si nivel de confianza empleado para determinar el intervalo es de $1-\alpha$.

- Si por el contrario el intervalo de confianza contiene a $\delta_0$ entonces diremos que no hay suficientes evidencias para rechazar la hipótesis nula. 

Además, el uso de intervalos de confianza complementando al p-valor es conveniente porque aporta más información al problema. Existen muchos casos en los que se detecta un efecto (rechazo de $H_0$), debido a que el p-valor es muy pequeño, y sin embargo mirando el intervalo de confianza se observa que el efecto aunque significativamente distinto de 0, es muy pequeño en la escala del problema. Esto es habitual cuando se trabaja con muestras grandes.

Como ejemplo de lo anterior, podemos pensar en pacientes a los que se suministra distintos tratamientos para reducir el colesterol. Al realizar el contraste de hipótesis obtenemos un p-valor menor de 0.05 que detecta una diferencia significativa en la media de la concentración de colesterol en sangre entre los pacientes que han tomado uno u otro tratamiento. Pero esta diferencia aunque significativamente distinta de 0 desde el punto de vista estadístico puede ser desde el punto de vista  clínico muy pequeña  para que sea relevante respecto al riego de padecer una enfermedad cardiovascular. 

Para ilustrar lo anterior, volvamos al ejemplo de la diferencia de medias entre el diámetro de cadera entre hombres y mujeres. Recordemos que mediante un contraste de hipótesis detectamos una diferencia del valor de las medias con un nivel de significación de $\alpha=0.05$ entre hombres y mujeres.

Vamos a obtener un intervalo de confianza para la diferencia de medias a partir de la información de la muestra. 
Podríamos, igual que hemos hecho para estimar el intervalo de confianza de la media de alturas, generar muestras mediante bootstrap de hombres y mujeres, calcular la diferencia de medias para cada par de muestras y obtener de esta manera la distribución de las diferencias de medias muestrales. Una vez obtenida esta distribución calcular el intervalo de confianza a partir de los cuantiles 0.025 y el 0.975 de esta distribución. 

Este método lo dejamos para el final de la sección. De primeras vamos a estimar el intervalo de confianza aprovechando parte del trabajo hecho en la sección anterior, ya que de esta forma se visualiza mejor la relación con el contraste de hipótesis.

Recordemos que a partir de proceso de permutación usando la función `shuffle` obtuvimos la distribución de la diferencia de medias suponiendo la hipótesis nula cierta (la media en hombres y mujeres es igual).

```{r}
ggplot(rand_diffmeans, aes(x =diffmean)) +
  geom_histogram(color = "white", bins = 20)  
```

Como esta distribución parece bastante gaussiana, podemos estimar el intervalo de confianza usando el error típico de esta distribución. 

La diferencia entre esta distribución, que hemos obtenido supuesta que la hipótesis nula de medias iguales es cierta, y la que obtendríamos mediante bootstrapping es que esta está centrada en 0 y la otra estaría centrada en la diferencia de medias observada en nuestra muestra (recuerda que solo tenemos una muestra). Pero para la calcular el error típico (la dispersión) nos sirve perfectamente.

```{r}
(std_err <- rand_diffmeans %>% summarize(se = sd(diffmean)))
```

Entonces estimaremos el intervalo de confianza como 

$$dif\_observada \pm 2 SE$$


```{r}
obs_diff
(lower <-  obs_diff - 2*std_err) 
(upper <-  obs_diff + 2*std_err) 

```

Podemos comprobar que el intervalo con un nivel de confianza del 95% no contiene al 0. Este resultado es equivalente a haber obtenido un p-valor menor que 0.05 en el contraste de hipótesis. 


Como ya hemos anticipado, es posible calcular el intervalo de confianza para la diferencia de medias usando bootstrap. De esta manera obtenemos la distribución de muestral de la diferencia de medias centrada entorno a la media de nuestra muestra y podemos estimar el intervalo de confianza a partir de los cuantiles de la distribución de bootstrap. Hagámoslo:

```{r}
set.seed(951)
# Obtengo la muestra de hombres y mujeres con usando la misma semilla aleatoria
body_sampl <- body %>% select(Gender, Hip_girth) %>% group_by(Gender) %>% sample_n(50) %>% ungroup()
body_sampl$Gender=factor(body_sampl$Gender)
levels(body_sampl$Gender)=c("Female","Male")
# Calculo la diferencia de medias entre hombres y mujeres en la muestra
sample_means <- body_sampl %>% group_by(Gender) %>% summarise(mean=mean(Hip_girth))
obs_diff <- diff(sample_means$mean)

# Obtengo muestras mediante bootstrap dentro de los grupos 
boots_gmeans <- mosaic::do(5000) * 
  (body_sampl %>% group_by(Gender) %>% sample_n(50,replace=TRUE) %>% 
     summarize(mean = mean(Hip_girth))
    )
# Calculo las diferencias entre medias entre los pares de muestras
boots_diffmeans <- boots_gmeans %>% group_by(.index) %>% summarise(diffmean= diff(mean))
```

Calculemos el intervalo de confianza al 95% a partir de los cuantiles


```{r}
(lower <- quantile(boots_diffmeans$diffmean,0.025))
(upper <- quantile(boots_diffmeans$diffmean,0.975))

```

que visto de  forma gráfica resulta

```{r}
ggplot(boots_diffmeans,aes(diffmean)) + geom_histogram(bins=20) + 
  geom_vline(aes(xintercept=quantile(boots_diffmeans$diffmean,0.025)),color="red") +
  geom_vline(aes(xintercept=quantile(boots_diffmeans$diffmean,0.975)),color="red")
  
```

Y podemos comprobar que que es muy parecido al que hemos obtenido a partir del error estándar de la distribución generada bajo la hipótesis nula. 

De hecho si comparamos la distribución de la diferencia de medias entre muestras generadas mediante bootstrap y mediante permutación, se observa que son muy parecidas salvo la posición central de cada una de ellas

```{r}
ggplot(boots_diffmeans) + geom_freqpoly(aes(diffmean)) + 
  geom_freqpoly(aes(diffmean),data=rand_diffmeans,color="red")
```


# ¿Qué has aprendido?

A lo largo de la unidad hemos aprendido que es hacer una inferencia estadística y en que fundamentos se basa. 

A partir de técnicas de simulación y de muestreo hemos aprendido a entender los conceptos de "contraste de hipótesis" o "intervalo de confianza" de una manera intuitiva, generando datos que somos capaces de visualizar e interpretar.

Ahora ya estás en condiciones de hacer inferencias a partir de tus propios datos y responder a preguntas tipo: 

- Ha respondido mejor el tratamiento A o el B, tras analizar los resultados de su aplicación en dos grupos diferenciados de pacientes

- Hacer una encuesta y ser capaz de proporcionar estimaciones del error de los resultados

- Imitar a los Pelayo y determinar si la ruleta de un casino tiene algún tipo de sesgo.


# Autoevaluación

1. El sesgo en la elección de una muestra

    a. representa la variabilidad de una variable en la muestra
    b. está relacionado con la predictibilidad de un determinado parámetro 
    c. corresponde a la prevalencia de un grupo en una población sobre otro grupo en una muestra
    d. todas las anteriores son correctas

2. Tomemos unas muestras no aleatorias de los datos body usando R. Definimos la variable $IMC = Weight/\frac{Weight}{Height^2}, que representa el índice de masa corporal. ¿Para cuál de las siguientes muestras el IMC medio será más parecido al IMC de la población?

    a. `s1=filter(body, Height <160)`
    b. `s2=filter(body, Height >190)`
    c. `s3=filter(body, Height %% 4 ==0)` (Altura es múltiplo de 4)
    d. `s2=filter(body, Weight < 80)`
    
3. La dispersión de la distribución de medias muestrales, con el tamaño de la muestra

    a. Aumenta
    b. Disminuye
    c. Permanece constante
    d. No se sabe

4. La hipótesis nula en un contraste de hipótesis representa habitualmente

    a. El efecto o la diferencia que queremos demostrar
    b. Un efecto o diferencia muy fuerte
    c. Un efecto o diferencia muy debil
    d. Que no hay efecto o diferencia

5. Cuando en un contraste de hipótesis con un nivel de significación fijado al 0.05 obtenemos un p-valor>0.05 decimos 

    a. La hipótesis nula es cierta
    b. La hipótesis nula es falsa
    c. No hay evidencias para rechazar la hipótesis nula
    d. No podemos decir nada acerca de la hipótesis nula

6.  Para realizar un contraste de hipótesis para la diferencia de medias mediante los métodos tradicionales, el test más apropiado es

    a. test t
    b. test $\chi^2$
    c. test F
    d. test z

7. Estimamos el intervalo de confianza para la el índice de masa corporal para una muestra de tamaño 50, con distintos niveles de confianza 99%, 95%, 90%  y 80%. ¿Cuál de los intervalos será más amplio?

    a. El de confianza al 99%
    b. El de confianza al 95%
    c. El de confianza al 90%
    d. El de confianza al 80%
    
8. Si la estimación de un intervalo de confianza con nivel de confianza del 99% a partir de una muestra, no contiene la media de la población

    a. Está mal calculado
    b. Es culpa de la muestra que no es aleatoria.
    c. Es posible, por definición de cada 100 muestras aleatorias, el intervalo de confianza calculado con una de ellas no contendrá a la media de la población.
    d. Ninguna de las anteriores

9. Si calculamos el intervalo de confianza (al 95%) para la diferencia de medias de alturas entre hombres y mujeres a partir de los datos de una muestra de 50 individuos y obtenemos (2.95,5.25), ¿cuál sería el resultado de un contraste de hipótesis con la significancia equivalente, donde la hipótesis nula es que las medias de alturas en hombres y mujeres son iguales?

    a. Rechazar $H_0$
    b. Fallar en rechazar $H_0$
    c. Aceptar $H_0$
    d. Indeterminación

10. El factor de cobertura es el factor por el que multiplicamos la estimación del error típico para determinar el intervalo de confianza por los métodos tradicionales en una muestra pequeña cuando la desviación típica de la población no es conocida, $\delta^{*} \pm f SE$. f se calcula a partir de los cuantiles de la distribución t. Para calcular un intervalo de confianza con un nivel de confianza del 90%, f vendrá dado por

    a. qt(0.9,df=n-1)
    b. qt(0.95,df=n-1)
    c. qt(0.85,df=n-1)
    d. qt(0.1,df=n-1)
    


# Bibliografía

- Chester Ismay, Albert Y. Kim.  ModernDive: an Introduction to Statistical and Data Sciences via R (2017)
<https://ismayc.github.io/moderndiver-book/references.html>

- Chihara, Laura M., and Tim C. Hesterberg.  Mathematical Statistics with Resampling and R. John Wiley & Sons (2011).

- Shravan Vasishth Lectures. <http://www.ling.uni-potsdam.de/~vasishth/>


# Soluciones

1-c;2-c;3-b;4-d;5-c;6-a;7-a;8-c;9-a;10-b


# Apéndice I: Distribuciones de probabilidad

## Variable aleatoria

Una **variable aleatoria** $X$ es una función $X : S \rightarrow \mathbb{R}$ que asocia a distintos eventos de carácter aleatorio $\omega \in S$  un solo número $X(\omega) = x$.

- $S$ se denomina espacio muestral y representa el conjunto  los diferentes sucesos del proceso aleatorio.
- $S_X$ son todos los posibles valores de  X, y se denomina soporte de X. 

**Ejemplo**: número de lanzamientos de una moneda hasta que sale cara. H representa una cara (head) y T una cruz (toss). 

- $X: \omega \rightarrow x$
- $\omega$: H, TH, TTH, \dots 
- $x=0,1,2,\dots; x \in S_X$

Para el ejemplo el valor numérico de la variable aleatoria asociada a los distintos eventos será: 

$H \rightarrow 1$

$TH \rightarrow 2$

$TTH \rightarrow 3$

$\vdots$


## Función Densidad 

Toda variable aleatoria X tiene asociada una **función de probabilidad* ** (PMF  por sus siglas en inglés). 
Las variables continuas tienen una **función de distribución de probabilidad** (PDF  por sus siglas en inglés). Les llamaremos a las dos PDF por simplicidad.

$$
p(x) = P(X(\omega) = x), x \in S_X
$$

Esta función nos dice la probabilidad de sacar una cara en 1, 2, 3,  $\dots$ tiradas.


## Función de distribución

La  **función de distribución de probabilidad acumulada** (CDF por sus siglas en inglés) en el caso discreto se define como

$$
F(a)=\sum_{x \leq a} p(x)
$$

En el caso continuo

$$
F(a)=\int_{-\infty}^{a} p(x)\, dx
$$

En el ejemplo, la CDF nos dice la probabilidad  de obtener cara en 1 o menos tiradas, en dos o menos tiradas, etc.


## Ejemplo discreto: Variable aleatoria binomial

La distribución binomial es el ejemplo más habitual de la distribución de probabilidad de variable discreta. 
Ilustrémosla mediante un ejemplo:

- Tiramos una moneda (posiblemente trucada) $n=10$  veces.
- En cada tirada hay dos posibles resultados Cara(H) o Cruz(T), cada uno con probabilidad
$\theta$ y $(1-\theta)$ respectivamente. 

La probabilidad de obtener $x$ caras en $n$ tiradas se define mediante la PMF: 

$$
p_X(x)=P(X=x) = {n \choose x} \theta^x (1-\theta)^{n-x} 
$$ 

${n \choose x}$ es un número combinatorio que representa el número de combinaciones de elegir x elementos de un conjunto de n elementos sin importarnos el orden.  

Para $\theta=0.5$ (moneda no trucada) y $n=10$, la función de densidad de probabilidad binomial será

```{r, fig.height = 4.5}
# dbinom devuelve la pdf de una distribución binomial
plot(0:10,dbinom(0:10,10,0.5),type="o")
# Con choose calcualmos lo mismo que con dbinom pero de forma explícita
lines(1:10,choose(10,1:10)*0.5^(1:9) * 0.5^(9:1),col=2)
```

Con ella podemos responder a diferentes preguntas.

¿Cuál es la probabilidad de obtener x o menos caras? Esto nos lo responde la CDF

```{r, fig.height = 4.8}
#pbinom  calcula la distribución acumulada binomial
plot(0:10,pbinom(0:10,10,0.5),type="o",xlab="x",
     ylab=expression(P(X<=x)),main="CDF")
```


¿Cuál es la probabilidad de obtener 6,7 u 8  caras? 

Se puede calcular con la PDF mediante

```{r}
sum(dbinom(6:8,10,0.5))
```

o con la CDF mediante
  
```{r}
pbinom(8,10,0.5) - pbinom(5,10,0.5)
```


## Ejemplo distribución continua: Variable aleatoria normal

La función densidad (PDF) de la distribución Normal (o Gaussiana) es:

$$
f_{X}(x)=\frac{1}{\sqrt{2\pi \sigma^2}}
e^{-\frac{1}{2}\frac{(x-\mu)^{2}}{\sigma^2}},\quad -\infty < x < \infty
$$

Escribiremos $X\sim \mathcal{N}(mean=\mu,\,sd=\sigma)$.

Las funciones asociadas en R para la PDF son `dnorm(x, mean = 0, sd = 1)`, y para CDF es `pnorm`. Por defecto $\mu$ y $\sigma$ son 0 y 1 respectivamente.


Gráficamente la PDF tiene la archiconocida forma de campana de Gauss.

```{r, fig.height=5}
plot(function(x) dnorm(x), -3, 3,
      main = "Normal density",ylim=c(0,.4),
              ylab="density",xlab="X")
```


Las probabilidades se calculan como el área  bajo la curva

```{r,echo=FALSE}
plot.prob<-function(x,
                           x.min,
                           x.max,
                           prob,
                           mean,
                           sd,
                           gray.level,main){

        plot(x,dnorm(x,mean,sd), 
                     type = "l",xlab="",
             ylab="",main=main)
        abline(h = 0)

## shade X<x    
    x1 = seq(x.min, qnorm(prob), abs(prob)/5)
    y1 = dnorm(x1, mean, sd)

    polygon(c(x1, rev(x1)), 
            c(rep(0, length(x1)), rev(y1)), 
            col = gray.level)
  }

shadenormal<- 
function (prob=0.5,
          gray1=gray(0.3),
          x.min=-6,
          x.max=abs(x.min),
          x = seq(x.min, x.max, 0.01),
          mean=0,
          sd=1,main="P(X<0)") 
{

     plot.prob(x=x,x.min=x.min,x.max=x.max,
               prob=prob,
                      mean=mean,sd=sd,
     gray.level=gray1,main=main)     
}

shadenormal(prob=0.975,main="P(X<1.96)")

```


Para calcular probabilidades es más sencillo si nos ayudamos de la CDF. Por ejemplo para una variable normal con media 0 y varianza 1.

```{r}
## Area entre  +infty y -infty:
pnorm(Inf)-pnorm(-Inf)
## Area entre  -2 y 2
pnorm(2)-pnorm(-2)
## Area entre  -1 y 1
pnorm(1)-pnorm(-1)
```

 
Podemos ir en la dirección contraria. Dada un probabilidad $p$, podemos encontrar el cuantil $x$ de $\mathcal{N}(\mu,\sigma)$ tal que $P(X<x)=p$.

Por ejemplo, el cuantil $x$ dada  $X\sim  \mathcal{N}(\mu=500,\sigma=100)$  tal que  $P(X<x)=0.975$ es

```{r}
qnorm(0.975,mean=500,sd=100)
```

Esto es de gran utilidad en inferencia estadística, ya que se usa en la estimación de intervalos de confianza.


### Variable normal estandarizada o unitaria 

Si $X$ está normalmente distribuida con parámetros  $\mu$ y $\sigma$, entonces 

$$
Z=\frac{(X-\mu)}{\sigma}
$$  

está normalmente distribuida con parámetros $\mu=0,\sigma = 1$.

Llamaremos $\Phi (x)$ a la  CDF de  $\mathcal{N}(0,1)$:

$$
\Phi (x)=\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x}  e^{\frac{-y^2}{2}} \, dy 
$$


En R,  $\Phi(2)$ se calcula como:

```{r}
pnorm(2)
```

Para $x$ negativas $\Phi(-x)= 1- \Phi (x)$

```{r}
pnorm(-2)
1-pnorm(2)
## alternativamente
pnorm(2,lower.tail=F)
```

Puede utilizarse la CDF estandarizada para hacer cálculos sobe una variable  $X$ normal pero con  $\mu$ y $\sigma$ distintos de 0 y 1.

$$
F_X(a) = P\{ X\leq a \} = P\left( \frac{X - \mu}{\sigma} \leq \frac{a - \mu}{\sigma}\right) = \Phi\left( \frac{a - \mu}{\sigma} \right)
$$

Comparémoslo mediante R

```{r}
pnorm(6,mean=5,sd=2)
pnorm((6-5)/2)

```


### dnorm, pnorm, qnorm, rnorm

- En R, las funciones asociadas a la distribución normal son:
    - dnorm: la  PDF
    - pnorm: la  CDF
    - qnorm: la inversa de la CDF
    - rnorm: generador de números aleatorios 
    
- Otras distribuciones tienen funciones análogas:
    - Binomial: dbinom, pbinom, qbinom
    - t-Student: dt, pt, qt
    - uniforme: dunif   , punif,qunif,runif
    - exponencial: dexp, pexp, ...
    - weibull: dweibull, pweibull, ...
    - $\chi^2$ : dchisq, pchisq, ..
    - F: df(),pf(), ...


### Relación entre distribución binomial y normal

Conforme n tiende a $\infty$ la distribución binomial puede aproximarse por una normal con media $np$ y varianza $np(1-p)$


Distribuciones binomiales para p=0.5 y distinto valor de $n$:

```{r,echo=FALSE,fig.height=5}
oldpar=par()
par(mfrow=c(2,2),mex=0.8,mar=c(3,3,2,1)+.1)
p=0.5
for(n in c(5,10,20,100)){
 x = 1:n
 if(n==5)
   plot(x,dbinom(x,n,p),type="o",xlab="",ylab="pdf",ylim=c(0,0.35),main=paste("n=",n,"p=",p))
 else
   plot(x,dbinom(x,n,p),type="o",xlab="",ylab="pdf",main=paste("n=",n,"p=",p))
 curve(dnorm(x,mean=n*p,sd=sqrt(n*p*(1-p))),0,n,col=2,add=TRUE)
 if(n==5)
   legend("bottomright",c("Binomial","Normal"),col=1:2,lty=1,pch=c(1,NA),cex=0.7)
}
 par=oldpar
```

Distribuciones binomiales para p=0.9 y distinto valor de $n$:

```{r,echo=FALSE,fig.height=5}
oldpar=par()
par(mfrow=c(2,2),mex=0.8,mar=c(3,3,2,1)+.1)
p=0.9
for(n in c(5,10,20,100)){
 x = 1:n
 plot(x,dbinom(x,n,p),type="o",xlab="",ylab="pdf",main=paste("n=",n,"p=",p))
 curve(dnorm(x,mean=n*p,sd=sqrt(n*p*(1-p))),0,n,col=2,add=TRUE)
 if(n==5)
   legend("bottomright",c("Binomial","Normal"),col=1:2,lty=1,pch=c(1,NA),cex=0.7)
}
 par=oldpar
```

Se observa que para valores de $p$ cercanos a 0 o a 1, se tarda más en alcanzar el comportamiento "normal".

Un criterio sencillo para decidir si la distribución binomial puede considerarse normal es si se cumple que 

$$np \pm 3\sqrt{np(1-p)} \in (0,n)$$ 

Es decir si el intervalo $(np - 3\sqrt{np(1-p)},np + 3\sqrt{np(1-p)})$ está contenido dentro del intervalo $(0,n)$

Por ejemplo para p=0.9 veamos como es este intervalo frente al número de eventos $n$. Para $n<81$ no se cumple la condición

```{r}
p=0.9
for(n in c(10,50, 81, 100))
  cat(n,p*n - 3*sqrt(n*p*(1-p)),p*n + 3*sqrt(n*p*(1-p)),"\n")

```

En cambio para $p=0.5$, a partir de $n=9$ se cumple la condición

```{r}
p=0.5
for(n in c(5,9, 20, 100))
  cat(n,p*n - 3*sqrt(n*p*(1-p)),p*n + 3*sqrt(n*p*(1-p)),"\n")
```

## Distribución t de Student

La definición  de la distribución t de Student se basa en la distribución de la media muestral cuando no se conoce la desviación típica de la población y se infiere a partir de la desviación típica de una muestra.

Supongamos una muestra de tamaño $n$ de una variable distribuida normalmente con media $\mu$ y desviación estándar $\sigma$. Entonces la cantidad:

$$
T=\frac{\bar{x}-\mu}{s/\sqrt{n}}
$$

está  distribuida según una distribución t de Student con n-1 grados de libertad.

$\bar{x}$ y $s$ son la media y la desviación típica de una muestra.


De forma analítica puede definirse la función densidad de probabilidad (PDF), en función de los grados de libertad, de la siguiente forma:


$$
f_{X}(x,r)=\frac{\Gamma[(r+1)/2]}{\sqrt{r\pi}\ \Gamma(r/2)}\left(1+\frac{x^{2}}{r}\right)^{-(r+1)/2},\quad -\infty < x < \infty.
$$

donde  $\Gamma$ se refiere a la función Gamma y $r=n-1$ representa los grados de libertad,

Como ya hemos visto, la forma de la distribución t es parecida a la distribución normal, pero el tamaño de las colas es considerablemente más grande para valores pequeños de los grados de libertad, como puede verse en el siguiente gráfico.

```{r}
ggplot(data.frame(x=seq(-4,4,.1)),aes(x)) + 
  stat_function(aes(color="Normal"),fun=dnorm, geom="line", size=1.5)+
  stat_function(aes(color="t df=3"),fun=dt,args=list(df=3),geom="line")+
  stat_function(aes(color="t df=10"),fun=dt,args=list(df=10),geom="line")+
  stat_function(aes(color="t df=20"),fun=dt,args=list(df=20),geom="line") +
  stat_function(aes(color="t df=100"),fun=dt,args=list(df=20),geom="line") +
  labs(y="",color="Distribución") + scale_color_brewer(palette = "Set1")

```


---
title: "Fundamentos del Aprendizaje Estadístico"
author: "MasterD"
output:
  html_document:
    highlight: tango
    theme: united
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
  pdf_document:
    toc: yes
  revealjs::revealjs_presentation:
    center: yes
    css: style.css
    highlight: pygments
    theme: sky
  word_document:
    reference_docx: www/plantillaMasterD_basica5.docx
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
options(width=100)

library(reshape2)
library(dplyr)
library(ggplot2)
library(grid)
#library(ElemStatLearn) #Se puede descargar aqui https://cran.r-project.org/web/packages/ElemStatLearn/index.html
install.packages("C:/Users/alber/Downloads/ElemStatLearn_2015.6.26.tar.gz",
                 repos = NULL, type = "source")  #Lo instalamos en zip desde descargas

library(GGally)
library(png)
library(igraph)
library(ISLR)
library(splines)
library(FNN)
library(MASS)
library(class)
```

# Tu reto en esta unidad

Cuando se habla de ciencia de datos y en particular de aprendizaje estadístico, minería de datos o machine learning en lo que se piensa habitualmente es el construir en modelos que sean capaces de predecir el valor de una magnitud en el futuro o en situaciones desconocidas. 

Para ello existen numerosas técnicas y herramientas, pero antes de conocer el detalle de su funcionamiento es necesario conocer los aspectos comunes de todas ellas y como saber elegir cual es la herramienta adecuada para cada tipo de problema. 

# Introducción

Hasta ahora en este curso hemos aprendido a capturar datos, transformarlos, tabularlos, resumirlos y visualizarlos. Hemos aprendido técnicas de análisis exploratorio para empezar a descubrir las relaciones que existen entre ellos y por último hemos aprendido a hacer inferencias sobre el valor de una magnitud en una población a partir de datos recogidos en una muestra.

Ahora queremos ir un paso más allá, queremos aprender un conjunto de técnicas que nos permitan entender las relaciones que existen entre los datos y a esto es a lo que vamos a denominar aprendizaje estadístico.

Estas herramientas pueden ser supervisadas o no supervisadas. Hablando en general, el aprendizaje estadístico supervisado implica la construcción de un modelo estadístico para predecir o estimar el valor de una determinada característica basado en uno o más datos de entrada. Con el aprendizaje estadístico no supervisado, tenemos datos de entrada pero no hay ninguna salida supervisada. No obstante, podemos aprender las relaciones y las estructuras presentes en los datos. 

Problemas de esta naturaleza los encontramos en campos tan diversos como los negocios, la medicina, la astrofísica o las políticas públicas. Para empezar vamos a considerar una serie de ejemplos de problemas reales de aprendizaje estadístico o proyectos de ciencia de datos que nos van a ayudar a entender mejor del tipo de problemas que queremos resolver.

  
# Ejemplos

## Precio de la vivienda

Una empresa inmobiliaria necesita un modelo para tasar viviendas según el precio real de mercado. 
Para ello recopila ofertas de diferentes portales que ofreces datos sobre 

- Precio de venta
- Superficie
- Tipo de vivienda
- Antigüedad
- Número de baños
- Localización


Veamos algunas de las relaciones entre el precio y otras características de las viviendas


```{r,cache=TRUE, echo=FALSE}

load("../../Datasets/apidat03.RData")

a<- ggplot(apidat03 %>% filter(cp<=50010,size<500,property_type %in% c("house","flat"))) + 
  geom_point(aes(size,price_high,col=factor(cp)),size=0.5) + 
  geom_smooth(aes(size,price_high),span=20,method="loess") + facet_wrap(~property_type) +
  ylab("Precio") + xlab("Superficie") + scale_color_hue("CP")

b <- ggplot(apidat03 %>% filter(cp<=50010)) + 
  geom_point(aes(as.numeric(construction_year),price_high/size,col=factor(cp))) + 
  geom_smooth(aes(as.numeric(construction_year),price_high/size))  + ylim(0,6000) +
  ylab("Precio/m2") + xlab("Año Construcción") + theme(legend.position="none") 

c <- ggplot(apidat03) + 
  geom_boxplot(aes(as.factor(cp),price_high/size),col="orange") + xlab("Código Postal") + 
  ylab("Precio/m2") +coord_cartesian(ylim=c(0,6000)) + theme(axis.text.x=element_text(angle=90))
  
grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 2)))
vplayout <- function(x, y)
  viewport(layout.pos.row = x, layout.pos.col = y)

print(a, vp = vplayout(1, 1:2))
print(b, vp = vplayout(2, 1))
print(c, vp = vplayout(2, 2))


```

Un factor importante para el precio de una vivienda es su situación geográfica

```{r, out.height= 400, fig.retina = NULL,echo=FALSE}
knitr::include_graphics("img/precio_m2_sscc.png")
```

El objetivo es desarrollar un modelo  que prediga el precio de las viviendas y que capture de forma precisa las relaciones que hemos esbozado en los gráficos anteriores.


## Cancer de Próstata {#smaller}

El objetivo es establecer una relación entre el PSA (Prostate Specific Antigen),  que se mide mediante un análisis de sangre,  con una serie de medidas clínicas. Los datos se tomaron en una serie de pacientes a los que se iba a realizar una prostatectomía (extracción de la próstata).

```{r,cache=TRUE,echo=FALSE,fig.height=5}
ggpairs(prostate %>% mutate(train=factor(train)),1:9)
```

Se observan correlaciones fuertes entre el PSA y las dimensiones de los tumores. Esto indica que es posible desarrollar un modelo que establezca una relación entre los niveles de PSA en sangre y la existencia de cáncer de próstata. Esta técnica lleva ya muchos años en funcionamiento para la detección temprana de cáncer de próstata. 

## Reconocimiento de Texto escrito

El objetivo es crear un modelo que reconozca automáticamente dígitos escritos en sobres de correos. Las entradas del modelo son las opacidades de los 256 pixels con los que se codifica la imagen de un dígito escaneado. 

El modelo debe de ser capaz de determinar de que dígito se trata a partir de los datos de la imagen escaneada.

![](img/zip_codes.png)


## Sistemas de recomendación

El objetivo es construir un sistema de recomendación de películas.
El modelo debe predecir la puntuación del 1 al 5 de un usuario a una película a partir de su historial de puntuaciones por comparación con otros usuarios

Película         |Ángel| Jesús| Javier| Pilar| María| ...|
-----------------|-----|-------|-------|-------| ------  |------|
Star Wars        |2| 5| 4| 4| 3 | ...|
Harry Potter     | 3| 4| 3| 3| ?| . . .|
Pretty Woman     | 4| ?| 2| ?| 5| . . .|
Titanic          | 5| ?| 2| 1|3 |. . . |
Lord of the Rings| ?| 5| 5| 4| 4| . . .|


Fue muy famoso hace unos años el desafío que lanzo Netflix, denominado Netflix Prize.
El desafío consintió en predecir la valoración real de películas realizada por sus usuarios con una mejora de un 10% sobre su algoritmo previo. Aquí algunos datos curiosos sobre el concurso:

- Conjunto de entrenamiento:valoraciones de 480000 usuarios de  18000 películas, con 98.7% de valoraciones ausentes
- Conjunto de Test: 1 408 789 valoraciones
- Premio: 1 millón  de dolares
- Fechas: Octubre 2006 - Agosto 2009.
- Métodos ganadores : Variaciones  del  SVD (Singular Value Decomposition) y  k-nearest neighbors (Bell & Koren, 2008).
- Más info: <http://www.netflixprize.com/>, <https://en.wikipedia.org/wiki/Netflix_Prize>
- Y finalmente Netflix no uso la solución ganadora: <https://www.wired.com/2012/04/netflix-prize-costs/>


## Modelización de Salarios

Una multinacional está pensando en establecerse en España y pretende evaluar el coste salarial. Mediante el análisis de la Encuesta de Estructura Salarial que elabora el INE quiere modelar los salarios en función de variables explicativas como el nivel de formación, edad, antigüedad, etc.

```{r,cache=TRUE,fig.height=4.2,echo=FALSE}
ees= MicroDatosEs::ees2010("../../Datasets/ine/EES14_WEB")
ees=as.data.frame(ees)
levels(ees$ANOS2) = c(15,25,35,45,55,65)
ees <- ees %>% mutate(ESTU_factor = as.factor(ees$ESTU))  
levels(ees$ESTU_factor) <- c("Sin estudios", "Educación primaria" , "Educación secundaria I" , "Educación secundaria II" , 
                             "Formación profesional de grado medio",  "Formación profesional de grado superior" , "Diplomados universitarios o equivalente"  )
ees = ees %>% filter(TIPOJOR=="Tiempo completo",SALBRUTO<1e5) %>% sample_n(2000)

a=ggplot(ees) + geom_boxplot(aes(ESTU_factor,SALBRUTO)) + theme(axis.text.x=element_text(color="white"))
b=ggplot(ees) + geom_point(aes(ANOANTI,SALBRUTO)) + xlab("Antiguedad")
c=ggplot(ees) + geom_point(aes(ANOS2,SALBRUTO),position=position_jitter(width = 2)) +
  geom_smooth(aes(as.numeric(factor(ANOS2)),SALBRUTO),method="loess") +
  xlab("Edad")
d=ggplot(ees) + geom_boxplot(aes(CNO1,SALBRUTO)) + theme(axis.text.x=element_text(size=6)) + 
  xlab("CNAE")


grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 2)))
vplayout <- function(x, y)
viewport(layout.pos.row = x, layout.pos.col = y)
print(a, vp = vplayout(1, 1))
print(b, vp = vplayout(1, 2))
print(c, vp = vplayout(2, 1))
print(d, vp = vplayout(2, 2))

```

Con este modelo puede evaluar el coste salarial de abrir una sede en España, en función del tipo de perfiles de trabajadores que necesita y compáralo con el coste salarial de la apertura en otros países. 


# Qué es el Aprendizaje Estadístico


Usemos el último ejemplo sobre predicción de salarios como vehículo para introducir algunos conceptos clave del aprendizaje estadístico. Concretamente, queremos predecir el salario en función de la edad, la experiencia y otras variables. 

En las gráficas siguientes se muestran los datos observados a través de una encuesta.

```{r,echo=FALSE,fig.height=3.2}
income=read.csv("../../Datasets/Income2.csv")
a=ggplot(income) + geom_point(aes(Education,Income)) + 
  geom_smooth(aes(Education,Income),span=1.5,se=FALSE,method="loess")
b=ggplot(income) + geom_point(aes(Seniority,Income)) + geom_smooth(aes(Seniority,Income),span=2,se=FALSE,method="loess")
grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 2)))
vplayout <- function(x, y)
viewport(layout.pos.row = x, layout.pos.col = y)
print(a, vp = vplayout(1, 1))
print(b, vp = vplayout(1, 2))
```

En la figura vemos la relación por un lado entre los salarios y los años de educación,  y por otro lado entre los salarios y la experiencia. Pero lo que queremos construir es un modelo conjunto, es decir queremos encontrar una función matemática de varias variables que nos proporcione el valor más preciso posible del salario esperado en función del nivel de educación, experiencia, edad, etc. 

$$ Income \sim f(Education,Senority,...) $$


## Notación

Aprovechemos ahora para definir algunos términos clave y fijar la notación matemática que emplearemos. 

Denotaremos por $n$ el número de observaciones y por $p$ el número de variables predictoras

*Income* es la **respuesta** o **target** que queremos predecir. Genéricamente nos referiremos a la respuesta como $Y$.
En realidad $Y$ es un vector de dimensión $n$

$$
  Y  = \;
   \begin{pmatrix}
      y_{1}  \\
      y_{2}  \\
      ..     \\
      y_{n}
  \end{pmatrix}
$$



*Education* es una **característica**, un **input** o un **predictor**. Le llamaremos $X_1$, así como a *Seniority* le llamaremos $X_2$ y así sucesivamente con otras variables que podamos utilizar. Al igual que antes los $X_i$ son vectores con n componentes.

Podemos referirnos al conjunto de  inputs globalmente como un vector $X$

$$
  X  = \left(X_1,X_2,\dots,X_p \right)
$$
que en realidad es una matriz de dimensiones $n \times p$.

$$
  X  = \;
   \begin{pmatrix}
      x_{11} &  x_{12} & \cdots & x_{1p} \\
      x_{21} &  x_{22} & \cdots & x_{2p} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      x_{n1} &  x_{n1} & \cdots & x_{np} \\
  \end{pmatrix}
$$

donde cada fila corresponde los valores de las $p$ variables asociadas a  una observación y cada columna corresponde a las $n$ observaciones de una de cada una de las variables predictoras.  

Si no estás familiarizado con las matrices puedes pensar en ellas como una hoja de cálculo organizada por filas y columnas.



## Modelo

Asumimos que existe una relación entre la respuesta cuantitativa $Y$ (Income/Salario) y los diferentes predictores $X  = \left(X_1,X_2,\dots,X_p \right)$

Escribiremos el modelo como

$$Y = f (X) + \epsilon$$

donde $f$ es una función definida, aunque desconocida, que captura toda la posible dependencia sistemática entre el *Income*, $Y$,  y los predictores, $X$ (Education, Income, Age, ...) 

$\epsilon$ es un término de error aleatorio que captura las  fuentes discrepancia entre
modelo y realidad, como la influencia de otros factores que no hemos tenido en cuenta o al 
puro azar inherente a los procesos del mundo real.

El Aprendizaje Estadístico es esencialmente una serie de técnicas para estimar $f$.  


En caso de 2 predictores $f$ es una superficie en dos dimensiones, que deberemos estimar en función de los datos disponibles

```{r, echo=FALSE,fig.height=5.5,results='hide',message=FALSE}

library(splines)

mod=lm(Income ~ ns(Education,df=3) + ns(Seniority,df=3),data=income)

x=seq(10,22,1)
y=seq(18,190,5)
new.data = expand.grid(Education=x,Seniority=y)
new.data$pred = predict(mod,newdata = new.data)
income$pred = mod$fitted.values

library(plot3D)

z=matrix(new.data$pred,nrow=length(x))
persp3D(x,y,z,facets=TRUE, border="grey30",
        zlab="Income",xlab="Education", ylab="Seniority",theta=30, phi=30,colkey = FALSE,r=5,d=5)
scatter3D(income$Education, income$Seniority, income$Income,col=1,add=TRUE,pch=19,cex=0.7)
apply(income,1,function(row) lines3D(x=rep(row[2],2),y=rep(row[3],2),z=c(row[4],row[5]),
                                     colkey=FALSE,add=TRUE,col=1))

```


## Para que nos sirve estimar f

Hay dos razones principales para estimar f: predicción e inferencia. Discutimos cada una a continuación:

### Predicción

En muchas situaciones, estamos interesados en predecir un valor futuro o hipotético de la variable respuesta a partir de los valores de las variables de entrada $X$ que si que están disponibles.

En ese caso si tenemos una estimación de $f$, que llamaremos $\hat{f}$, podemos predecir $Y$ mediante

$$
\hat{Y} = \hat{f}(X)
$$

Observa que denotamos con $\hat{Y}$ al resultado de la predicción para $Y$.
En el contexto de la predicción, $f$ se trata a menudo como una caja negra, en el sentido
de que habitualmente no nos preocupa demasiado la forma exacta de $f$, siempre que
produzca predicciones precisas para $Y$.

Como ejemplo, supongamos que $X_1, \dots , X_p$ son características del paciente que se miden a través de análisis de sangre,  e $Y$ es una variable que representa el riesgo del paciente de reaccionar de forma adversa frente al suministro de un nuevo medicamento. Es natural querer predecir $Y$ usando $X$, ya que de esta manera podemos evitar suministrar el fármaco a pacientes que presenten un riesgo alto de reaccionar adversamente, es decir a aquellos para los que la estimación de $Y$ sea alta.

La precisión en la estimación de $\hat{Y}$ como predicción para $Y$ depende de dos cantidades, que llamaremos el error reducible y el error irreducible. 

En general, $\hat{f}$ no será una estimación perfecta para $f$, y esta inexactitud introducirá algo de error. Este **error es reducible** porque podemos mejorar la precisión de $\hat{f}$ utilizando una técnica de aprendizaje estadístico más apropiada.  

Sin embargo, incluso si fuera posible realizar una estimación perfecta para
$f$,  nuestra predicción todavía tendría algún error. Esto es porque $Y$ es también una función de $\epsilon$, que, por definición, no puede predecirse a partir de $X$. 

Por lo tanto, la variabilidad asociada con $\epsilon$ también afecta la precisión de nuestras predicciones. Esto es conocido como el **error irreducible**, porque no importa cómo de bien estimemos f  que no podremos reducir el error introducido por $\epsilon$.

¿Por qué existe siempre este  error irreducible?  \epsilon puede depender de  variables que no hemos medido pero que son útiles para predecir $Y$: puesto que no las medimos, $f$ no puede usarlas para realizar predicciones. Pero además $\epsilon$ puede contener variaciones debidas a efectos no medibles. Por ejemplo, el riesgo a una reacción puede variar para un paciente dado en un día dado, dependiendo de la variación de la fabricación en el propio medicamento o el estado general de salud del paciente ese día.


El objetivo del aprendizaje estadístico está principalmente en las técnicas para estimar $f$ con el objetivo de minimizar el error reducible. Aunque como vamos a ver un poco más adelante en esta unidad estas estimaciones también dependen en parte del error irreducible. 

Además, es importante tener en cuenta que el error irreducible siempre proporcionará 
un límite superior en la exactitud de nuestras predicciones para $Y$, y  desafortunadamente este límite es casi siempre desconocido en la práctica.

### Inferencia

A menudo estamos más interesados  en entender la manera en que $Y$ se ve afectado por los cambios en  las variables predictoras. En esta situación, deseamos estimar $f$, pero nuestra meta no es necesariamente hacer predicciones precisas para $Y$. En cambio, queremos entender
la relación existente entre $X$ e $Y$, o más específicamente cómo cambia $Y$ como función de $X_1,\dots , X_p$. 

En este caso no debemos considerar $f$  como una caja negra, porque necesitamos saber su forma exacta. 

En este contexto, uno se plantea preguntas del tipo:

-*¿Qué predictores están asociados con la respuesta?* A menudo nos encontramos que solo una pequeña fracción de los predictores tienen relación real con los la respuesta. Identificar los pocos predictores importantes entre un conjunto grande de variables puede ser extremadamente útil para muchas aplicaciones. 

-*¿Cuál es la relación entre la respuesta y cada predictor?*
Algunos predictores pueden tener una correlación positiva con $Y$, en el sentido
de que el aumento del predictor está asociado con valores crecientes de $Y$. 
Otros predictores pueden tener la relación opuesta.  Por otro lado, dependiendo de la complejidad de $f$, la relación entre la respuesta y un predictor dado también puede depender de los valores de los otros predictores.

-*¿Puede resumirse adecuadamente la relación entre $Y$ y cada predictor utilizando una ecuación lineal, o es la relación más complicada?* Históricamente, la mayoría de los métodos para estimar $f$ han tomado una forma lineal. En algunas situaciones, tal suposición es razonable e incluso deseable. Pero a menudo la verdadera relación es más complicada, en cuyo caso un modelo lineal puede no proporcionar una representación precisa de la relación entre las variables de entrada y salida.


## ¿Como estimamos f?

Antes de aprender posibles técnicas para estimar $f$, deberíamos preguntarnos si existe una $f$ ideal.

Consideremos una variable $Y$ que queremos predecir a partir de otra variable $X$ tal y como muestra la gráfica siguiente

```{r,fig.height=3.7,echo=FALSE}
x=rnorm(400,0,3)
x=x[abs(x)<5]
y= x^3-2*x^2 + x + rnorm(length(x),0,15)
plot(x,y,col=rgb(0,0,0,0.5))
curve(x^3-2*x^2 + x,-5,5,col=2,add=TRUE)
points(2,2^3-2*2^2 + 2, pch=19,cex=1.5,col=2)
abline(v=2,col=2)
```
 
¿Existe una $f$ ideal? y en particular, ¿cuál es un buen valor de $f$ para un valor dado de X como X=2?

Un buen valor es $f (2) = E(Y |X = 2)$ que significa  el valor esperado de Y dado  X = 2. Es decir si tuviéramos un número infinito (o muy grande) de observaciones con $X=2$ el valor esperado es el promedio de los valores de $Y$ correspondientes a las observaciones con $X$ exactamente igual a 2. 

Esta $f (x) = E(Y |X = x)$ ideal se llama **función de regresión**

```{r,fig.height=3.7,echo=FALSE}
x=rnorm(400,0,3)
x=x[abs(x)<5]
y= x^3-2*x^2 + x + rnorm(length(x),0,15)
plot(x,y,col=rgb(0,0,0,0.5))
curve(x^3-2*x^2 + x,-5,5,col=2,add=TRUE)
points(2,2^3-2*2^2 + 2, pch=19,cex=1.5,col=2)
abline(v=2,col=2)
```

En múltiples dimensiones $f(x)$ se define de forma equivalente como 

$$f(x) = f (x 1 , x 2 , x 3 ) = E(Y |X_1 = x_1 ,X_2=x_2, X 3=x_3)$$


$\epsilon= Y − f (x)$ es el error irreducible. Es decir, aunque conociéramos la $f(x)$ ideal, seguiríamos cometiendo errores de predicción, ya que para cada $X=x$ hay típicamente una distribución de valores de $Y$.

En todo caso, para cualquier estimación  $\hat{f}(x)$  de $f(x)$, tenemos 

$$  E[(Y − \hat{f}(X))^2 \,|\,X = x] =\underbrace{[f(x) − 
  \hat{f}(x)]^2}_{Reducible} + \underbrace{Var(\epsilon)}_{Irreducible}$$


Sin embargo, en la práctica, para estimar $f(X=2)$, típicamente tenemos muy pocos (o ningún) puntos con X=2 exactamente, por tanto no podemos calcular $E(Y |X = x)$


Podemos relajar ligeramente la condición a $\hat{f}(x) = Ave(Y |X \in \mathcal{N}(x))$

donde  $\mathcal{N}(x)$ es algún conjunto de puntos cercanos a $x$, y por $Ave$ denotamos la media. La clave será como de grande es ese conjunto de puntos cercanos que elegimos. 

Veamos un ejemplo de estimación de $f$ usando R. Dada una distribución de puntos (X,Y) generados según el modelo:

$$ Y= x^3-2 x^2 + x + \epsilon$$
donde $\epsilon$ es una distribución Normal con media 0 y $\sigma=20$

La estimación la realizamos escogiendo $\mathcal{N}(x)$ como un intervalo de longitud 1 centrado en $x$.

```{r,fig.height=3.7,echo=FALSE}
set.seed(5)
# Generamos los puntos x e y para que tengan una relación y= x^3-2*x^2 + x + epsilon
x=runif(500,-5,5)
# epsilon se recrea generando puntos aleatorios segun una distribución normal con media 0 y sd 20 (en este caso)
y= x^3-2*x^2 + x + rnorm(length(x),0,20)
#plot(x,y,col=rgb(0,0,0,0.5))
curve(x^3-2*x^2 + x,-5,5,col=2,add=TRUE)
#points(2,2^3-2*2^2 + 2, pch=19,cex=1.5,col=2)
abline(v=2,col=2)
abline(v=1.5,col=2,lty=2)
abline(v=2.5,col=2,lty=2)

#Estimo la f promediando los valores de y cogiendo un entorno de longitud delta
delta=2
xx=seq(-5+delta,5-delta,0.25)
yy=xx
for(i in 1:length(xx)){
  l=xx[i] - delta/2
  u=xx[i] + delta/2
  ysel=y[x>l & x<u]
  if(length(ysel)>0)
    yy[i]=mean(ysel) 
  else
    yy[i]=NA
}
lines(xx,yy,col=3)

```

La curva roja representa la función $f$ ideal, mientras que la curva verde representa la estimación $\hat{f}$ con el método descrito.

La estimación anterior, es esencialmente el método de k-vecinos o k nearest neighbors (KNN).

El método de k-vecinos funciona bien si la dimensión de los inputs, $p$, es pequeña ($p \lt 4$). Sin embargo, puede ser muy impreciso para dimensiones grandes. La razón se denomina: "la maldición de la dimensión". Y viene a decir que conforme aumentamos la dimensionalidad del problema los primeros vecinos cada vez están más lejos.

A la hora de definir modelos para estimar $f$ podemos dividirlos en dos clases: paramétricos y no paramétricos.



## Modelos Paramétricos

Un modelo paramétrico es aquel que define $f(x)$ en función de una serie de parámetros $\beta_i$.

Un ejemplo típico es el modelo lineal

$$f_L(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + . . . + \beta_p X_p $$

- El modelo se define a partir de p+1 parámetros, los $\beta_i$.

- Estimaremos los parámetros *ajustando* el modelo a un conjunto de datos de entrenamiento.

- Aunque **casi nunca es correcto**, en muchas ocasiones sirve como una aproximación buena e interpretable de la función desconocida $f(X)$.


Para los dos ejemplos que se muestran en la gráfica: 

```{r,echo=FALSE,fig.height=5,fig.width=12}
par(mfrow=c(1,2))
x=runif(30,0,1)
y=2*x + 3 + rnorm(30,0,0.5)
mod=lm(y~x)
plot(x,y,ylim=c(1.9,6),xlim=c(-0.1,1.1))
abline(mod$coefficients,col=2)

x=runif(30,0,1)
y= 3*(x-0.5)^2 + 1 + rnorm(30,0,0.2)
mod=lm(y~ I(x^2) + x)
plot(x,y)
curve(mod$coefficients[1] + mod$coefficients[2]*x^2 + mod$coefficients[3]*x ,col=2,add=TRUE,0,1)
par(mfrow=c(1,1))
```


- Un modelo lineal $\hat{f}_L(X) = \beta_0 + \beta_1 X$ da un buen resultado en la figura de la izquierda.
- Uno cuadrático $\hat{f}_Q(X) = \beta_0 + \beta_1 X + \beta_2 X^2$ funciona mejor en la derecha.



En dos dimensiones el modelo lineal representa un plano, mientras que otros modelos representarán superficies más complejas. Veamos en el caso de los salarios, la representación gráfica de un modelo lineal con variables predictoras `(Education, Seniority)`.

$$income \approx  \beta_0 + \beta_1 \times education + \beta_2 \times seniority $$

```{r,echo=FALSE,cache=TRUE, fig.height=5,results='hide'}
mod=lm(Income ~ Education + Seniority,data=income)

x=seq(10,22,1)
y=seq(18,190,5)
new.data = expand.grid(Education=x,Seniority=y)
new.data$pred = predict(mod,newdata = new.data)
income$pred=mod$fitted.values

z=matrix(new.data$pred,nrow=length(x))

persp3D(x,y,z,facets=TRUE,
        zlab="Income",xlab="Education", ylab="Seniority",theta=30, phi=30,colkey = FALSE,r=5,d=5)
scatter3D(income$Education, income$Seniority, income$Income,col=1,add=TRUE,pch=19,cex=0.7)
apply(income,1,function(row) lines3D(x=rep(row[2],2),y=rep(row[3],2),z=c(row[4],row[5]),
                                     colkey=FALSE,add=TRUE,col=1))
```

## Modelos no-paramétricos

Los modelos no paramétricos no hacen asunciones explicitas sobre la forma de $f$. Se busca estimar f cerca de los datos sin ser el ajuste demasiado "rugoso". 

Ejemplos de modelos no paramétricos son el k-vecinos, los Splines o el LOESS (Ajuste polinómico local).

Tienen la ventaja de la flexibilidad de no tener que hacer asunciones previas sobre la forma de la función, sin embargo sus resultados, aunque a veces precisos, pueden ser difíciles de interpretar.

Los modelos paramétricos, en cambio, son computacionalmente sencillos, se trata de estimar unos pocos parámetros, y fáciles de interpretar. 


Veamos, un ejemplo de ajuste usando el método de "Splines". Siguiendo con el ejemplo de estimación de salarios, queremos ajustar el `Income` (salario) usando como predictores los años de educación (`Education`) y la experiencia (`Seniority`). 



**Autotexto**
Un modelo de regresión tipo splines consiste en dividir el rango de ajuste en diferentes regiones y en cada una de ellas hacer un ajuste a un polinomio de tercer grado. Se exige además que estos polinomios ajustados empalmen con suavidad. Cuanto mayor sea el número de regiones más flexible será el ajuste.  
**Fin Autotexto**

```{r,echo=FALSE,fig.height=4,results='hide'}

income=read.csv("../../Datasets/Income2.csv")

mod=lm(Income ~ ns(Education,df=3) + ns(Seniority,df=3) ,data=income)

x=seq(10,22,1)
y=seq(18,190,5)
new.data = expand.grid(Education=x,Seniority=y)
new.data$pred = predict(mod,newdata = new.data)
income$pred = mod$fitted.values


library(plot3D)

z=matrix(new.data$pred,nrow=length(x))

persp3D(x,y,z,facets=TRUE, border="grey30",
        zlab="Income",xlab="Education", ylab="Seniority",theta=30, phi=30,colkey = FALSE,r=5,d=5)
scatter3D(income$Education, income$Seniority, income$Income,col=1,add=TRUE,pch=19,cex=0.7)
#apply(income,1,function(row) lines3D(x=rep(row[2],2),y=rep(row[3],2),z=c(row[4],row[5]),
#                                     colkey=FALSE,add=TRUE,col=1))
```
 
No te preocupes ahora por como funcionan estos modelos, solo importa que tengas en cuenta que tienen un parámetro que son los grados de libertad (o número de regiones o puntos de ancla). Cuanto más grande sea este parámetro más flexibilidad tendrá el método para ajustar los puntos y la función $\hat{f}$ será una superficie más rugosa. Subiendo los grados de libertad obtenemos:

```{r,echo=FALSE,fig.height=4,results='hide'}

mod=lm(Income ~ ns(Education,df=8) + ns(Seniority,df=8) ,data=income)

x=seq(10,22,1)
y=seq(18,190,5)
new.data = expand.grid(Education=x,Seniority=y)
new.data$pred = predict(mod,newdata = new.data)
income$pred = mod$fitted.values

library(plot3D)

z=matrix(new.data$pred,nrow=length(x))

persp3D(x,y,z,facets=TRUE, border="grey30",
        zlab="Income",xlab="Education", ylab="Seniority",theta=30, phi=30,colkey = FALSE,r=5,d=5)
scatter3D(income$Education, income$Seniority, income$Income,col=1,add=TRUE,pch=19,cex=0.7)
apply(income,1,function(row) lines3D(x=rep(row[2],2),y=rep(row[3],2),z=c(row[4],row[5]),
                                     colkey=FALSE,add=TRUE,col=1))
```

Aquí la superficie ajusta casi de manera perfecta a los puntos de entrenamiento. Esto veremos que en realidad es un "sobre-ajuste" ya que al aplicar la función sobre datos distintos al conjunto de entrenamiento se cometen errores más grandes que usando una superficie más suave.

## Consideraciones y compromisos

Recapitulando, ya hemos esbozado alguna de las cuestiones más importantes a la hora de elegir un modelo de aprendizaje estadístico:

- Precisión versus Interpretación
    - Los modelos lineales y los paramétricos en general son fáciles de interpretar
    - Los no paramétricos no son fáciles de interpretar

- Flexibilidad del ajuste: Un modelo lineal puede ser demasiado sencillo y podemos encontrar en algunos casos situaciones de "sub-ajuste". En cambio, un modelo paramétrico con muchos parámetros o uno no paramétrico con mucha flexibilidad puede generar "sobre-ajustes". Ninguna de las dos situaciones es deseable y gran parte del arte del aprendizaje estadístico está en encontrar el punto intermedio óptimo entre estas dos situaciones.

- Simplicidad frente a la *caja negra*: Normalmente preferimos un modelo simple que involucre pocas variables frente a una "caja negra" que las involucre a todas, siempre que su precisión  sea parecida. En este compromiso dependerá mucho la aplicación que queramos dar al modelo, si estamos más interesados en la precisión o la inferencia o interpretación.

El siguiente gráfico  ilustra la competencia entre explicación y flexibilidad para diferentes métodos de aprendizaje estadístico.  


```{r, out.height= 550, out.width== 1000, fig.retina = NULL,echo=FALSE}
knitr::include_graphics("img/explic_vs_flex.png")
```

## Aprendizaje supervisado frente a no-supervisado

Los ejemplos que hemos visto hasta ahora son casos de **aprendizaje supervisado**: para cada observación de las variables predictoras $x_i$ tenemos una respuesta asociada $y_i$. Lo que hacemos es ajustar un modelo que relacione la respuesta con los inputs.

Por el contrario, el **aprendizaje no supervisado** describe una situación en la que para cada observación $i=1,2,\dots,n$  tenemos un vector de medidas $x_i$, pero no tenemos ninguna respuesta asociada.

Los métodos de **aprendizaje no supervisado** buscan establecer relaciones entre las variables o entre observaciones. 

El ejemplo más típico de aprendizaje no supervisado es el análisis de "clusters". El objetivo del análisis de clusters es determinar, basándose  en $x_1,\dots,x_n$, si las observaciones caen en grupos relativamente distintos


```{r, out.height= 500, out.width= 1000,fig.retina = NULL,echo=FALSE}
knitr::include_graphics("img/clustering.png")
```

Algunos ejemplos de aprendizaje no supervisado 

- **Estudio de segmentación de clientes**: observamos múltiples características asociadas a nuestros clientes, tales como  edad, género, nivel de formación, ingresos o código postal.  Podríamos creer en un principio que los clientes pueden clasificarse en diferentes grupos, tales como los grandes consumidores o consumidores ocasionales. Si la información sobre los patrones de gasto de cada cliente estuviera disponible, entonces sería posible un análisis supervisado. Sin embargo, si esta información no está disponible, es decir, no sabemos si cada cliente potencial es un gran consumidor o no. En este contexto, podemos intentar agrupar a los clientes sobre la base de las variables medidas, con el fin de identificar distintos grupos de clientes potenciales. La identificación de estos grupos puede ser de interés porque puede ser que los grupos difieran con respecto a alguna propiedad de interés, como los hábitos de gasto.
    
- **Análisis de cesta de la compra:** Identificación de productos que se compran de forma conjunta. Este análisis se realiza para planificar la distribución de productos en un supermercado o para elaborar ofertas.

- **Comunidades de usuarios**: Mediante el estudio de las relaciones entre personas y mediante técnicas de análisis de grafos (o redes), se pueden detectar comunidades, que dicho de una manera simple es definir  conjuntos de elementos que tienen una conexión dentro del grupo mucho mayor que hacia el exterior. Un ejemplo lo vemos en el siguiente gráfico que analiza las relaciones entre personajes de la saga Juego de Tronos, construyendo un grafo en el que los nodos son los personajes y los enlaces entre dos nodos existen si tienen diálogos comunes.  El análisis de comunidades sobre este grafo  clasifica de forma no supervisada los personajes en grupos (o clusters). Si eres seguidor de la trama puedes comprobar que estos grupos se corresponden con las diferentes familias de la serie (Lannister, Stark, Targarien, etc) 


![](img/game_of_thrones.png)



# Determinando la precisión de los modelos

Uno de los objetivos clave de este curso es darte a conocer la existencia de  una amplia gama de métodos de aprendizaje estadístico. Pero, ¿por qué es necesario introducir tantos métodos de aprendizaje estadístico diferentes, en lugar de un método único que sea el mejor? 

Este método no existe: ningún método domina a todos los demás sobre todos los posibles conjuntos de datos. En un conjunto de datos determinado, un método específico puede funcionar mejor, pero otro método puede funcionar mejor en un conjunto de datos similar aunque diferente. 

Por lo tanto, es muy importante decidir para cada problema o proyecto de ciencia de datos qué  método produce los mejores resultados. Seleccionar el mejor modelo o familia de modelos es una de las tareas más  difíciles en un proyecto real de aprendizaje estadístico.

En esta sección, discutiremos algunos de los conceptos más relevantes que surgen en el proceso de seleccionar un procedimiento de aprendizaje estadístico  determinado para un conjunto de datos específico.  A medida que avancemos en el curso, explicaremos cómo los conceptos que presentamos aquí se aplican en la práctica.

## Métricas de precisión de los modelos

### Conjuntos de Entrenamiento y de Test

Con el fin de evaluar el rendimiento de un método de aprendizaje estadístico en un conjunto de datos dado, necesitamos alguna manera de medir lo bien que sus predicciones coinciden con los datos observados. Es decir, necesitamos cuantificar hasta qué punto el pronóstico para el valor de la variable respuesta  para una observación dada está próximo al valor verdadero  de la respuesta para esa observación. 

En el ajuste de regresión, la medida más utilizada es el error cuadrático medio (MSE - Mean Square Error), dado por

$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} \left(y_i − \hat{f}(x_i)\right)^2 $$ 

Supongamos que hemos ajustado un modelo  $\hat{f}(x)$ en unos ciertos datos, que llamaremos **conjunto de entrenamiento** $Tr = \{x_i , y_i \}_1^n$, medimos la precisión del ajuste en este conjunto mediante el $MSE$ 

$$\text{MSE}_{Tr} = Ave_{i \in Tr} \left[y_i − \hat{f}(x_i)\right]^2  = \frac{1}{n_{tr}} \sum_{i=1}^{n_{tr}} \left(y_i − \hat{f}(x_i)\right)^2$$ 

Pero en general, no nos importa lo bien que funcione el método  en los datos de entrenamiento. Estamos interesados en la precisión de las predicciones que obtenemos cuando aplicamos nuestro método a datos de prueba que no hemos visto nunca.

¿Por qué es esto lo que nos importa? Pongamos un ejemplo: supongamos que estamos interesados en desarrollar un algoritmo para predecir el precio de una acción basado en las evoluciones de las cotizaciones en bolsa en periodos anteriores. Podemos entrenar el método utilizando los rendimientos de las acciones de los últimos 6 meses. Pero no nos importa lo bien que nuestro método prediga el precio de las acciones de la semana pasada. Lo que nos preocupa es lo bien que va a predecir el precio de mañana o el precio del próximo mes.

En muchas ocasiones un valor muy pequeño del $MSE$ en el conjunto de entrenamiento puede revelar que el ajuste puede estar sesgado hacia modelos sobre-entrenados.

Por tanto, debemos, siempre que sea posible, calcular el error en conjunto nuevo de datos "frescos" que llamamos **conjunto de test** $Te = \{x_i , y_i \}_1^{n_{test}}$

$$\text{MSE}_{Te} = Ave_{i \in Te} \left[y_i − \hat{f}(x_i)\right]^2 $$ 


Veamos un ejemplo. Construimos un conjunto de puntos de entrenamiento $(x,y)$, que siguen un modelo real

$$ Y = 2 +sin(X-2.5) + \epsilon$$

Ajustemos diferentes modelos con diferente grado de flexibilidad al conjunto de datos de entrenamiento.

```{r,fig.height=4.5,fig.width=7,echo=FALSE}
set.seed(231)
x=runif(30,0,5)
y= 2 +sin(x-2.5) + rnorm(30,0,0.2)
plot(x,y,ylim=c(0,3.5))
curve(2 + sin(x-2.5),0,5,add=TRUE)
mod1 = lm(y~x)
mod2 = lm(y~ns(x,3))
mod3 = lm(y~ns(x,15))
xnew = seq(0,5,.05)

y1 = predict(mod1,newdata = data.frame(x=xnew))
y2 = predict(mod2,newdata = data.frame(x=xnew))
y3 = predict(mod3,newdata = data.frame(x=xnew))

lines(xnew,y1,col=2)
lines(xnew,y2,col=3)
lines(xnew,y3,col=4)

legend("topleft", c("Real","Linear","Medium Flex","High Flex"), col = 1:4,
       text.col = "grey20", lty = 1, pch = NA,cex=0.8)

```

La curva negra es la $f$ "verdadera" (con la que hemos generado los datos), mientras que las curvas roja, verde y azul son modelos con diferente grado de flexibilidad

Los $MSE_{Tr}$ de entrenamiento para los diferentes modelos son decrecientes con la flexibilidad del modelo. 

```{r, echo=FALSE}
mse.tr=data.frame(tipo="train",flex=c(1,3,15),mse=c(mean((y-mod1$fitted.values)^2),mean((y-mod2$fitted.values)^2), mean((y-mod3$fitted.values)^2)))
knitr::kable(mse.tr,digits=3)

```

Ahora evaluamos los modelos anteriores sobre un conjunto nuevo de datos, generados con el mismo mecanismo

```{r,echo=FALSE}
set.seed(1414)

x=runif(30,0,5)
y= 2 +sin(x-2.5) + rnorm(30,0,0.2)
plot(x,y,ylim=c(0,3.5),pch=19)
curve(2 + sin(x-2.5),0,5,add=TRUE)

y1 = predict(mod1,newdata = data.frame(x=x))
y2 = predict(mod2,newdata = data.frame(x=x))
y3 = predict(mod3,newdata = data.frame(x=x))

points(x,y1,col=2,cex=0.7)
points(x,y2,col=3,cex=0.7)
points(x,y3,col=4,cex=0.7)

legend("topleft", c("Real model","Test points", "Linear prediction","Medium Flex pred","High Flex pred"), col = c(1,1:4), text.col = "grey20", lty = c(1,NA,NA,NA,NA), pch = c(NA,19,rep(1,3)),cex=0.8)

```

En la gráfica, los puntos negros son los datos de test, mientras que los puntos rojos, verdes y azules son las predicciones realizadas en el conjunto de test por los modelos que hemos ajustado sobre los puntos de entrenamiento.

Observamos que  los errores en un conjunto de test son menores para el modelo de flexibilidad intermedia.

```{r,echo=FALSE}
mse.te=data.frame(tipo="test",flex=c(1,3,15),mse=c(mean((y-y1)^2),mean((y-y2)^2), mean((y-y3)^2)))
knitr::kable(mse.te,digits=3)
```

Veamos en una única gráfica la evolución con la flexibilidad del modelo del error en el conjunto de entrenamiento $MSE_{Tr}$  y en el conjunto de test $MSE_{Te}$

```{r,echo=FALSE}
ggplot(rbind(mse.tr,mse.te)) + geom_point(aes(flex,mse,color=tipo)) +
  geom_line(aes(flex,mse,color=tipo),lty=2,size=0.3)
  
```


Si hacemos un  un barrido más amplio en la flexibilidad de los modelos obtenemos

```{r,echo=FALSE,message=FALSE,fig.height=4.5}
set.seed(92871)
N=400
x=runif(N,0,5)
y= 2 +sin(x-2.5) + rnorm(N,0,0.2)
df=data.frame(x,y)
train=1:200
test=-train
mse.tr=data.frame()
mse.te=data.frame()
for(ns in 1:15){
  mod = lm(y~ns(x,df=ns),data = df[train,])
  yp=predict(mod,newdata = df[test,])
  mse.tr=rbind(mse.tr,data.frame(tipo="train",flex=ns, mse= mean((y[train] - mod$fitted.values)^2)))
  mse.te=rbind(mse.te,data.frame(tipo="test",flex=ns, mse= mean((y[test] - yp)^2)))
}

ggplot(rbind(mse.tr,mse.te)) + geom_point(aes(flex,mse,color=tipo),size=1) +
  geom_line(aes(flex,mse,color=tipo),lty=2,size=0.5) 

```

Se observa que el error de entrenamiento decrece siempre con la flexibilidad, mientras que el error de test (el que importa) tiene un mínimo en flexibilidad=5. 

La flexibilidad de un modelo es concepto algo difuso y que varía de un tipo de modelo a otro. En el ejemplo estamos ajustando modelos tipo "Splines", la flexibilidad representa el número de grados de libertad. Un caso equivalente sería ajustar un modelo polinómico y la flexibilidad sería el orden del polinomio. 

¿Existe una receta general para decidir cuál es el punto de flexibilidad óptimo de un modelo?

En función de la naturaleza de los datos (la forma de la $f$ verdadera y la variabilidad irreducible), la flexibilidad óptima para la que se alcanza el mínimo de la curva del $MSE_{Te}$ de test es diferente.

En el caso de que la función real de regresión $f$ sea suave y el error irreducible sea grande, la flexibilidad óptima será pequeña. En este caso es posible que un modelo lineal sea la mejor elección.


```{r, out.height= 500, out.width= 1000, fig.retina = NULL,echo=FALSE}
knitr::include_graphics("img/mse_smooth.png")
```



En el caso opuesto, en el que la $f$ verdadera es rugosa y el error irreducible pequeño, es recomendable un modelo con flexibilidad alta.

```{r, out.height= 600, out.width= 1200, fig.retina = NULL,echo=FALSE}
knitr::include_graphics("img/mse_wiggle.png")
```



## Compromiso entre Sesgo y Varianza

La forma de "U" de las curvas del error de test $MSE_{Te}$ se debe a la competencia de dos características de los modelos de aprendizaje estadístico: la **varianza** y el **sesgo (bias)**.

Supongamos que ajustamos un modelo $\hat{f(x)}$ sobre un cierto conjunto de entrenamiento Tr y sea $(x_0, y_0)$ una observación de *test* obtenida de la población

Si el modelo verdadero es $Y = f(X) + \epsilon$ con  $f (x) = E(Y |X = x)$, entonces el error cuadrático medio esperado en los puntos de test puede expresarse como

$$E(y_0 − \hat{f}(x_0)^2) = \text{Var}(\hat{f}(x_0)) + [\text{Bias}(\hat{f}(x_0))]^2 + Var(\epsilon)$$.

donde se promedia sobre las posibles observaciones de test $(x_0, y_0)$ y los posibles conjuntos de entrenamiento Tr. 

Aparte del error irreducible, en el resultado influyen dos términos:

- La **varianza** ($\text{Var}(\hat{f}(x_0))$) se refiere a la variabilidad de la función estimada $\hat{f}$ sobre los distintos conjuntos de entrenamiento.

- El **sesgo (bias)** se refiere al error debido a aproximar un problema complicado por un modelo más simple.

Normalmente, conforme aumentamos la flexibilidad/complejidad de los modelos disminuimos el sesgo, pero aumentamos la varianza. Por tanto minimizar el error en un conjunto de test, equivale a adoptar el compromiso óptimo entre sesgo y varianza para el modelo elegido.

En la siguiente gráfica mostramos para tres conjuntos de datos distintos, la dependencia del sesgo, la varianza y error de test con la flexibilidad de los modelos ajustados. En cada caso, la curva azul sólida representa el sesgo cuadrado, para diferentes niveles de flexibilidad, mientras que la curva naranja corresponde a la varianza. La línea horizontal discontinua representa $\epsilon$, el error irreducible. Finalmente, la curva roja, es el  $MSE$ esperado en el  conjunto de test, que en el caso de un conjunto de test ideal es la suma de estas tres magnitudes.

En los tres casos, la varianza aumenta y el sesgo disminuye a medida que aumenta la flexibilidad del método. Sin embargo, el nivel de flexibilidad correspondiente al  $MSE$ óptimo difiere considerablemente entre los tres conjuntos de datos, porque el sesgo al cuadrado y la varianza cambian a diferentes velocidades en cada uno de los conjuntos de datos. 

- En el panel izquierdo de la figura, el sesgo inicialmente disminuye rápidamente, dando como resultado una disminución inicial abrupta en el $MSE$ de test esperado. Los datos son claramente no lineales con un error irreducible grande. 

- En el panel central,  la $f$ verdadera es casi lineal, por lo que sólo hay una pequeña disminución en el sesgo al aumentar la flexibilidad. El MSE de test sólo disminuye ligeramente antes de aumentar rápidamente a medida que aumenta la varianza. 

- En el panel de la derecha, a medida que aumenta la flexibilidad, hay una disminución abrupta en el sesgo porque la $f$ verdadera es muy no lineal. Pero también hay muy poco aumento en la varianza a medida que aumenta la flexibilidad ya que el error irreducible es pequeño. En consecuencia, el MSE de test disminuye sustancialmente antes de experimentar un pequeño aumento a medida que aumenta la flexibilidad del modelo.

![](img/bias_variance.png)

La relación entre el sesgo, la varianza y el $MSE$ en el conjunto de test se denomina compromiso entre sesgo y varianza. Para obtener buenos resultados de un método de aprendizaje estadístico se requiere baja varianza así como un sesgo bajo. 
Sin embargo esto es muy difícil de obtener, en cambio debemos llegar a un compromiso  porque es fácil obtener un método con un sesgo extremadamente bajo pero una alta varianza (por ejemplo, dibujando una curva que pasa por cada observación de entrenamiento) o un método con una varianza muy baja pero un sesgo alto (ajustando  los datos a una línea horizontal). El reto consiste en encontrar un método para el cual tanto la varianza como el sesgo al cuadrado sean bajos.



## Ejemplo curva de potencia

La curva de potencia de un aerogenerador, es la relación entre la energía que produce y la velocidad del viento. Aunque de forma teórica debería ser una relación bien establecida, existen otros factores que afectan a la producción de energía del aerogenerador tales como la temperatura, la presión atmosférica o la dirección del viento. De ahí la dispersión que se observa y que constituye el error irreducible de nuestro problema. 


```{r,echo=FALSE}
wind_data = read.csv("../../Datasets/windprod.csv",sep=";",dec=".")
wind_data$datetime = as.POSIXct(wind_data$datetime)
wind_data = subset(wind_data,!is.na(wind_speed))
wind_data = subset(wind_data,!(wind_speed >5 & prod<100) & wind_speed>2.5)

#Población
#ggplot(wind_data) + geom_point(aes(wind_speed,prod),alpha=0.1,size=0.3)
train=which(format(wind_data$datetime,"%Y-%m") %in% c("2007-10","2007-09","2007-08"))
test=which(format(wind_data$datetime,"%Y-%m")=="2007-11")
ggplot(wind_data[train,]) + geom_point(aes(wind_speed,prod),alpha=0.3,size=1)
```




El objetivo es hacer un modelo estadístico que estime la producción de energía de un aerogenerador en función de la velocidad del viento.

### Splines

Ajustemos varios modelos tipo splines con diferente flexibilidad. No te preocupes por como funcionan estos modelos, solo importa que tengas en cuenta que tienen un parámetro que son los grados de libertad que controla la flexibilidad de los modelos ajustados.

Elegimos los datos de los meses de agosto a octubre como conjunto de entrenamiento y los datos de noviembre como conjunto de test.

Las curvas ajustadas para los datos de entrenamiento serían

```{r,echo=FALSE}

train=which(format(wind_data$datetime,"%Y-%m") %in% c("2007-10","2007-09","2007-08","2007-07"))
test=which(format(wind_data$datetime,"%Y-%m")=="2007-11")
mse.tr=data.frame()
mse.te=data.frame()
y=wind_data$prod
models=list()
pred.df.tr=data.frame()
pred.df=data.frame()
i=1
for(ns in c(1,2,3,4,5,10,20,30,40,50)){
  mod = lm(prod ~ ns(wind_speed,df=ns),data = wind_data[train,])
  models[[i]]=mod
  i=i+1
  yp=predict(mod,newdata = wind_data[train,])
  pred.df.tr <- rbind(pred.df,cbind(wind_data[train,],flex=ns, pred=yp)) 
  yp=predict(mod,newdata = wind_data[test,])
  pred.df <- rbind(pred.df,cbind(wind_data[test,],flex=ns, pred=yp)) 
  mse.tr=rbind(mse.tr,data.frame(tipo="train",flex=ns, mse= mean((y[train] -   mod$fitted.values)^2)))
  mse.te=rbind(mse.te,data.frame(tipo="test",flex=ns, mse= mean((y[test] - yp)^2)))
}

pred.df.tr <- pred.df.tr %>% arrange(flex,wind_speed) %>% filter(flex %in% c(1,3,5,20,50))

ggplot(wind_data[test,]) + geom_point(aes(wind_speed,prod),alpha=0.3,size=1) + 
  geom_line(aes(wind_speed,pred,color=factor(flex)),
                data=pred.df) + scale_color_brewer("Flexibidad",palette = "Set1")

```



Si miramos los errores de entrenamiento y test, obtenemos:

```{r,echo=FALSE}
mse=rbind(mse.tr,mse.te) %>% mutate(mser=sqrt(mse)/mean(wind_data$prod[test]))
ggplot(mse) + geom_point(aes(flex,mser,color=tipo),size=1) +
  geom_line(aes(flex,mser,color=tipo),lty=2,size=0.5) 
```

Donde la curva del $MSE$ de test muestra la forma de "U" característica, con un mínimo en flexibilidad 4.

###  k-vecinos

Aplicamos el método de k-vecinos para realizar predicciones de producción de energía eólica. El método de k-vecinos consiste en lo siguiente: para cada punto del conjunto de test, buscamos los k puntos del conjunto de entrenamiento con velocidad del viento más próxima y predecimos la producción de energía de la observación de test como el promedio de las producciones de los puntos de entrenamiento seleccionados.

El parámetro k (número de vecinos elegidos) es el que controla la flexibilidad del modelo. Cuanto más pequeño sea, mas flexibles (rugosos) serán los ajustes mientras que cuanto más grande sea más puntos se usan para calcular los promedios y el ajuste será más suave (menos flexible). Definimos la flexibilidad como 

$$flexibilidad=\frac{1}{k}$$

```{r,echo=FALSE}
# Unsamos el paquete FNN para el ajuste de regresión por k-vecinos
library(FNN)
mse.tr=data.frame()
mse.te=data.frame()
y=wind_data$prod
models=list()
pred.df <- data.frame()
i=1
for(k in c(5,10,20,30,40,50,75,100,200)){
  mod = knn.reg(train=matrix(wind_data$wind_speed[train],ncol=1), 
        test=matrix(wind_data$wind_speed[test],ncol=1), y=y[train], k = k)
  models[[i]]=mod
  i=i+1
  yp=mod$pred
  pred.df <- rbind(pred.df,cbind(wind_data[test,],k=k, pred=yp)) 
  mse.tr=rbind(mse.tr,data.frame(tipo="train",k=k,flex=1/k, mse= mean((y[train] - mod$fitted.values)^2)))
  mse.te=rbind(mse.te,data.frame(tipo="test",k=k,flex=1/k, mse= mean((y[test] - yp)^2)))
}

# Solo represento algunas k's para hacer visible las lineas

pred.df <- pred.df %>% arrange(k,wind_speed) %>% filter(k %in% c(10,30,40,100,200))

ggplot(wind_data[test,]) + geom_point(aes(wind_speed,prod),alpha=0.3,size=1) + 
  geom_line(aes(wind_speed,pred,color=factor(k)),
                data=pred.df) + scale_color_brewer("k",palette = "Set1")


```



Y si representamos el error de  test frente a la flexibilidad se obtiene un óptimo en k alrededor de 40

```{r,echo=FALSE}
mse=rbind(mse.tr,mse.te) %>% mutate(mser=sqrt(mse)/mean(wind_data$prod[test]))
ggplot(mse) + geom_point(aes(flex,mser,color=tipo),size=1) +
  geom_line(aes(flex,mser,color=tipo),lty=2,size=0.5) +
  geom_line(aes(flex,mser,color=tipo),lty=2,size=0.5) +
  geom_text(aes(flex,mser+0.0015,label=k),size=3,angle=90)
```


# Clasificación

## Problemas de clasificación

Hasta ahora nos hemos centrado en problemas de regresión, donde la variable respuesta tenía un valor numérico. 

En un problema de clasificación, en cambio,  la variable de respuesta $Y$ es cualitativa. Por ejemplo:

- $\mathcal{C}=\{spam, ham\}$ (ham es correo bueno) en un sistema de detección de correo basura.
- En el problema de clasificación de dígitos escritos, las clases del dígito serían  $\mathcal{C}=\{0, 1, \dots , 9\}$. 


El objetivo es similar al de los problemas de regresión: 

- Construir un modelo clasificador que asigne una etiqueta de clase de $\mathcal{C}$ para futuras observaciones $x$ sin etiquetar.
- Asignar una incertidumbre/probabilidad a cada clasificación
- Entender los papeles de cada una de los predictores $X =(X_1,X_2,\dots,X_p)$ en el clasificador.


Veamos un ejemplo de problema de clasificación a partir de unos datos simulados. Consideremos que queremos predecir la probabilidad de avería en una máquina industrial en función de los años de funcionamiento. 

En la gráfica se muestran los resultados de inspecciones que se han hecho en varios años y en diferentes equipos, representando en azul los componentes averiados (que necesitan mantenimiento) y en naranja los sanos, en función de su tiempo de vida en el momento de la inspección. Se observa que en los primeros años la proporción de equipos averiados es muy pequeña y esta va aumentando conforme pasa  el tiempo de vida de las máquinas.


```{r,echo=FALSE,fig.height=4.5}
  prob <-function(x,a=1,x0=3){1/(1+exp(-a*(x-x0)))}
  curve(prob(x),0,5,xlab="Años",ylab="Probabilidad")
  plot(1,1, type="n",xlim=c(0,5),ylim=c(0,1),xlab="Años",ylab="Inspección Sano/Averiado")
  set.seed(123)
  # Generemos datos de eventos (averiado o no para distintos tiempos)
  x=seq(0,5,.01)
  cl=rep(NA,length(x))
  for(i in 1:length(x)){
    p=prob(x[i])
    cl[i]=ifelse(runif(1)<p,1,0)
  }
  segments(x0=x[cl==0],y0=0,x1=x[cl==0],y1=0.05,col="orange")
  segments(x0=x[cl==1],y0=0.95,x1=x[cl==1],y1=1,col="lightblue")
  
```

## Clasificador ideal

¿Existe un clasificador $C(X)$ ideal?

Al igual que en el caso de la regresión, el clasificador ideal depende de poder calcular las probabilidades condicionadas. 

Si existen k posibles clases en $\mathcal{C}=\{C_1,C_2, ... ,C_k\}$

$$ p_k(x) = Pr(Y = k \,|\, X = x) \, , \;k = 1, 2, . . . , K$$


representa la probabilidad condicionada de que el evento pertenezca a la clase $k$ dado un valor $x$ de los predictores. En ese caso, el clasificador óptimo (que se denomina Clasificador de Bayes), clasificará, dado un valor de los predictores $x$, en aquella clase para la que la probabilidad condicionada sea máxima

$$ C(x) = j \;\; \text{si} \;\; p_j(x) = \text{max}\{p_{1}(x), p_2(x), \dots , p_K(x)\}$$


La dificultad en este caso es calcular estas probabilidades condicionadas.  

En el ejemplo, como los datos han sido generados de forma simulada conocemos la probabilidad condicionada al tiempo de vida de que haya avería.

```{r,echo=FALSE,fig.height=4}
  curve(prob(x),0,5,ylim=c(0,1),xlab="Años",ylab="Probabilidad")
  set.seed(123)
  # Generemos datos de eventos (averiado o no para distintos tiempos)
  x=seq(0,5,.01)
  cl=rep(NA,length(x))
  for(i in 1:length(x)){
    p=prob(x[i])
    cl[i]=ifelse(runif(1)<p,1,0)
  }
  segments(x0=x[cl==0],y0=0,x1=x[cl==0],y1=0.05,col="orange")
  segments(x0=x[cl==1],y0=0.95,x1=x[cl==1],y1=1,col="lightblue")
  segments(x0=0,x1=3,y0=0.5,y1=0.5,col="orange")
  segments(x0=3,x1=5,y0=0.5,y1=0.5,col="lightblue")
  
```

En este caso el clasificador de Bayes, clasificará **sin riesgo** de avería ($k=0$) a aquellos componentes con menos de 3 años y **en riesgo** de avería ($k=1$) aquellos con más de tres años. 3 es el valor de x para el que $p_{\text{averia}}(x)=0.5$

## K-vecinos

En la práctica, no conocemos las probabilidades condicionadas y no tenemos puntos suficientes para poderlas calcular de forma exacta. Podemos adoptar una estrategia de k vecinos como en el caso de regresión. 

El método de k-vecinos funciona de forma análoga al caso de regresión. Para un punto de test dado, busca los k-vecinos más próximos en el conjunto de entrenamiento y calcula la probabilidad condicionada de pertenecer a la clase $m$ como el número de vecinos que pertenecen a la clase $m$ dividido por $k$ (el número total de vecinos).

En la gráfica siguiente, la curva negra es aquella con la que se han generado los eventos y se corresponde con la probabilidad condicionada real. La curva roja es una estimación usando el método de k-vecinos.

```{r,echo=FALSE,fig.height=4}
  N=100
  plot(1,1, type="n",xlim=c(0,5),ylim=c(0,1),xlab="Años",ylab="Probabilidad")
  curve(prob(x),0,5,ylim=c(0,1),xlab="Años",ylab="Probabilidad")

  # Generemos datos de eventos (averiado o no para distintos tiempos)
  x=runif(N,0,5)
  cl=rep(NA,length(x))
  for(i in 1:length(x)){
    p=prob(x[i])
    cl[i]=ifelse(runif(1)<p,1,0)
  }
  segments(x0=x[cl==0],y0=0,x1=x[cl==0],y1=0.05,col="orange")
  segments(x0=x[cl==1],y0=0.95,x1=x[cl==1],y1=1,col="lightblue")
  # En realidad aquí no hacemos un k vecinos sino que seleccionamos puntos en un entorno
  # x +- 1
  # clasifiquemos promediando puntos en un entorno +-1
  xx=seq(0,5,0.25)
  e=0.5
  p1=rep(NA,length(xx))
  for (i in 1:length(xx)){
    pos=which(x>xx[i]-e  & x<xx[i]+e)
    clase = cl[pos]
    p1[i]=length(which(clase==1))/length(clase)
  }
  points(xx,p1,type="l",col=2)
  
   abline(v=2,col="grey30")
   abline(v=1,lty=2,col="grey30")
   abline(v=3,lty=2,col="grey30")
   abline(h=0.5,lty=2,col="red")
```


El modelo de k-vecinos para clasificación funciona bastante bien si el número de variables predictoras $p$  es pequeño, pero sufre de la "maldición de la dimensión" al igual que en la regresión.


## Medida de la precisión del clasificador

Típicamente mediremos el rendimiento del clasificador mediante el error de clasificación (tanto para el conjunto de entrenamiento como para el de test):

 $$Err_{Te} = Ave_{i\in Te} I[y_i \neq C(x)]$$
 
 $$Err_{Tr} = Ave_{i\in Tr} I[y_i \neq C(x)]$$

donde la función $I$ vale $1$ si $y_i \neq C(x)$, es decir si el clasificador falla en la previsión de la clase y vale $0$ si $y_i = C(x)$, es decir si el clasificador acierta la clase. La función de error mide la proporción de observaciones que no han sido clasificadas en la categoría correcta. 

El clasificador de Bayes es que el que menor error da para la población completa


## Modelos de clasificación 

- Existen clasificadores estructurados (paramétricos)  para la clase $C(x)$ como los *support vector machines*

- Otros, como la *regresión logística* o los *modelos aditivos generalizados* son modelos estructurados que calculan la probabilidad condicionada $p_k$

- Existen otros modelos no paramétricos, algunos basados en el ajuste local como el *k-vecinos* 

- Un método de clasificación no paramétrico muy utilizado son los *árboles de clasificación*. Son modelos muy interpretables aunque en general no demasiado precisos. Basados en los árboles de decisión están dos de los clasificadores más potentes que hay en la actualidad: *Random Forests* y *Boosting Trees*

## Ejemplo de clasificación en 2 dimensiones 

Simulemos  unos datos en dos dimensiones pertenecientes a dos posibles categorías azul o naranja.
Los datos se generan de acuerdo a una densidad de probabilidad dada que vale 0.5 en la región punteada.  Esa será la región de decisión del clasificador ideal (clasificador de Bayes)

```{r, out.height= 500, out.width=600, fig.retina = NULL,echo=FALSE}
knitr::include_graphics("img/class_knn_truemodel.png")
```

El método de k-vecinos con $k=10$ funcionaría de la siguiente manera. Busca los 10 puntos más próximos, en el conjunto de entrenamiento,  a la observación que se quiere predecir. Se miran las clases a las que pertenecen esos 10 puntos y se le asigna la clase mayoritaria de los 10 puntos. 

Para $k=10$, el clasificador knn es muy próximo al ideal

```{r, out.height= 500, out.width=600, fig.retina = NULL,echo=FALSE}
knitr::include_graphics("img/knn_10.png")
```

Conforme más alto es k menos flexible es el modelo y por tanto es más suave. Veamos dos casos extremos para $k=1$ y $k=100$.

```{r, out.height= 500, out.width=1000, fig.retina = NULL,echo=FALSE}
knitr::include_graphics("img/knn_1_100.png")
```



## Error de entrenamiento y test

Si disponemos de unos datos de test generados con el mismo mecanismo podemos ver como evolucionan los errores en función del número de vecinos del clasificador.

Se observa la misma forma de "U" para la curva del $MSE$ en el conjunto de test que veíamos en los modelos de regresión. Por tanto, los mismos argumentos sobre sesgo y varianza discutidos para los problemas de regresión se aplican al caso de clasificación. 

```{r, out.height= 550, out.width=800, fig.retina = NULL,echo=FALSE}
knitr::include_graphics("img/train_test_class_rate.png")
```



## Ejemplo - Body

Vamos a hacer un clasificador del género de las personas en función de su altura y su peso.
Dividiremos los datos por la mitad en entrenamiento y test

```{r,echo=TRUE}

body <- read.table("http://ww2.amstat.org/publications/jse/datasets/body.dat.txt")
BodyMeasurements <- c("Biacromial_diameter","Biiliac_diameter","Bitrochanteric_diameter","Chest_depth","Chest_diameter","Elbow_diameter","Wrist_diameter","Knee_diameter","Ankle_diameter","Shoulder_girth","Chest_girth","Waist_girth","Navel_girth","Hip_girth","Thigh_girth","Bicep_girth","Forearm_girth","Knee_girth","Calf_max_girth","Ankle_min_girth","Wrist_min_girth","Age","Weight","Height","Gender")    
names(body) <- BodyMeasurements
body$Gender = factor(body$Gender)

```

Calculemos la región de frontera, para $k=10$ utilizando el conjunto de entrenamiento.


```{r,echo=FALSE}
vars=c("Height","Weight")
k=10
n=nrow(body)
set.seed(511)
train = sample(1:n,round(n/2))
test=-train
x=body[,vars]
g=body$Gender[train]
# Clasificador k vecinos
#Train
mod.train <- class::knn(x[train,],x[train,],k=k, cl=g)
#Test
mod.test  <- class::knn(x[train,],x[test,],k=k, cl= g)
# Para pintar el contour
x1=seq(floor(min(body[,vars[1]])),ceiling(max(body[,vars[1]])),.1)
x2=seq(floor(min(body[,vars[2]])),ceiling(max(body[,vars[2]])),.1)

xc <- expand.grid(Ankle_min_girth=x1,Wrist_min_girth=x2)
mod.cont  <- class::knn(x[train,],xc,k=k, cl= g,prob = TRUE)

prob <- attr(mod.cont, "prob")
prob <- ifelse( mod.cont=="1", prob, 1-prob) # prob is voting fraction for winning class!
probm <- matrix(prob, length(x1), length(x2))

contour(x1,x2, probm, levels=0.5, labels="",xlab=vars[1],ylab=vars[2],main=paste("knn",k," Train data"))
points(x[train,],cex=0.7,col=g)
#points(x[train,],col=mod.train,cex=0.5,pch=19)
```

Podemos dibujar la región de frontera sobre el conjunto de datos de test,  y calcular los errores de clasificación

```{r,echo=FALSE}
contour(x1,x2, probm, levels=0.5, labels="",xlab=vars[1],ylab=vars[2],main=paste("knn",k," Test data"))
points(x[test,],cex=0.7,col=body$Gender[test])
```

Podemos repetir el proceso para diferentes valores del parámetro $k$, el número de vecinos.
Veamos como evoluciona el error de clasificación en los conjuntos de clasificación y test frente al número de vecinos. Recuerda que la flexibilidad del modelo decrece con $k$.

```{r,echo=FALSE}
#ks <- c(1,5,10,13,15,17,20,25,35,45,55,83,101,151)
ks <- c(1,2,3, seq(5,150,5))
err.train=rep(NA,length(ks))
err.test=rep(NA,length(ks))
cltest=body$Gender[test]
for(i in 1:length(ks)){
  mod.train <- class::knn(x[train,],x[train,],k=ks[i], cl=g)
  #Test
  mod.test  <- class::knn(x[train,],x[test,],k=ks[i], cl= g)
  err.train[i] <- 1 - sum(mod.train==factor(g))/length(g)
  err.test[i] <- 1 - sum(mod.test==cltest)/length(cltest)
}
err=rbind(data.frame(tipo="train",k=ks,err=err.train),data.frame(tipo="test",k=ks,err=err.test))
ggplot(err) + geom_point(aes(k,err,color=tipo),size=1) +
  geom_line(aes(k,err,color=tipo),lty=2,size=0.5) 
```

Se observa que el error de entrenamiento aumenta, en general,  con $k$ mientras que el error de test tiene un mínimo alrededor de $k=55$.

# Qué has aprendido

Además de tener un primer encuentro con los problemas de aprendizaje estadístico, conocer los diferentes tipos de problemas que pueden plantearse y algunas de las estrategias para resolverlos, el principal hecho que has aprendido es que existen numerosos tipos de modelos de aprendizaje estadístico pero ninguno de ellos es el mejor en todos los casos.

El modelo adecuado depende de las características de los datos y de adoptar el compromiso apropiado entre sesgo y varianza. Dominar esto es a veces un arte más que un ciencia que se aprende principalmente con la experiencia. A lo largo de este curso insistiremos en el asunto y aprenderás algunos procedimientos muy eficaces para este fin.  

# Autoevaluación

1. Cuando hacemos un modelo de aprendizaje estadístico, asumimos que existe una relación entre la respuesta cuantitativa $Y$ y los diferentes predictores $X  = \left(X_1,X_2,\dots,X_p \right)$. Escribiremos el modelo como $Y = f (X) + \epsilon$,  donde $f$ es una función definida, aunque desconocida, que captura toda la posible dependencia sistemática entre la respuesta y las variables predictoras. $\epsilon$ es un término de error aleatorio que 

    a. captura las  fuentes discrepancia entre modelo y realidad,
    b. incorpora las dependencias con otras variables que no estamos considerando
    c. tiene en cuenta las variaciones en la respuesta debidas al puro azar
    d. Todas las respuestas anteriores son válidas
    
    
2. Hay dos razones para estimar f: predicción e inferencia. Cuando estamos interesados en la inferencia

    a. queremos entender detalladamente las relaciones entre la respuesta $Y$ y las variables predictoras
    b. no nos interesan los detalles de las relaciones entre variables, queremos tener buenas predicciones. 
    c. estamos hablando de aprendizaje no supervisado
    d. nos interesan modelos no parámetricos con mucha flexibilidad  
    
3. Existe una función de regresión ideal $f(x)$ que se puede expresar como el valor esperado de $Y$ cuando $X=x$, es decir $f(x) = E(Y|X=x)$. ¿Por qué habitualmente no es posible estimar $f$ de esta manera?

    a. Es muy complicado calcular el valor esperado
    b. Debido a los valores erroneos o valores ausentes
    c. Porque típicamente tenemos muy pocos (o ningún) puntos con X=x exactamente
    d. Si se puede estimar $f$ de esa manera
 
     

4. Un ejemplo de modelo  no paramétricos es

    a. Regresión lineal
    b. k-vecinos
    c. Regresión polinómica
    d. Regresión logística
    
5. El análisis de clusters

    a. es un método de aprendizaje no supervisado
    b. es un método de aprendizaje supervisado
    c. es un método de aprendizaje paramétrico
    d. es un modelo de regresión
    
    
6. Para medir la precisión de un modelo de regresión, la métrica utilizada habitualmente es

    a. la media de los errores en cada punto, tambien llamados residuos
    b. el error máximo
    c. el error cuadrático mediano
    d. el error cuadrático medio
    
    
7. Un modelo de regresión con un error muy pequeño en el conjunto de entrenamiento

    a. seguro que genera buenas predicciones en un conjunto de test
    b. seguro que genera malas predicciones en un conjunto de test
    c. es posible que esté sobre-entrenado y genere malas predicciones en un conjunto de test
    d. seguro que es dificil de interpretar sus resultados



8. Para un conjunto de datos relacionados por una función de regresión real muy suave con un error irreducible pequeño el menor error de test lo encontraremos con un modelo 

    a. con flexibilidad baja
    b. con flexibilidad alta
    c. no paramétrico
    d. no lineal
    
    
9. El error esperado de un modelo sobre un conjunto de test es la suma de dos términos: el sesgo y la varianza. En este contexto, por sesgo entendemos 

    a. a la variabilidad de la función estimada sobre los distintos conjuntos de entrenamiento.
    b. al error debido a aproximar un problema complicado por un modelo más simple.
    c. a los errores debidos a variables no consideradas o al puro azar
    d. Ninguna de las respuestas anteriores es correcta
    
    
    
    
10. Si usamos un modelo de k-vecinos como clasificador. ¿Para que valores de k tendremos un modelo con mucha flexibilidad?

    a. k grande
    b. k pequeño
    c. k=0
    d. k no tiene que ver con la flexibilidad del modelo
    

# Soluciones

1-c; 2-a; 3-c; 4-b; 5-a;6-c;7-c;8-a; 9-b; 10-b

# Bibliografía

"An Introduction to Statistical Learning, with applications in R", G. James, D. Witten,  T. Hastie and R. Tibshirani (Springer, 2013) 

## Otros créditos 

Some of the figures in this presentation are taken from "An Introduction to Statistical Learning, with applications in R"  (Springer, 2013) with permission from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani 






---
title: "Regresión Lineal"
author: "MasterD"
output:
  html_document:
    highlight: tango
    theme: united
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
  pdf_document:
    toc: yes
  revealjs::revealjs_presentation:
    center: yes
    css: style.css
    highlight: pygments
    theme: sky
  word_document:
    reference_docx: www/plantillaMasterD_basica5.docx
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
options(width=100)

library(knitr)
library(reshape2)
library(dplyr)
library(ggplot2)
library(grid)
library(ElemStatLearn)
library(GGally)
library(png)
#library(igraph)
library(ISLR)
library(splines)
library(plot3D)
#library(FNN)
#library(MASS)
#library(class)

```

# Tu reto en esta unidad

Imagina que eres el director de marketing de una multinacional. Tienes datos de ventas en diferente países/regiones del mundo junto con el presupuesto empleado para publicidad en televisión, radio y prensa en cada uno de esos países/regiones. 

Quieres hacer un modelo que explique las ventas función del presupuesto gastado en publicidad en los diferentes medios. Estos son los datos de los que dispones. 


```{r, echo=FALSE, fig.height=4}

adv=read.csv("../Datasets/advertising .csv",sep=";")
a <- ggplot(adv) + geom_point(aes(TV,Sales))
b <- ggplot(adv) + geom_point(aes(Radio,Sales)) 
c <- ggplot(adv) + geom_point(aes(Newspaper,Sales)) 

grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 3)))
vplayout <- function(x, y)
  viewport(layout.pos.row = x, layout.pos.col = y)

print(a, vp = vplayout(1, 1))
print(b, vp = vplayout(1, 2))
print(c, vp = vplayout(1, 3))

```

Cada punto/observación representa a un determinado producto en un país/región del mundo. 

Algunas preguntas a las que  querrías que respondiera tu modelo son las siguientes:

- ¿Hay una relación entre el presupuesto en publicidad y las ventas del producto?
- ¿Cómo de fuerte es esa relación?
- ¿Qué medio contribuye más a las ventas?
- ¿Con qué precisión podemos predecir futuras ventas?
- ¿Es la relación entre ventas y publicidad lineal?
- ¿Hay sinergias entre los diferentes canales de publicidad?


A lo largo de la unidad, vamos a aprender un tipo de modelos bastante sencillo pero que serán capaces de responder satisfactoriamente a tus necesidades y te permitirán invertir el presupuesto de publicidad de forma más eficiente en los próximos países/regiones en los que se produzcan lanzamientos de producto. 






# Introducción

Esta unidad trata sobre la regresión lineal, uno de los métodos más simples de aprendizaje supervisado que usaremos predecir una respuesta de tipo cuantitativo.

A pesar de que los modelos lineales son sencillos en comparación con otros métodos de aprendizaje estadístico más modernos, la regresión lineal sigue siendo un método de aprendizaje estadístico muy útil y ampliamente utilizado. Además, sirve como punto de partida para entender los conceptos y buenas prácticas del aprendizaje estadístico. 
Más aún, muchos de los métodos más sofisticados que se usan en proyectos de ciencia de datos son generalizaciones o extensiones de los modelos lineales.

Por tanto, la regresión lineal es un punto de partida imprescindible para comenzar en el estudio de los métodos de aprendizaje estadístico.


La regresión lineal  asume que la relación entre la respuesta $Y$ y los predictores $(X_1,X_2,\dots,X_p)$ es lineal, es decir que la relación entre la variable respuesta y la variable predictora se puede representar mediante una línea recta, en dos dimensiones, un plano en 3 dimensiones o un hiperplano para 4 o más dimensiones. 

Aunque la función de regresión verdadera pocas veces es completamente lineal, en la práctica la aproximación lineal puede ser  extremadamente útil y práctica. 

Para ilustrar los conceptos fundamentales de los modelos lineales vamos a usar el conjunto de datos de *Advertising* que hemos introducido anteriormente. 


# Regresión lineal simple

## Modelo lineal

Asumimos que los datos están distribuidos según un modelo 

$$ Y = \beta_0 + \beta_1 X + \epsilon$$

donde $\beta_0 + \beta_1$ son dos constantes desconocidas, que representan la pendiente (slope)  y la ordenada en el origen (intercept) de la recta. También les llamaremos **coeficientes** o **parámetros**.

$\epsilon$ es el error irreducible y en principio asumimos que está distribuido normalmente con media 0 y una cierta varianza $\sigma$.  

## Estimación de parámetros

Supongamos que tenemos conjunto de datos $\{(x_1,y_1),(x_2,y_2),..,(x_n,y_n)\}$ y pretendemos estimar cuales son los valores de los coeficientes  $\hat{\beta}_0$ y $\hat{\beta}_1$ que hacen que el modelo lineal se ajuste mejor a nuestros datos.

**Autotexto**
Date cuenta de la notación empleada. Llamamos $\beta_0$ y $beta_1$ a los paramétros del modelo lineal que describe a la población. Por el contrario llamamos $\hat{\beta_0}$ y $\hat{\beta_1}$ a los parámetros estimados a partir de una muestra. Para cada muestra distinta que tengamos de una población, los valores estimados de los parámetros son distintos. 
**Fin Autotexto**

El procedimiento para estimar los coeficientes se denomina *ajuste por mínimos cuadrados* y consiste esencialmente en minimizar la suma de las distancias al cuadrado entre los $y_i$ reales y los $\hat{y_i}= \beta_0 + \beta_1 x_i$ estimados.


Sea $\hat{y}_i = \hat{\beta_0}+ \hat{\beta}_1 x_i$, una predicción de $Y$ basada en el valor de $x_i$ para un valor dado de los parámetros $\hat{\beta_0}$ y $\hat{\beta_1}$ 

Al error entre el valor real y la predicción $e_i= y_i - \hat{y_i}$  se le  denomina   **residuo** asociado a la i-esima observación.

Definamos la suma de los cuadrados de los residuos ($RSS$) como 

$$RSS= e_1^2 + e_2^2 + \dots + e_n^2$$

o equivalentemente como 

$$RSS= \sum_{i=1}^{n}(y_i - \hat{\beta_0}+ \hat{\beta}_1 x_i)^2 $$

El método de  mínimos cuadrados escoge los valores de $\hat{\beta_0}$ y $\hat{\beta_1}$ que minimizan $RSS$. Por tanto debe cumplirse que 

$$ \frac{\partial RSS}{\partial \beta_0}=0$$

$$ \frac{\partial RSS}{\partial \beta_1}=0$$

Resolviendo las ecuaciones anteriores, los valores que minimizan $RSS$ son  

$$
\hat{\beta_1} = \frac{\sum_{i=1}^{n}(y_i-\bar{y})(x_i-\bar{x})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}=
\frac{Cov(x,y)}{\sigma^2_{x}}

$$


$$
\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}
$$



Veamos un ejemplo, ajustamos un modelo lineal de las ventas frente al presupuesto de anuncios en `TV`.
Calculamos la estimación de los coeficientes del modelo lineal, usando las formulas anteriores y las comparamos con los resultados de la función de R que ajusta modelos lineales `lm`

```{r,echo=TRUE}
##Estimación manual

beta1=cov(adv$TV,adv$Sales)/var(adv$TV);beta1
beta0= mean(adv$Sales) - beta1*mean(adv$TV);beta0

##Estimación con lm

mod=lm(formula=Sales~TV,data=adv)
summ=summary(mod);summ
```

Veamos gráficamente, la recta ajustada y los residuos

```{r,echo=FALSE}
plot(adv$TV,adv$Sales,col=2,pch=19,cex=0.8,xlab="TV",ylab="Sales")
abline(beta0,beta1,lwd=2)
segments(adv$TV,adv$Sales,adv$TV,mod$fitted.values,col=4)
```

Conozcamos con más detalle la función `lm` de R, cuyo cometido es ajustar modelos lineales. 
La sintaxis básica de esta función es `lm(formula, data)` donde 

- `formula`: Es una descripción simbólica del modelo. Casi todas las funciones de R que ajustan modelos estadísticos usan la sintaxis de fórmulas para definir los modelos. En el ejemplo anterior hemos ejecutado
`lm(formula=Sales~TV,data=adv)`. La fórmula `Sales ~ TV` indica que queremos modelar las ventas `Sales` en función del presupuesto gastado en televisión `TV`.
Si quisiéramos definir un modelo con más variables predictoras escribiríamos `Sales ~ TV + Radio + Newspaper` o también `Sales ~ .` para indicar el resto de las variables disponibles en nuestros datos.
También es posible seleccionar todas menos algunas de ellas con el signo -, 
por ejemplo `y~.-x1` para usar todas las variables menos `x1`.

-`data` es un data frame que contenga las columnas referenciadas en la fórmula.  


Hay muchos más argumentos opcionales que se pueden pasar a la función `lm`, puedes consultarlos mediante la ayuda de R `?lm`.


## Precisión en la estimación de los parámetros

Normalmente, la  estimación de los parámetros la realizamos a partir de una muestra de datos de una cierta población. Y para cada muestra distinta tendremos una estimación distinta de los parámetros. 

Para estimar la dispersión en la estimación de los parámetros en todas las posibles muestras de valores $\{(x_i,y_i)\}$ de tamaño n, procedemos de la misma manera que hicimos en la unidad 5 en los cálculos de intervalos de confianza. 

Dada una muestra de datos $\{(x_i,y_i)\}$ podríamos realizar un proceso de obtención de muestras de bootstrap y estimar la distribución de los parámetros estimados. En este caso, vamos a fiarnos de los métodos tradicionales de estimación de estas dispersiones, y dejaremos que R las calcule por nosotros.

Para mostrar algunas propiedades de la incertidumbre en la determinación de los coeficientes, partiremos de un modelo lineal conocido (esto nunca pasará en la realidad). Generaremos diferentes muestras de valores $\{(x_i,y_i)\}$ de tamaño finito, estimamos los coeficientes para cada muestra y visualizamos las distribuciones. 

Supongamos una población de $Y$ y $X$ que responde a un modelo $Y=2 + 3X + \epsilon$ 
donde $\epsilon=\mathcal{N}(0,0.5)$ (Normal de media 0 y $\sigma=0.5$).

Simulemos un número elevado de muestras de tamaño 10 y veamos la distribución de los parámetros estimados 

Veamos primero las rectas ajustadas para 10 muestras de tamaño 10

```{r}
N=100
ng=10
a=2
b=3
s=0.5

res<- data.frame(samp=factor(1:10)) %>% group_by(samp) %>% do({
  x=runif(ng,0,1)
  y=a + b*x + rnorm(ng,0,s)
  data.frame(x,y)
})

ggplot(res) + geom_point(aes(x,y,color=samp)) + geom_smooth(aes(x,y,color=samp),method="lm",se=FALSE) +
  geom_abline(aes(slope=3,intercept=2))
```

Para cada muestra se obtiene una recta distinta caracterizada por un valor distinto de los parámetros estimados $\hat{\beta_0}$ y $\hat{\beta_1}$. 

Ahora simulemos 10000 muestras y estudiemos las distribuciones de los parámetros estimados. 



```{r,echo=TRUE,cache=TRUE,fig.height=4.8}
nsim=10000
N=10
a=2
b=3
s=0.5
res=data.frame()
for(i in 1:nsim){
  x=runif(N)
  y=a + b*x + rnorm(N,0,s)
  mod=lm(y~x)
  sum=summary(mod)
  ae=mod$coefficients[1]
  be=mod$coefficients[2]
  # asd=sum$coefficients[1,2]
  # bsd=sum$coefficients[2,2]
  res <- rbind(res,data.frame(a,b,ae,be))
}
asd=sd(res$ae)
bsd=sd(res$be)
par(mfrow=c(1,2))
hist(res$ae,probability = TRUE,main="Histograma beta0",xlab="beta0")
curve(dnorm(x,a,asd),1,5,col=2,add=TRUE)
hist(res$be,probability = TRUE,main="Histograma beta1",xlab="beta1")
curve(dnorm(x,b,bsd),1,5,col=2,add=TRUE)
par(mfrow=c(1,1))
```


Como se observa en la figura, la distribución de parámetros estimados es Normal. Como vamos a ver ahora la varianza de estas distribuciones depende de la $\sigma$ del $\epsilon$ (error irreducible), del tamaño de la muestra y de la dispersión de la variable  $x$

El error estándar para la estimación de los parámetros de regresión lineal se puede calcular de forma exacta y viene dado por las siguientes fórmulas:

$$SE(\hat{\beta_1})^2=\frac{\sigma^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2} \; , \;
SE(\hat{\beta_0})^2 = \sigma^2\left( \frac{1}{n} +  \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2} \right)$$

donde $\sigma^2=Var(\epsilon)$

Normalmente no conocemos  $\sigma$ pero la estimaremos a partir de la muestra mediante el error estándar de los residuos ($RSE$)

$$ RSE = \sqrt{\frac{RSS}{n-2}}= \sqrt{\frac{\sum_{i=1}^{n}(y_i - \hat{\beta_0}+ \hat{\beta}_1 x_i)^2 }{n-2}}$$

Al igual que para los valores de los parámetros, los valores de los errores estándar pueden calcularse a partir de los valores de las observaciones $(x_i,y_i)$. De todas formas, no te preocupes si estas fórmulas te parecen complicadas ya que la función `lm` de R calcula por ti el error estándar de los parámetros. 

En el ejemplo utilizado anteriormente para `Sales ~ TV` la estimación de coeficientes y sus errores estádar serían: 

```{r, results='as.is'}
mod=lm(formula=Sales~TV,data=adv)
# Para obteter los coeficientes debemos llamar a la función summary del modelo
summ=summary(mod)
coef=coef(summ)[,1:2]
knitr::kable(coef)
```


Una vez conocido el error estándar,  si el tamaño de la muestra es suficientemente grande, podemos calcular el intervalo de confianza al 95 % para el valor de los coeficientes como 

$$(\hat{\beta_1} - 2 SE(\hat{\beta_1})\, , \,\hat{\beta_1} + 2 SE(\hat{\beta_1}))$$

En general, como la desviación típica de los parámetros los estimamos a partir de valores muestrales, lo que ocurre es que la variable $\frac{\hat{\beta_1} - \beta_1}{SE(\hat{\beta_1})}$ sigue una distribución t de Student con $n-2$ grados de libertad. Por tanto el intervalo de confianza lo calcularemos como  

$$(\hat{\beta_1} - f_c SE(\hat{\beta_1})\, , \,\hat{\beta_1} + f_c SE(\hat{\beta_1}))$$

donde $f_c=qt(0.975,df=n-2)$ es el factor de cobertura análogo al que vimos en la unidad 5.

Se calcula normalmente el intervalo de confianza asociado al parámetro $\beta_1$, la pendiente de la recta, ya que un valor de este que contenga al 0, querrá decir que la relación lineal entre $Y$ y $X$ no estadísticamente significativa. 

Recordemos lo que significa un intervalo de confianza, que es que existe un 95% de probabilidad de que la estimación del intervalo para una muestra dada contenga al parámetro $\beta$ real. 
Simulemos un número elevado de muestras del ejemplo anterior y veamos en cuantas ellas el intervalo de confianza estimado contiene al $\beta_1$ real que es 3.

```{r,echo=FALSE,cache=TRUE,fig.height=4.5}
set.seed(5234)
nsim=5000
N=10 #Tamaño de las muestras
a=2
b=3
s=0.5

res=data.frame()
for(i in 1:nsim){
  x=runif(N)
  y=a + b*x + rnorm(N,0,s)
  mod=lm(y~x)
  sum=summary(mod)
  ae=mod$coefficients[1]
  be=mod$coefficients[2]
  asd=sum$coefficients[1,2]
  bsd=sum$coefficients[2,2]
  res <- rbind(res,data.frame(a,b,ae,be,asd,bsd))
}
# factor de cobertura
f=qt(0.975,N-2)
# Calculo si los intervalos de confianza contienen a b
# Intervalo de confianza para beta 1 be +/- f*bsd
res <- res %>% mutate(out=ifelse(be-f*bsd >b | be+f*bsd <b,TRUE,FALSE))
```

```{r}
ggplot(res %>% sample_n(100)) + geom_linerange(aes(x=1:100,ymin=be-f*bsd,ymax=be+f*bsd,color=out)) + 
  geom_hline(aes(yintercept=b),color="black")

```

El número de intervalos de confianza que no contienen $\beta=$ `r b`  es 
`r sum(res$out)` de `r nsim` simulaciones  (`r round(100*mean(res$out),3)` %).


## Test de Hipótesis

A partir de los errores estándar también podemos hacer contrastes de hipótesis, para determinar si los coeficientes son significativamente distintos de 0.

Típicamente lo que queremos saber es si la relación entre la respuesta $Y$ y los inputs $X$ detectada por el modelo lineal es real o ha podido ser fruto de la casualidad debido a la muestra. 

El test hipótesis más común establece como hipótesis nula:

- $H_0$: No hay relación entre $Y$ y $X$, frente a la hipótesis alternativa  
- $H_A$: Existe alguna relación entre $Y$ y $X$

De forma matemática, esto equivale a: 

- $H_0$: $\beta_1=0$  
- $H_A$: $\beta_1 \neq 0$


En primer lugar calculamos el t-valor asociado

$$t=\frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})}$$

Si la hipótesis nula es cierta $t$ sigue una distribución t de Student con $n-2$ grados de libertad

El p-valor lo calcularemos como la probabilidad de obtener un valor de $|t|$ mayor que el medido en la muestra, suponiendo cierta $H_0$ 

`p-valor = 2 * (1 - pt(abs(t),df=n-2))`

Veamos de nuevo un ejemplo en R, usando  los datos de *Advertising*. Ajustamos el modelo `Sales~TV`.

```{r}
mod=lm(Sales~TV,data=adv)
# La función summary muestra los resultados principales del ajuste, incluyendo los parámetros, 
# errores estandar, t valores y p valores
summary(mod)
```

En la tabla se muestra la estimación de los coeficientes, el error estándar, el estadístico t asociado y el p-valor considerando como hipótesis nula $H_0: \beta_i=0$. En este caso $\beta_1$ vale $0.047$ con un error estándar de $0.0026$. Se ve ya a simple vista que $0.047 \pm 2 \times 0.0026$ no contiene al 0. De todas formas el cálculo preciso del p-valor nos dice que la probabilidad de encontrar un valor tan extremo o más que $0.047$ dadas las características de la muestra y supuesta cierta la hipótesis nula es menor de $2 10^{-16}$. 

Para hacer más visible la significación de las estimaciones de los parámetros R añade tres asteriscos `***` al lado del p-valor si este es menor de $0.001$, añade `**` si es menor de $0.01$ y `*`si es menor de $0.05$

Es posible guardar en un data frame la tabla de estimaciones de la siguiente manera

```{r}
mod=lm(Sales~TV,data=adv)
summ=summary(mod)
coefs= summ$coefficients
knitr::kable(coefs)
```


## Determinación de la precisión global del modelo

En primer lugar puede medirse  el **error estándar de los residuos**

$$ RSE = \sqrt{\frac{RSS}{n-2}}= \sqrt{\frac{\sum_{i=1}^{n}(y_i - \hat{\beta_0}+ \hat{\beta}_1 x_i)^2 }{n-2}}$$

Cuanto más pequeño sea el $RSE$ mejor será el ajuste. El problema de esta medida es que depende de la escala de la variable $Y$, y una escala puede depender de cosas arbitrarias como las unidades elegidas para medir una magnitud, por tanto es un valor que cambia de un caso a otro y existen unos valores de referencia para saber a partir del valor numérico de $RSE$ como de bueno es un ajuste lineal

**Autotexto**
Fíjate que dividimos el error estándar de los residuos por $n-2$ en lugar de por $n-1$ como haciamos con la varianza. Esto es debido a razones similares a las que argumentamos en el caso de la varianza muestral, de por qué debiamos dividir por $n-1$ y no por $n$.
En este caso el $RSE$ pretende estimar el error irreducible $\sigma$ de modelo. Como se usan dos estimaciones muestrales $\hat{\beta_0}$ y $\hat{\beta_1}$ en lugar de los parámetros verdaderos $\beta_1$ y $\beta_2$, que desconocemos, para que $RSE$ sea un estimador no sesgado debemos dividir por $n-2$ 
**Fin Autotexto**

### $R^2$

El parámetro $R^2$ es una buena alternativa para medir la precisión del ajuste ya que se define como una proporción

$$R^2=\frac{TSS-RSS}{TSS}= 1- \frac{RSS}{TSS}$$

donde $TSS=\sum_i(y_i-\bar{y})^2$ y $RSS=\sum_i(y_i-\hat{y_i})^2$. 

$RSS$ está relacionada con  la varianza de los residuos, es decir la media de las desviaciones cuadráticas de los valores de las respuestas $y_i$ frente a los valores ajustados $\hat{y_i}= \beta_0 + \beta_1 x_i$. 
Mientras que $TSS$ está relacionada con la varianza de la respuesta, es decir la media de las desviaciones cuadráticas de los valores de las respuestas $y_i$ frente a su media.

Por tanto $R^2$ puede interpretarse como la fracción de la varianza total de $Y$ explicada por el modelo.

Para el caso del modelo lineal $R^2=r^2$, donde 

$$ r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}
  { \sqrt{\sum_{i=1}^{n} ( x_i - \bar{x})^2} \sqrt{\sum_{i=1}^{n} ( y_i - \bar{y})^2} }$$

es el coeficiente de correlación lineal.

El $R^2$ es útil para determinación de la precisión de otro tipo de modelos, pero en ese caso $R^2 \neq r^2$. Debe calcularse según la definición original. 

$R^2$ toma valores siempre entre 0 y 1. Será uno cuando la relación entre $Y$ y $X$ sea perfectamente lineal, con error irreducible 0. En el otro extremo será 0 cuando no hay ninguna relación entre $Y$ y $X$. 


### Error de predicción 

Una vez estimados los parámetros del modelo, para un valor nuevo de $X=x$ podemos estimar la respuesta como:

$$ \hat{y} = \hat{\beta_0} + \hat{\beta_1} x $$


Pero ya hemos visto que existe una incertidumbre en la determinación de los parámetros, además de que el modelo tiene implícito un término de error irreducible. Por tanto esta estimación tiene una incertidumbre. La buena noticia es que esta incertidumbre la podemos estimar. 

Debemos distinguir entre lo que es el error en la estimación (error reducible) y el error en la predicción (error reducible más error irreducible). 

- Los errores estándar que hemos visto para la estimación de los coeficientes tienen que ver con el **error reducible**. A partir de ellos puedo calcular un intervalo de confianza para determinar  como de cerca está $\hat{y}$ de $f(X)=E(Y|X)$. 

- Además, en la práctica asumir un modelo lineal para $f(X)$ es una aproximación de la realidad. Está es otra fuente potencial de error que se denomina *sesgo (bias)*. De momento nos olvidaremos de esta fuente de error. 

- Pero además, aunque el modelo lineal fuera perfecto y conociéramos los coeficientes verdaderos, aun nos queda una fuente de error: el error irreducible $\epsilon$, es decir lo que varía $Y$ respecto de $\hat{Y}$. Para dar un intervalo de confianza a las predicciones debemos tener en cuenta los errores estándar de los parámetros y una estimación del error irreducible.


De forma concreta, podemos estimar el  **Intervalo de confianza** para la estimación ($E(Y|X=x)$) como 

$$\hat{y} \pm  f_c \sigma \sqrt{\frac{1}{n} +\frac{(x-\bar{x})^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2}} $$

Y un intervalo de confianza para la predicción como 

$$\hat{y} \pm  f_c \sigma \sqrt{1 + \frac{1}{n} +\frac{(x-\bar{x})^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2}} $$

donde $\sigma$ lo estimamos mediante el $RSE= \sqrt{\frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{n-2}}$ y  $f_c= qt(0.975,df=n-2)$ es el factor de cobertura que para muestras grandes vales aproximadamente 2. 

Veamos de nuevo un ejemplo basado en el modelo `Sales~TV` en el conjunto de datos `adv`. En la gráfica se muestran con lineas azules los  intervalos de confianza para la estimación de $E(Y|X=x)$ y en gris los intervalos de confianza para las predicciones. 



```{r}
mod=lm(Sales~TV,data=adv)
summ=summary(mod)
rse=summ$sigma
f=qt(0.975,nrow(adv)-2)
xx=seq(0,300,1)
pred1=predict(mod,newdata = data.frame(TV=xx),se.fit = TRUE,interval = "confidence")
pred2=predict(mod,newdata = data.frame(TV=xx),se.fit = TRUE,interval = "prediction")
pred.df=data.frame(x=xx,pred2$fit,se.conf=pred1$se.fit)

ggplot(adv) + geom_point(aes(TV,Sales),color="orange") + geom_line(aes(x,fit),data=pred.df,color="black") + 
  geom_line(aes(x,fit-f*se.conf),data=pred.df,color="blue2") + 
  geom_line(aes(x,fit+f*se.conf),data=pred.df,color="blue2") + 
  geom_line(aes(x,lwr),data=pred.df,color="grey") + 
  geom_line(aes(x,upr),data=pred.df,color="grey")

```

Para hacer predicciones en R usando un modelo entrenado usaremos la función genérica `predict`.
Cada tipo de modelo de aprendizaje estadístico en R, tiene una función `predict` asociada. 
Observa las llamadas a `predict` en el ejemplo anterior.

```{r, eval=FALSE}
pred1=predict(mod,newdata = data.frame(TV=xx),se.fit = TRUE,interval = "confidence")
pred2=predict(mod,newdata = data.frame(TV=xx),se.fit = TRUE,interval = "prediction")
```

Se pasan como argumentos:

- `mod`: el modelo ajustado con `lm`. 
- `newdata`: un data frame conteniendo por columnas los datos (nuevos) de las variables predictoras. En este caso `newdata`  solo tiene una columna llamada `TV`, con los valores sobre los cuales queremos hacer las predicciones. Si no se proporciona un argumento `newdata`, `predict` devuelve las predicciones sobre el conjunto de entrenamiento. 
- `se.fit`: TRUE/FALSE si queremos estimación de error estándar de las predicciones. Argumento opcional. 
- `interval`: intervalo de estimación (`confidence`) o de predicción (`prediction`). Argumento opcional. 



Para terminar la sección, veamos un ejemplo de ajuste significativo pero poco útil. Cuando tenemos un número elevado de datos, es posible encontrar casos en los que el ajuste obtiene un coeficiente con un p-valor muy bajo pero sin embargo no tiene ninguna utilidad predictiva, ya que el error irreducible $\epsilon$ es bastante mayor que la variabilidad de las predicciones $\hat{y_i}$en todo el rango de $X$ observadas. Por ejemplo 

```{r,echo=TRUE}
N=1000
x=runif(N,0,1);y= 0.1*x + rnorm(N,0,0.4)
df=data.frame(x,y)
mod = lm(y~x,data=df)
summary(mod)
```


```{r}
plot(x,y,cex=0.5)
xx=seq(0,1,.01)
pred=predict(mod,newdata = data.frame(x=xx),interval = "confidence")
lines(xx,pred[,1],col=2)
lines(xx,pred[,2],col=2)
lines(xx,pred[,3],col=2)
```




# Regresión lineal múltiple

Hasta ahora hemos estudiado la regresión lineal simple,  un método para predecir una respuesta en base a una sola variable predictora. Sin embargo, en la práctica a menudo tenemos más de un predictor. Por ejemplo, en los datos de publicidad, hemos examinado la relación entre las ventas y la publicidad televisiva, pero también tenemos datos de la cantidad de dinero gastado en  publicidad en radio y en prensa. Y nos gustaría saber si las ventas están relacionadas con el presupuesto de publicidad para  estos dos medios. ¿Cómo podemos ampliar nuestro análisis para acomodar estos dos predictores adicionales?

Un modelo lineal con $p$ variables predictoras se expresa como: 

$$Y= \beta_0 +  \beta_2 X_1 +  \beta_2 X_2 + \dots +  \beta_p X_p + \epsilon$$

El objetivo es el mismo que en el caso simple, determinar los valores de los parámetros $\hat{\beta_j}$ que minimizan la suma de cuadrados de los residuos. Una vez estimados los parámetros podemos hacer predicciones con 

$$\hat{y}= \hat{\beta_0} +  \hat{\beta_2} x_1 +  \hat{\beta_2} x_2 + \dots +  \hat{\beta_p} x_p$$

Para derivar los resultados hay usar algo de álgebra matricial. Construyamos la matriz $X$ con las observaciones de cada input en columnas, con 1's en la primera columna.

$$
  \mathbf{X}  = \;
   \begin{pmatrix}
      1 & x_{11} &  .. & x_{1p}\\
      1 & x_{21} &  .. & x_{2p}\\
      ..         &  .. & ..    \\
      1 & x_{n1} & ..  & x_{np}    
  \end{pmatrix}
$$

Sea $\mathbf{\beta}$ el vector de parámetros

$$
\mathbf{\beta} = \;
   \begin{pmatrix}
   \beta_0 \\
   \beta_1 \\
    ..     \\
   \beta_p
   \end{pmatrix}
$$


La suma de los cuadrados de los residuos es

$$RSS= \sum_{i=1}^{n}(y_i - \hat{\beta_0} + \hat{\beta}_1 x_1 + \dots + \hat{\beta}_p x_p)^2 $$

que puede escribirse en forma matricial como 

$$RSS = (\mathbf{y}-\mathbf{X}\beta)^T (\mathbf{y}-\mathbf{X}\beta)$$

La condición de mínimo puede escribirse de forma vectorial

$$ \frac{\partial RSS}{\partial \beta}= −2 \mathbf{X}^T (\mathbf{y}-\mathbf{X}\beta) $$

Y el valor de los parámetros $\beta$ que minimizan los RSS viene dado por

$$\hat{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} $$.

El operador $^T$ significa matriz traspuesta, es decir una matriz que respecto a la original tiene intercambiadas las filas por las columnas. Los productos que se indican siguen la regla del producto de matrices, es decir $C=AB$ donde el elemento de la fila $i$ y columna $j$ de $C$ viene dado por

$$c_{ij} = \sum_{k=1}^{k=m} a_{ik} b_{kj}$$


De nuevo no te preocupes si tienes poca experiencia con el álgebra matricial y estas fórmulas te resultan difíciles de interpretar. En este curso, y en general en la experiencia del científico de datos, los ordenadores hacen los cálculos por ti. Aunque ello no quita que sea recomendable conocer la teoría que hay detrás de los métodos. 



## Precisión de los coeficientes

Los errores típicos en la estimación de los coeficientes vienen dados por

$$ Var(\hat{\beta}) = (\mathbf{X}^T \mathbf{X}) \sigma^2  $$

donde $\sigma$ lo estimaremos, igualmente,  a partir de error estándar de los residuos 

$$ \hat{\sigma^2} = RSE = \frac{1}{n-p-1} \sum_i (y_i - \hat{y_i})^2$$

Podemos realizar contrastes de hipótesis de la misma manera que en el caso de la regresión simple para cada uno de los coeficientes estimados. 


Veamos un ejemplo, con los datos de publicidad pero ahora utilizando todas las variables predictoras disponibles.

Ajustemos ahora un modelo: 
$Sales = \beta_0 + \beta_1 TV + \beta_2 Radio +\beta_3 Newspaper$

```{r}
mod=lm(Sales ~ TV + Radio + Newspaper, data=adv)
summary(mod)
```

## Interpretación de los coeficientes

El coeficiente $\beta_j$ puede interpretarse como el efecto promedio que tiene en $Y$ incrementar en una unidad $X_j$.

Esta interpretación es correcta cuando los predictores $X_j$ no están correlacionados. En este caso también cada coeficiente puede estimarse y contrastarse de forma individual

Sin embargo si los predictores están correlacionados:

- La varianza de todos los coeficientes tiende a aumentar, a veces
dramáticamente

- Las interpretaciones pueden ser  peligrosas. Hay que tener en cuenta que cuando $X_j$ cambia,
todo lo demás cambia.

Veamos en nuestro ejemplo la estimación de coeficientes del modelo con sus errores estándar y la correlación entre las variables predictoras:

```{r,echo=FALSE,results='as.is'}
library(knitr)
mod=lm(Sales ~ TV + Radio + Newspaper, data=adv)
summ=summary(mod)
kable(summ$coefficients,digits = 3)
```


Correlaciones:


```{r, echo=FALSE,results='as.is'}
kable(round(cor(adv),2))
```

Se observa que la mayor correlación entre las ventas y las variables predictoras se da con la variable `TV`y esto se refleja en la estimación de coeficientes del modelo. Sin embargo tanto `Radio` como `Newspaper` también presentan correlación con las ventas y están correlacionadas entre si con un coeficiente de $0.35$. Está correlación es la responsable de que la estimación del coeficiente para `Newspaper` sea compatible con 0. Toda la dependencia queda absorvida en el coeficiente estimado para `Radio`.



## Algunas Preguntas Importantes


1. ¿Es al menos uno de los predictores $X_1, X_2,\dots,X_p$ útil para predecir el valor de $Y$?

2. ¿Ayudan todos los predictores a  explicar $Y$, o solo es útil un subconjunto de ellos?

3. ¿Cómo de bien se ajusta  el modelo a los datos?


### ¿Es al menos un predictor útil?


En este caso debemos hacer un contraste de hipótesis donde la hipótesis nula es: 

$$ H_0: \beta_1=\beta_2=\dots=\beta_p=0$$
La respuesta nos la da el estadístico 

$$ F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}  \sim F_{p,n-p-1}$$

que sigue un distribución F de Snedecor con $p$ y $n-p-1$ grados de libertad. 

De nuevo no te preocupes demasiado por cómo es esta distribución, si tienes curiosidad puedes consultar algún libro de Estadística de las referencias de las unidades 4 y 5 y leer la secciones relacionadas con el análisis de varianza o test ANOVA. 

De todas formas, lo único que nos interesa es que el estadístico F lo calculamos a partir de los datos de la muestra con la formula anterior y que supuesta la hipótesis nula sigue una distribución F con $p$ y $n-p-1$ grados de libertad. Dejaremos que R, calcule el p-valor asociado a ese valor y nos diga como es de probable supuesta cierta la hipótesis nula encontrar un valor tan o más extremo que el encontrado en nuestra muestra. 

```{r,echo=FALSE}
mod=lm(Sales ~ ., data=adv)
summ=summary(mod)
summ
```

Podemos ver en la última linea que el valor de $F$ calculado en nuestra muestra vale `r summ$fstatistic[1]` y el p-valor asociado es `r 2*(1-pf(summ$fstatistic[1],df1=1,df2=198))`. Estos valores pueden calcularse, si lo necesitáramos usando las funciones de R relativas a la distribución F.

```{r}
# Estadístico F
summ$fstatistic
# p-valor
2*(1-pf(summ$fstatistic[1],df1=summ$fstatistic[2],df2=summ$fstatistic[3]))
```


También puede mirarse a los p-valores asociados a los coeficientes. Pero con muchas variables, es normal que aunque todos los coeficientes del *modelo real* sean nulos que para nuestra muestra de datos algún coeficiente salga no nulo con p-valor <0.05 por puro azar. 



### ¿Como de bueno es el ajuste del modelo?

Al igual que en la regresión simple podemos seguir utilizando como métricas de precisión del ajuste a: 

- Error estándar de los residuos $RSE = \sqrt{\frac{RSS}{n-p-1}}= \sqrt{\frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{n-p-1}}$

- $R^2=\frac{TSS-RSS}{TSS}= 1- \frac{RSS}{TSS}$  donde $TSS=\sum_i(y_i-\bar{y})^2$ y $RSS=\sum_i(y_i-\hat{y_i})^2$


En el caso multi-lineal $R^2$ coincide con la correlación lineal entre las $y_i$ observadas y las $\hat{y_i}$ ajustadas.  

De todas formas, al aumentar el número de variables aumenta la posibilidad de tener un sobre-entrenamiento en nuestros datos de entrenamiento. Como veremos más adelante, es posible estimar el efecto en error de test introduciendo  efectos asociados al tamaño del modelo para evitar un sobre-entrenamiento indeseable para nuestros modelos.

### ¿Cuales son las variables importantes?

Ya hemos visto que en la regresión multi-lineal, fijarse únicamente en los p-valores de los coeficientes individuales puede llevar a confusiones. 

La alternativa más directa, es evaluar todos los posibles modelos que pueden formarse con las p variables y elegir la mejor combinación usando algún criterio que corrija el error de entrenamiento con el tamaño (número de variables elegidas) del modelo.

Sin embargo, está alternativa cuando el número de variables es grande es inviable. El número de modelos a evaluar crece con $2^p$. Para $p=40$ tenemos ¡más de 1 billón ($10^{12}$) de posibles modelos! 

Por tanto tenemos que idear una alternativa que busque sobre una selección de todos los posibles modelos. En la última sección de esta unidad  discutiremos con mayor profundidad los métodos y criterios  para la elección de un subconjunto óptimo de variables para nuestro modelo.



# Predictores Cualitativos


Hasta ahora hemos tratado con variables predictoras continuas, pero es posible trabajar también con predictores cualitativos, es decir que toman solo un conjunto discreto de valores, sin ningún orden definido.

Veamos por ejemplo los datos  de *credit card* en la siguiente figura.

```{r,echo=FALSE,cache=TRUE}
credit=read.csv("../Datasets/Credit.csv")
credit=credit[-1]
pairs(credit)
```


Además de las variables cuantitativas, hay cuatro variables cualitativas : `Gender` (Male/Female) , `Student`(Yes/No), `Married`( Yes/No) , y `Ethnicity` (Caucásicos , afroamericanos (AA) o asiático).



Si queremos predecir el balance de la tarjeta de crédito en función de si la persona considerada es estudiante o no, debemos definir una nueva variable:

$$ 
x_i = 
\begin{cases}
1 \; \text{si la persona i-esima es estudiante} \\
0 \; \text{si la persona i-esima no es estudiante} \\
\end{cases}
$$

quedando el modelo lineal en 

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i = 
\begin{cases}
\beta_0 + \beta_1  + \epsilon_i &  \text{si la persona i-esima es estudiante} \\
\beta_0  + \epsilon_i           &  \text{si la persona i-esima no es estudiante} \\
\end{cases}
$$


Veamos cómo serían los coeficientes de un modelo lineal cuya variable respuesta es el balance de la tarjeta de crédito (es decir, el gasto mensual) frente a la variable binaria estudiante. En R definimos el modelo como si se tratase de una variable cuantitativa, no tenemos que preocuparnos de hacer la variable binaria. 

```{r,results='as.is'}
res = summary(lm(Balance ~ Student,data=credit)) 
kable(res$coefficients,digits=3)
```

Los coeficientes estimados tienen una interpretación directa:

- El balance estimado para un estudiante es de 396 \$ mayor que un no estudiante.
- El balance estimado para un no estudiante es de 480 \$. `StudentNo` es el nivel base que queda englobado en el `Intercept` .

Con más de dos niveles ($m$) debemos crear $m-1$ variables *dummy*. En el caso de la etnia hay 3 valores posibles: asiática, caucásica, afro-americana . Debemos crear 2 variables nuevas 

$$ 
x_{1i} = 
\begin{cases}
1 \; \text{si la persona i-esima es asiática} \\
0 \; \text{si la persona i-esima no es asiática} \\
\end{cases}
$$

$$ 
x_{2i} = 
\begin{cases}
1 \; \text{si la persona i-esima es caucásica} \\
0 \; \text{si la persona i-esima no es caucásica} \\
\end{cases}
$$

Y ambas variables forman parte del modelo lineal

$$
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i = 
\begin{cases}
\beta_0 + \beta_1  + \epsilon_i &  \text{si la persona i-esima es asiática} \\
\beta_0 + \beta_2  + \epsilon_i &  \text{si la persona i-esima es caucásica} \\
\beta_0  + \epsilon_i           &  \text{si la persona i-esima es afro-americana} \\
\end{cases}
$$

Observemos, que se define siempre una variable *dummy* menos que el número de niveles. El nivel sin variable dummy, en este caso los afro-americanos, recibe el nombre de *baseline*.

Veamos los resultados del ajuste usando la variable `Ethnicity`. Para el modelo, de nuevo, solo tenemos que decir que `Etnicity` es un predictor y ya se encarga el de crear las variables *dummy* asociadas. 


```{r,results='as.is'}
res = summary(lm(Balance ~ Ethnicity,data=credit)) 
kable(res$coefficients,digits=3)
```

# Extensiones del modelo lineal

## Interacción entre variables

En nuestro análisis anterior  de los datos de publicidad, hemos supuesto que el efecto en las ventas de aumentar una unidad el gasto en un  medio publicitario es independiente de la cantidad gastada en los otros medios de comunicación.

Por ejemplo, el modelo lineal para las ventas

$$Sales = \beta_0 + \beta_1 TV + \beta_2 Radio$$

establece que el efecto promedio de las ventas del aumento de una sola unidad del presupuesto en  TV será siempre $\beta_1$, independientemente de la cantidad gastada en la radio.


Para ver si nos hemos dejado alguna dependencia importante en le modelo es conveniente mirar los residuos. 

Veamos  el ajuste y los residuos de un modelo  $Sales = \beta_0 + \beta_1 TV + \beta_2 Radio$

```{r,echo=FALSE,fig.height=3.7,cache =TRUE,results='hide'}
mod=lm(Sales ~ TV + Radio, data=adv)

x=seq(0,300,10)
y=seq(0,50,5)
new.data = expand.grid(TV=x,Radio=y)
new.data$pred = predict(mod,newdata = new.data)
adv$pred=mod$fitted.values

z=matrix(new.data$pred,nrow=length(x))

persp3D(x,y,z,facets=TRUE,
        zlab="Sales",xlab="TV", ylab="Radio",theta=30, phi=30,colkey = FALSE,r=5,d=5)
scatter3D(adv$TV, adv$Radio, adv$Sales,col=1,add=TRUE,pch=19,cex=0.7)
apply(adv,1,function(row) lines3D(x=rep(row[2],2),y=rep(row[3],2),z=c(row[5],row[6]),
                                     colkey=FALSE,add=TRUE,col=1))
```

Se observa que cuando las cantidades gastadas en TV y Radio son grandes a la vez, el modelo sub-estima los valores, mientras que cuando las cantidades son altas en uno solo de los 2 medios, el modelo sobre-estima.
Este hecho nos está indicando que hay una sinergia entre los anuncios simultáneos en ambos medios, que hace que se incrementen las ventas

Añadir un término de interacción al modelo equivale a transformar el modelo en:

$$Sales = \beta_0 + \beta_1 TV + \beta_2 Radio + \beta_3 TV*Radio $$ 

En R definimos este modelo usando como fórmula `Sales ~ TV*Radio`. También puede usarse el operador `:`, el modelo definido por `Sales ~ TV:Radio` solo contendría el término de interacción y no los términos independientes de `Radio`y `TV`. Resumiendo `Sales ~ TV*Radio` es lo mismo que `Sales ~ TV + Radio + TV:Radio`.

```{r}
mod=lm(Sales ~ TV*Radio, data=adv)
summary(mod)
```

Los resultados sugieren que las interacciones son importantes:

- El  p-valor para el término de interacción de TV × radio es extremadamente bajo, lo que indica que existe una fuerte evidencia de $H_A: \beta_3 \neq 0$ .
- El $R^2$ para el modelo con interacción es de 0.968, en comparación con  el 0.897 para el modelo predice las ventas usando TV y Radio sin un término de interacción.

### Interacción con variables cualitativas

Consideremos de nuevo los datos de crédito y consideremos que queremos predecir el `balance` en función del  `income` y de la variable `Student`. Un modelo sin interacción sería

$$
balance \sim \beta_0 + \beta_1 \, income +  
\begin{cases}
\beta_2   &  \text{si la persona i-esima es estudiante} \\
0         &  \text{si la persona i-esima no es estudiante} \\
\end{cases}
$$
$$
= \beta_1 \, income + 
\begin{cases}
\beta_0 + \beta_2   &  \text{si la persona i-esima es estudiante} \\
\beta_0           &  \text{si la persona i-esima no es estudiante} \\
\end{cases}
$$

El *balance* para  estudiantes y los no estudiantes crece con *income* con la misma pendiente pero con distinta ordenada en el origen.

Si añadimos un término de interacción tenemos: 

$$
balance \sim \beta_0 + \beta_1 income +  
\begin{cases}
\beta_2  + \beta_3\, income &  \text{si  estudiante} \\
0                                     &  \text{si  no es estudiante} \\
\end{cases}
$$
$$
= 
\begin{cases}
(\beta_0 + \beta_2) + (\beta_1 + \beta_3)\, income   &  \text{si  estudiante} \\
\beta_0 +  \beta_1 \, income                         & \text{si no estudiante} \\
\end{cases}
$$

Veamos las diferencias entre ambos modelos

```{r}
summary(lm(Balance ~ Income + Student,data=credit))$coefficients
summary(lm(Balance ~ Income*Student,data=credit))$coefficients 

```

Y de forma gráfica 

```{r,echo=FALSE,fig.height=5,fig.width=10}
m1 = lm(Balance ~ Income + Student,data=credit)
m2 = lm(Balance ~ Income*Student,data=credit)

new.data.s=data.frame(Income=seq(0,200,50),Student="Yes")
new.data.ns=data.frame(Income=seq(0,200,50),Student="No")

y1s=predict(m1,new.data.s)
y1ns=predict(m1,new.data.ns)

y2s=predict(m2,new.data.s)
y2ns=predict(m2,new.data.ns)

res=data.frame(Income=rep(seq(0,200,50),4),Student=rep(c("Yes","No"),each=10),
               model=rep(c("Independent","Interaction"),2,each=5),
               Balance=c(y1s,y2s,y1ns,y2ns))

ggplot(res) + geom_point(aes(Income,Balance,color=Student),size=1,alpha=0.5,data=credit) + 
  geom_line(aes(Income,Balance,color=Student)) + facet_wrap(~model)

```

El modelo sin interacción, ajusta dos rectas paralelas en función de los ingresos, una para `StudentYes` y otra para `StudentNo`, en cambio en el modelo con interacción son dos rectas con pendiente e intercepto distintos. 

## No linealidad

Es posible extender el modelo lineal a relaciones no lineales usando el marco de la regresión múltiple. 

Por ejemplo, podemos hace ajustes polinómicos, añadiendo nuevas variables que representan las potencias de la variable predictora. 


Estudiemos la dependencia de la demanda eléctrica española diaria  con la temperatura media.

Ajustemos a un polinomio de orden 2

$$Demanda = \beta_0 + \beta_1 T + \beta_2 T^2$$


```{r,echo=TRUE}
demanda = read.csv("../Datasets/demanda_diaria.csv",sep=";",stringsAsFactors = FALSE)
demanda=mutate(demanda,fecha=as.Date(fecha),
               wd=factor(weekdays(fecha),levels=c("lunes","martes","miércoles","jueves",
                                                  "viernes","sábado","domingo")))

# Filtro solo días martes, miercoles y jueves para no tener que considerar la dependencia con el día de la semana
dem_mxj  = demanda %>% filter(wd %in% c("martes", "miércoles", "jueves"),demanda_mwh>25000)
# Elimino festivos
festivos=c("01-01","01-06","05-01","08-15","08-15","10-12","11-01","12-06","12-08","12-24","12-25","12-31")
ss=c("2014-04-17","2014-04-18","2014-04-19","2014-04-20","2015-04-02","2015-04-03","2015-04-04","2015-04-05")
dem_mxj = dem_mxj %>% filter(!format(fecha,"%m-%d") %in% festivos,!fecha %in% ss)

mod = lm(demanda_mwh ~ temperatura_mean + I(temperatura_mean^2),data=dem_mxj)
summary(mod)
```

Predicciones

```{r,fig.height=3.5,echo=FALSE}
tmp=dem_mxj
tmp$pred = predict(mod)
tmp= tmp %>% arrange(temperatura_mean)
ggplot(tmp) +geom_point(aes(temperatura_mean,demanda_mwh)) + 
  geom_line(aes(temperatura_mean,pred),col="red")
```

Evaluemos la inclusión de términos polinómicos de  más alto orden

```{r,results='as.is'}
# Evaluemos  4 modelos
models = c("demanda_mwh ~ temperatura_mean",
  "demanda_mwh ~ temperatura_mean + I(temperatura_mean^2)",
  "demanda_mwh ~ temperatura_mean + I(temperatura_mean^2) + I(temperatura_mean^3)",
  "demanda_mwh ~ temperatura_mean + I(temperatura_mean^2) + I(temperatura_mean^3) + I(temperatura_mean^4)")

tmp= tmp %>% arrange(temperatura_mean)

res=data.frame()
pred=data.frame()
for(i in 1:4){
  mod=lm(as.formula(models[i]),data=tmp)
  tmp$pred = predict(mod)
  pred=rbind(pred,data.frame(orden=i,tmp[,c("fecha","temperatura_mean","demanda_mwh","pred")]))
  summ= summary(mod)
  res=rbind(res,
            data.frame(orden=i,rse=summ$sigma,r2=summ$r.squared,r2.adj=summ$adj.r.squared))

  }
kable(res)

```

Se observa que el $R^2$ disminuye conforme aumentamos el orden del polinomio de ajuste. La tabla además muestra el $R^2$ ajustado que contiene una penalización debida al aumento de variables predictoras. Esté estadístico también disminuye hasta el orden considerado. 

Veamos las curvas ajustadas

```{r}
 ggplot(pred) + geom_point(aes(temperatura_mean,demanda_mwh)) + 
  geom_line(aes(temperatura_mean,pred,color=factor(orden))) + 
              scale_color_hue("Orden")

```


# Evaluación de modelos

Como ya vimos en la unidad 6, para evaluar el rendimiento de un modelo debemos separar los datos en un conjunto de entrenamiento y otro de test. 

Recordemos de nuevo la diferencia entre los errores de *entrenamiento* y *test*

El **error de test** es el promedio de los errores que resultan de aplicar un cierto método de aprendizaje estadístico en una observación nueva, que no ha sido utilizada para el entrenamiento del modelo.

Por el contrario, el **error de entrenamiento** se calcula fácilmente aplicando el modelo ajustado a las mismas observaciones que se han usado para entrenarlo (determinar sus parámetros en el caso de un modelo paramétrico).

Pero, como ya sabemos, el error de entrenamiento puede ser muy diferente al error de test, y normalmente el primero subestima notablemente el segundo.

![](img/train_vs_test_error.png)

La mejor solución es disponer un gran conjunto de test expresamente designado. Pero, a menudo no es posible disponer de él.

## Estrategia del conjunto de validación

La estrategia más sencilla consiste en dividir aleatoriamente los datos en dos conjuntos. Uno para entrenar el modelo y otro para evaluar los resultados

El modelo se ajusta en el conjunto de entrenamiento, y se utiliza para predecir la
respuesta a las observaciones en el conjunto de validación.

El error de validación  proporciona una estimación del error de test. 
Normalmente se calcula el  MSE en el caso de una respuesta cuantitativa y la tasa de  error de clasificación en 
el caso de una respuesta cualitativa (discreta).


```{r, out.height= 400, out.width= 900, fig.retina = NULL}
knitr::include_graphics("img/validation_set.png")
```

Dividimos los datos aleatoriamente en dos partes: la izquierda para entrenamiento y la derecha  para la validación.

En el ejemplo de la dependencia de la demanda eléctrica con la temperatura, estimemos mediante la estrategia del conjunto de validación, el error de test para ajustes polinómicos de diferente orden. 
Usamos $2/3$ de los datos para entrenamiento y el resto para validación.

Veamos los resultados de los errores de test para diferentes elecciones de los conjuntos de test y valoración

```{r,fig.height=4}

n = nrow(dem_mxj)
nrep=20

set.seed(123)
test.rmse= data.frame()  
train.rmse= data.frame()  
for(i in 1:nrep){
  train=sample(1:n,round(2*n/3))
  test = -train
  for(order in c(1:8)){
    mod=lm(demanda_mwh ~ poly(temperatura_mean, order, raw=TRUE),data=dem_mxj[train,])
    rmse=sqrt(mean((dem_mxj[train,]$demanda_mwh-mod$fitted.values)^2))
    train.rmse=rbind(train.rmse,data.frame(it=i,orden=order,rmse=rmse))
    pred = predict(mod,newdata = dem_mxj[test,])
    rmse=sqrt(mean((dem_mxj[test,]$demanda_mwh-pred)^2))
    test.rmse=rbind(test.rmse,data.frame(it=i,orden=order,rmse=rmse))
    if(rmse>1e5){
      plot(dem_mxj$temperatura_mean[test],pred)
      points(dem_mxj$temperatura_mean[train],dem_mxj$demanda_mwh[train],col=2,cex=0.5)
    }
  }
}

err.df  = rbind(data.frame(set="train",train.rmse),data.frame(set="test",test.rmse))

ggplot(err.df) + 
  geom_point(aes(orden,rmse,color=factor(it)),size=1) + geom_line(aes(orden,rmse,color=factor(it))) + 
  facet_wrap(~set, scales="free") + theme(legend.position="none")

```



Las diferentes curvas corresponden a distintas elecciones de los conjuntos de entrenamiento y validación. Sobre todo, para ajustes con orden polinómico alto se observa mucha variación de los resultados del error de test de un conjunto a otro.

La estimación del error de test sobre el conjunto de validación puede ser  muy
variable, y depende de que observaciones se  incluyan en el conjunto de entrenamiento, y que observaciones se incluyan en el conjunto de validación.

En la estrategia del conjunto de  validación, sólo un subconjunto de las observaciones, aquellas que se incluyen en el conjunto de entrenamiento, se utilizan para ajustar el modelo.
Esto sugiere que el error estimado en el conjunto de validación puede tender a sobrestimar el error de test comparado con un modelo ajustado en el conjunto de datos total.


## Validación cruzada (k-fold cross validation)

El método de validación cruzada ayuda a mejorar las estimaciones del error de test, superando parte de los problemas que hemos mencionado anteriormente.

La idea del método es dividir aleatoriamente los datos en K conjuntos de igual tamaño. 
Entonces, dejamos aparte el conjunto k, ajustamos el modelo en las restantes K-1 partes y hacemos predicciones sobre el conjunto k.

Repetimos el procedimiento para cada parte $k=1,2,\dots,K$ y combinamos los resultados de los errores de test obtenidos  

![](img/k_fold_cv.jpg)



Veamos ahora como estimar el error de test este caso. Llamemos  $C_1,C_2, \dots,C_K$ a los K conjuntos en los que hemos dividido los datos. Cada conjunto contiene $n_k$ observaciones. Si $N$ es múltiplo de K, $n_k=N/K$ y si no haremos el reparto lo más equitativamente posible.

Estimamos el $MSE$ de test como 

$$ CV_{(K)} = \sum_{k=1}^K \frac{n_k}{n} MSE_k$$

donde $MSE_k = \sum_{i\in C_k} (y_i - \hat{y_i})^2/n_k$ e $\hat{y_i}$ es el ajuste obtenido para la observación i-esima por un modelo ajustado con conjunto total con la parte $C_k$ eliminada.

Puede estimarse el error en la estimación del error de test como 

$$ SE(CV_{(K)}) = \sqrt{\frac{(MSE_k - \bar{MSE_k})^2}{K-1}}$$

La fórmula anterior **no es correcta** del todo  pero es útil para hacerse una idea del posible error en la estimación del error de test


Veamos un ejemplo del uso de la validación cruzada, con $k=5$, para estimar error de test para los diferentes ajustes polinómicos de la demanda eléctrica en función de la temperatura 

```{r}
K=5
set.seed(112)
folds = sample(rep(1:K, length = nrow(dem_mxj)))
n=nrow(dem_mxj)
cv.mse= data.frame()  
train.mse= data.frame()  
for(k in 1:K){
  train=which(folds!=k)
  test=-train
  for(order in c(1:8)){
    mod=lm(demanda_mwh ~ poly(temperatura_mean, order, raw=TRUE),data=dem_mxj[train,])
    mse=mean((dem_mxj[train,]$demanda_mwh-mod$fitted.values)^2)
    train.mse=rbind(train.mse,data.frame(k=k,orden=order,num=length(train),mse=mse))
    pred = predict(mod,newdata = dem_mxj[test,])
    mse=mean((dem_mxj[test,]$demanda_mwh-pred)^2)
    cv.mse=rbind(cv.mse,data.frame(k=k,orden=order,num=n-length(train),mse=mse))
  }
}

err.df  = rbind(data.frame(set="train",train.mse),data.frame(set="cv",cv.mse))

cv.err = err.df %>% filter(set == "cv")  %>% group_by(orden) %>%
  summarise(mse=sum(num*mse)/n)


ggplot(err.df %>% filter(set == "cv")) + 
  geom_point(aes(orden,sqrt(mse),color=factor(k)),size=1) + geom_line(aes(orden,sqrt(mse),color=factor(k))) +
  geom_line(aes(orden,sqrt(mse)),data=cv.err,color="black",size=1) +
  scale_color_brewer("fold",palette = "Set1") + 
    theme(legend.position="bottom")

```


La curva negra es la estimación por método de validación cruzada del error de test en función del orden del polinomio de ajuste. Se observa que él mínimo está entre 4 y 6. En este caso, siguiendo la máxima de cuanto más simple mejor, elegiríamos orden 4 para el ajuste de nuestros modelos predictivos de la demanda eléctrica en función de la temperatura.


¿Cuál es la elección adecuada de $K$ ? 

Cuanto más grande es $K$ mayores son los conjuntos de entrenamiento y por tanto menor es el sesgo de los modelos ajustados. En el extremo $K=n$ cada modelo se entrena con $n-1$ observaciones, por tanto todos los modelos entrenados serán muy parecidos al que obtendríamos entrenando con el conjunto completo.

Sin embargo, cuanto más grande es $K$ las predicciones sobre los distintos conjuntos de validación, están muy correlacionadas entre si, por tanto su varianza será mayor (la varianza en series de datos correlacionadas es mayor que si no lo están)

Por tanto, tenemos de nuevo que adoptar un compromiso entre sesgo y varianza. La experiencia y muchos estudios al respecto, recomiendan usar $K=5$ o $K=10$ para la validación cruzada.

# Selección de variables

Ahora vamos a centrarnos en problemas donde disponemos de múltiples variables predictoras. Como ya hemos visto utilizar en los modelos demasiadas variables, puede producir efectos de "sobre-entrenamiento" que empeoren los resultados al aplicar los modelos sobre datos nuevos.

Vamos a ver unos métodos para evaluar el error de test para las diferentes combinaciones de variables que pueden entrar en los modelos. En este curso nos centraremos de nuevo en modelos lineales, aunque las ideas son generalizables a otros modelos más complicados o a problemas de clasificación.


En general estos métodos pretenden reducir el número de predictores que intervienen en los modelos, con dos objetivos:


- **Reducir la varianza**: Se evita el sobre-entrenamiento y por tanto la varianza excesiva en las predicciones en un conjunto de test.

- **Interpretabilidad**: Eliminando las características irrelevantes obtenemos modelos que se interpretan más fácilmente


Estas técnicas se denominan de **Selección de variables** (subset selection) y consisten en identificar el subconjunto de los p predictores para los que tenemos evidencia que realmente están relacionados con la respuesta para después ajustar un modelo lineal restringido a ese conjunto de predictores.


## Selección del mejor subconjunto

La estrategia general para resolver este problema sería  explorar todos los posibles modelos que se pueden crear con p variables. Puede resumirse de la siguiente manera:

1. Sea $\mathcal{M_0}$ el modelo nulo, sin predictores. Este modelo predice simplemente la media de las observaciones de la respuesta $y_i$

2. Para $k=1,2,\dots,p$:
    - Ajustamos los ${p \choose k}$ posibles modelos que usan exactamente k predictores
    - Escogemos el mejor de los ${p \choose k}$ modelos, y le llamamos  $\mathcal{M_k}$. Por mejor entendemos el tenga menor $RSS$ o mayor $R^2$. 
    
3. Por último seleccionaremos el mejor entre $\mathcal{M_0},\dots,\mathcal{M_p}$ mediante algún mecanismo que estime el error de test de cada uno de los modelos, como la validación cruzada. 

Veamos un ejemplo. Con los datos de *Credit Card*, exploramos los posibles modelos para predecir el balance.

```{r}
credit=read.csv("../Datasets/Credit.csv")[-1]
head(credit)
```


Tenemos un total de 10 variables predictoras, contando con las variables dummy creadas para modelar la etnicidad.
Para $k=1,2,\dots,10$ evaluamos todos los posibles modelos que pueden crearse con $k$ variables:

- k=1: $\mathcal{M}_{11}:Balance \sim Income$,  $\mathcal{M}_{12}:Balance \sim Rating$, ...
Elegimos entre $\mathcal{M}_{11}, \dots, \mathcal{M}_{12}$, $\dots$, $\mathcal{M}_{1p}$,  aquel con menor $RSS$ (o mayor $R^2$) en el conjunto de entrenamiento y le llamamos $\mathcal{M}_{1}$

- k=2: $\mathcal{M}_{21}:Balance \sim Income + Rating$,  $\mathcal{M}_{22}:Balance \sim Income + Cards$, ...
Elegimos entre $\mathcal{M}_{21}, \dots, \mathcal{M}_{22}$, $\dots$, $\mathcal{M}_{1\binom{p}{2}}$ aquel con menor $RSS$ (o mayor $R^2$) en el conjunto de entrenamiento y le llamamos $\mathcal{M}_{2}$

- \dots

- k=p: $\mathcal{M}_{p}:Balance \sim .$





Veamos gráficamente el $RSS$ y $R^2$ de todos los posibles modelos realizables con los datos y la linea roja muestra el menor para cada posible número de variables

```{r,cache=TRUE,fig.height=4.5,fig.width=11, echo=FALSE, fig.show='hold'}

x=model.matrix(Balance ~ .,data=credit) 
y=credit$Balance
p=ncol(x)-1
n=nrow(x)
rss=list()
r2=list()
#Null model
mod=lm(y~ 1)
rss[[1]] = sum((y-mod$fitted.values)^2)
r2[[1]] = summary(mod)$r.squared
for(k in 1:p){
  combs=combn(p,k)
  rss[[k+1]]=numeric()
  r2[[k+1]]=numeric()
  for(j in 1:ncol(combs)){
    df=data.frame(Balance=y,x[,combs[,j]+1])
    mod=lm(Balance~.,data=df)
    rss[[k+1]] = c(rss[[k+1]],sum((y-mod$fitted.values)^2))
    r2[[k+1]] = c(r2[[k+1]],summary(mod)$r.squared)
  }
}

par(mfrow=c(1,2))
minrss=NULL
plot(0,rss[[1]],cex=0.5,xlim=c(0,11),ylim=c(0,1e8),ylab="RSS",xlab="Número de variables")
for(k in 0:p){
  nm=length(rss[[k+1]])
  points(rep(k,nm),rss[[k+1]],cex=0.5)
  minrss=c(minrss,min(rss[[k+1]]))
}
lines(0:p,minrss,col=2)

maxr2=NULL
plot(0,r2[[1]],cex=0.5,xlim=c(0,11),ylim=c(0,1),ylab="R square",xlab="Número de variables")
for(k in 0:p){
  nm=length(r2[[k+1]])
  points(rep(k,nm),r2[[k+1]],cex=0.5)
  maxr2=c(maxr2,max(r2[[k+1]]))
}
lines(0:p,maxr2,col=2)
par(mfrow=c(1,1))

```

Ahora por último deberíamos evaluar los modelos $\mathcal{M_0},\dots,\mathcal{M_p}$ mediante validación cruzada para ver cual de ellos da mejor estimación del error de test. 

De todas formas, en el punto anterior ya nos encontramos un problema cuando el número de variables crece por encima de 20. El número de modelos a explorar es $2^p$, con 20 variables tendríamos que explorar ya más de un millón de modelos. 

Por otro lado, la enorme cantidad de modelos a explorar puede hacer tender el método hacia el sobre-aprendizaje. Con $2^p$ modelos donde elegir siempre habrá alguno que por casualidad ajuste mejor los datos de entrenamiento provocando errores mayores en datos de test.  

Para simplificar la exploración de modelos suelen usarse los métodos de selección paso a paso, que tienen dos versiones: 

- Selección hacía delante

- Selección hacía atrás

## Selección hacía delante 

En este método se empieza por el modelo nulo y se va añadiendo un predictor por turno

En cada paso se incorpora la variable que provoca una mayor mejora en la bondad del ajuste:


1. Sea $\mathcal{M_0}$ el modelo nulo, sin predictores. 

2. Para $k=1,2,\dots,p$:

    - Ajustamos los $p-k$ posibles modelos que incorporan un predictor a $\mathcal{M_{k-1}}$ 
    - Escogemos el mejor de los $p-k$ modelos, y le llamamos  $\mathcal{M_k}$. Por mejor entendemos el tenga menor $RSS$ o mayor $R^2$. 
    
3.  Por último seleccionaremos el mejor entre $\mathcal{M_0},\dots,\mathcal{M_p}$ mediante algún mecanismo que estime el error de test de cada uno de los modelos, como la validación cruzada.


La ventaja computacional es clara: en este método se ajustan $1 + p(p + 1)/2$ modelos frente a los $2^p$ modelos de la búsqueda exhaustiva

Por otro lado, este tipo de búsqueda jerárquica no garantiza la obtención del modelo óptimo. Aunque esto, en algún caso puede ser una ventaja en lugar de un problema debido al sobre-aprendizaje. 

Veamos un ejemplo de los modelos que se seleccionarían para  los datos de *credit card*.

Si comparamos la búsqueda completa con la búsqueda hacia delante, la única diferencia, en este ejemplo, está en los modelos con 4 variables. 

Para ello usaremos el paquete `leaps` y en concreto la función `regsubset`, cuyo argumento `method` puede tomar los argumentos "exhaustive", "forward" o  "backward".

La siguiente tabla muestra las variables seleccionadas para los modelos candidatos $\mathcal{M_0},\dots,\mathcal{M_p}$ mediante la busqueda exhaustiva y paso a paso hacía delante.

```{r,eval=TRUE, fig.height=4}
library(leaps)
regfit.full=regsubsets(Balance~., data=credit, nvmax=11, method = "exhaustive")
regfull.summary=summary(regfit.full)

regfit.fdw=regsubsets(Balance~., data=credit, nvmax=11, method = "forward")
regfdw.summary=summary(regfit.fdw)

varmat= regfull.summary$which[,-1]
varnames=dimnames(varmat)[[2]]
vars.full = apply(varmat,1,function(x){paste(varnames[x],collapse = ",")})
p=ncol(varmat)  
varmat= regfdw.summary$which[,-1]
varnames=dimnames(varmat)[[2]]
vars.fdw = apply(varmat,1,function(x){paste(varnames[x],collapse = ",")})

res=data.frame(k=1:p,Full=vars.full, Forward=vars.fdw)
kable(res[1:11,])

```

## Selección hacía atrás

La otro posibilidad de búsqueda paso a paso es la selección hacía atrás

1. Comenzamos por  $\mathcal{M_p}$ el modelo con todos  predictores. 

2. Para $k=p-1,p-2,\dots,1$:
    - Ajustamos los $k$ posibles modelos que eliminan un predictor al modelo del paso anterior $\mathcal{M_{k+1}}$ 
    - Escogemos el mejor de los $k$ modelos, y le llamamos  $\mathcal{M_k}$. Por mejor entendemos el tenga menor $RSS$ o mayor $R^2$. 
    
    

Comparemos, con el mismo ejemplo,  las variables seleccionadas **forward** y **backward**

```{r}
regfit.fdw=regsubsets(Balance~., data=credit, nvmax=11, method = "forward")
regfdw.summary=summary(regfit.fdw)

regfit.bkw=regsubsets(Balance~., data=credit, nvmax=11, method = "backward")
regbkw.summary=summary(regfit.bkw)

varmat= regbkw.summary$which[,-1]
varnames=dimnames(varmat)[[2]]
vars.bck = apply(varmat,1,function(x){paste(varnames[x],collapse = ",")})
p=ncol(varmat)  

varmat= regfdw.summary$which[,-1]
varnames=dimnames(varmat)[[2]]
vars.fdw = apply(varmat,1,function(x){paste(varnames[x],collapse = ",")})

res=data.frame(k=1:p,Backward=vars.bck, Forward=vars.fdw)
kable(res[1:11,])
```

## Elección del mejor modelo

El modelo que contiene todos los predictores siempre tendrá el $RSS$ más pequeño y el 
$R^2$ más grande, ya que estas cantidades están relacionadas con el error de entrenamiento.

Pero, lo que deseamos es elegir un modelo con error de test pequeño, no un modelo
con error de entrenamiento bajo. Por lo tanto , el $RSS$ y el  $R^2$ no son las métricas adecuadas para la selección de la mejor modelo entre una colección de modelos con diferente número de predictores .

Para la estimación del error de test habitualmente se usan dos estrategias:

- Podemos estimar indirectamente el error de test  haciendo un 
ajuste al error de entrenamiento para tener en cuenta el sesgo debido
al sobre-entrenamiento que puede provocar un número mayor de predictores. Para ello existen varios estimadores como el $C_p$, $AIC$, $BIC$  y el  $R_2$ ajustado. 


- O podemos estimar directamente el error de test, usando un enfoque de  conjunto de validación o de  validación cruzada, tal como hemos visto  anteriormente. Este es el enfoque que vamos a utilizar en este curso. Disponiendo de ordenadores potentes, no tiene sentido usar estimaciones aproximadas del error de test. Además  esos métodos solo son válidos para modelos lineales, en cambio la validación cruzada es válida para cualquier tipo de modelo. 


El objetivo, recordemos, es seleccionar entre los modelos $\mathcal{M_0},\dots, \mathcal{M_p}$ seleccionados por el método empleado para la selección de variables, sea la búsqueda completa, o paso a paso, hacia delante o hacía atrás, el modelo en el que el error de test estimado sea el mínimo.  

Calculamos el error en un conjunto de validación o mediante validación cruzada
para cada modelo $\mathcal{M_k}$ en cuestión, y luego
seleccionaremos el $k$ para el cual es el error de test estimado sea mínimo.


Veamos para el ejemplo de Credit Card el RMSE estimado por conjunto de validación y validación cruzada. Lo hacemos para los modelos seleccionados mediante la selección paso a paso hacia delante

```{r,results='hide'}

# Definimos una función predict para simplificar las cosas
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(eval(object$call[[2]]))
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  mat[,names(coefi)] %*% coefi
}

# k-fold cross validation

K=10
nv=11
folds=sample(rep(1:K,length=nrow(credit)))
#table(folds)
cv.errors=matrix(NA,K,nv)

for(k in 1:K){
  print(k)
  # Elegimos los modelos candidatos para cada fold de la validación cruzada
  best.fit=regsubsets(Balance~.,data=credit[folds!=k,],nvmax=nv,method="forward")
  for(i in 1:nv){
    print(i)
    pred=predict(best.fit,credit[folds==k,],id=i)
    cv.errors[k,i]=mean((credit$Balance[folds==k]-pred)^2)
  }
}
rmse.cv=sqrt(apply(cv.errors,2,mean))

plot(rmse.cv,pch=19,type="b")
imin=which.min(rmse.cv)
points(imin,rmse.cv[imin],pch=19,col=2)

```

Se observa que el mínimo error de test se produce para un modelo con 6 variables. Para elegirlas lo haríamos con el conjunto completo de datos y serían Income,Limit,Rating,Cards,Age,StudentYes.

# Qué has aprendido

En esta unidad hemos aprendido a ajustar modelos y realizar predicciones mediante modelos lineales. Aunque las dependencias entre variables en la realidad rara vez son lineales, hemos visto que estos modelos pueden ser muy eficaces y útiles.

Además muchos de los conceptos que hemos desarrollado en este tema, como: 

- medida de la precisión de los modelos
- determinación de la influencia de las distintas variables
- métodos para la determinación del error de tes
- selección de variables

son aplicables a modelos estadísticos más complejos. Ahora ya tienes los conocimientos necesarios para adentrarte por tu cuenta en el aprendizaje de métodos más sofisticados como:

- Modelos aditivos generalizados (GAM)
- Árboles de regresión
- Bosques aleatorios (Random Forests)
- Gradient boosting
- Redes Neuronales

# Autoevaluación

1.  Gráficamente la relación según un modelo lineal entre la variable respuesta $Y$ y las variables predictoras $X_1$,$X_2$ se representa por una

    a. recta
    b. parábola
    c. plano
    d. hiperplano

2.  El procedimiento habitual para estimar los coeficientes de un modelo lineal se denomina 

    a. Estimación por máxima verosimilitud
    b. Ajuste por componentes principales
    c. Optimización convexa
    d. Ajuste de mínimos cuadrados
    
3.  Si ejecutamos en R `lm(dist~speed, data=cars)`. ¿Cuál es la pendiente ($\beta_1$) estimada?

    a. $0.932$  
    b. $1.932$  
    c. $2.932$  
    d. $3.932$ 
    
4.  Si `mod <- lm(dist~speed, data=cars)`, ¿qué comando me proporciona el detalle de los coeficientes, errores y p-valores? 

    a. summary(mod)
    b. summarize(mod)
    c. coef(mod)
    d. predict(mod)
    
5.  Si entreno un modelo lineal sobre los datos `iris`con fórmula 
`Petal.Length ~ .` , ¿qué variables tienen un parámetro asociado distinto de cero con un grado de significación de 0.001? 

    a. Sepal.Length, Petal.Width
    b. Sepal.Length, Petal.Width, Speciesversicolor
    c. Sepal.Length, Petal.Width, Speciesversicolor,Speciesvirginica 
    d. Sepal.Length, Sepal.Width, Petal.Width, Speciesversicolor,Speciesvirginica 

6.  Cómo definirías un modelo lineal para los datos iris  para modelar la variable `Petal.Length` en función de  la variable `Species` y `Petal.Width` incluyendo un término de interacción entre ellas

    a. `lm(Petal.Length ~ Species + Petal.Width, data=iris)`
    b. `lm(Petal.Length ~ Species*Petal.Width, data=iris)`
    c. `lm(Petal.Length ~ Species - Petal.Width, data=iris)`
    d. `lm(Petal.Length ~ Species^Petal.Width, data=iris)`
    
7.  ¿Cuál se estos 4 modelos tiene menor error estándar de los residuos en el conjunto de entrenamiento? (Usa la función summary para averiguarlo)

    a. `lm(Petal.Length ~ Species + Petal.Width, data=iris)`
    b. `lm(Petal.Length ~ Species:Petal.Width, data=iris)`
    c. `lm(Petal.Length ~ Species*Petal.Width, data=iris)`
    d. `lm(Petal.Length ~ .-Species - Petal.Width, data=iris)`

8.  Para ajustar un modelo polinómico de orden 2 la fórmula apropiada es

    a. `y ~ x + I(x^2)`
    b. `y ~ x + x**2`
    c. `y ~ x + x^2` 
    d. Todas las anteriores son válidas

9.  ¿En cuántos grupos es recomendable dividir los datos en procedimiento de validación cruzada)

    a. 5 o 10
    b. Más de 30
    c. Un número par de grupos
    d. Un número impar de grupos
    
10.  Cuál de los siguientes comandos usarías para un proceso de selección de variables paso a paso hacia delante 

    a. `regsubsets(y ~ ., data=mydata, method = "backward")` 
    b. `regsubsets(y ~ ., data=mydata, method = "forward")` 
    c. `regsubsets(y ~ ., data=mydata, method = "forehand")` 
    d. `regsubsets(y ~ ., data=mydata, method = "exhaustive")` 
    


# Soluciones

1-c;2-d;3-d;4-a;5-c;6-b;7-c;8-a;9-a;10-b

# Bibliografía

- "An Introduction to Statistical Learning, with applications in R", G. James, D. Witten,  T. Hastie and R. Tibshirani (Springer, 2013) 

- R for Data Science, Garrett Grolemund, Hadley Wickham. O’Reilly (2016)
  Disponible online en  <http://r4ds.had.co.nz>
  
- Chester Ismay, Albert Y. Kim.  ModernDive: an Introduction to Statistical and Data Sciences via R (2017)
<https://ismayc.github.io/moderndiver-book/references.html>

- Dalgaard P. Introductory statistics with R (Springer, 2002)

## Otros créditos

Some of the figures in this presentation are taken from "An Introduction to Statistical Learning, with applications in R"  (Springer, 2013) with permission from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani 





---
title: "Regresión Lineal"
author: "MasterD"
output:
  html_document:
    highlight: tango
    theme: united
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
  pdf_document:
    toc: yes
  revealjs::revealjs_presentation:
    center: yes
    css: style.css
    highlight: pygments
    theme: sky
  word_document:
    reference_docx: www/plantillaMasterD_basica5.docx
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
options(width=100)

library(knitr)
library(reshape2)
library(dplyr)
library(ggplot2)
library(grid)
library(ElemStatLearn)
library(GGally)
library(png)
#library(igraph)
library(ISLR)
library(splines)
library(plot3D)
#library(FNN)
#library(MASS)
#library(class)

```

# Tu reto en esta unidad

Imagina que eres el director de marketing de una multinacional. Tienes datos de ventas en diferente países/regiones del mundo junto con el presupuesto empleado para publicidad en televisión, radio y prensa en cada uno de esos países/regiones. 

Quieres hacer un modelo que explique las ventas función del presupuesto gastado en publicidad en los diferentes medios. Estos son los datos de los que dispones. 


```{r, echo=FALSE, fig.height=4}

adv=read.csv("../Datasets/advertising .csv",sep=";")
a <- ggplot(adv) + geom_point(aes(TV,Sales))
b <- ggplot(adv) + geom_point(aes(Radio,Sales)) 
c <- ggplot(adv) + geom_point(aes(Newspaper,Sales)) 

grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 3)))
vplayout <- function(x, y)
  viewport(layout.pos.row = x, layout.pos.col = y)

print(a, vp = vplayout(1, 1))
print(b, vp = vplayout(1, 2))
print(c, vp = vplayout(1, 3))

```

Cada punto/observación representa a un determinado producto en un país/región del mundo. 

Algunas preguntas a las que  querrías que respondiera tu modelo son las siguientes:

- ¿Hay una relación entre el presupuesto en publicidad y las ventas del producto?
- ¿Cómo de fuerte es esa relación?
- ¿Qué medio contribuye más a las ventas?
- ¿Con qué precisión podemos predecir futuras ventas?
- ¿Es la relación entre ventas y publicidad lineal?
- ¿Hay sinergias entre los diferentes canales de publicidad?


A lo largo de la unidad, vamos a aprender un tipo de modelos bastante sencillo pero que serán capaces de responder satisfactoriamente a tus necesidades y te permitirán invertir el presupuesto de publicidad de forma más eficiente en los próximos países/regiones en los que se produzcan lanzamientos de producto. 






# Introducción

Esta unidad trata sobre la regresión lineal, uno de los métodos más simples de aprendizaje supervisado que usaremos predecir una respuesta de tipo cuantitativo.

A pesar de que los modelos lineales son sencillos en comparación con otros métodos de aprendizaje estadístico más modernos, la regresión lineal sigue siendo un método de aprendizaje estadístico muy útil y ampliamente utilizado. Además, sirve como punto de partida para entender los conceptos y buenas prácticas del aprendizaje estadístico. 
Más aún, muchos de los métodos más sofisticados que se usan en proyectos de ciencia de datos son generalizaciones o extensiones de los modelos lineales.

Por tanto, la regresión lineal es un punto de partida imprescindible para comenzar en el estudio de los métodos de aprendizaje estadístico.


La regresión lineal  asume que la relación entre la respuesta $Y$ y los predictores $(X_1,X_2,\dots,X_p)$ es lineal, es decir que la relación entre la variable respuesta y la variable predictora se puede representar mediante una línea recta, en dos dimensiones, un plano en 3 dimensiones o un hiperplano para 4 o más dimensiones. 

Aunque la función de regresión verdadera pocas veces es completamente lineal, en la práctica la aproximación lineal puede ser  extremadamente útil y práctica. 

Para ilustrar los conceptos fundamentales de los modelos lineales vamos a usar el conjunto de datos de *Advertising* que hemos introducido anteriormente. 


# Regresión lineal simple

## Modelo lineal

Asumimos que los datos están distribuidos según un modelo 

$$ Y = \beta_0 + \beta_1 X + \epsilon$$

donde $\beta_0 + \beta_1$ son dos constantes desconocidas, que representan la pendiente (slope)  y la ordenada en el origen (intercept) de la recta. También les llamaremos **coeficientes** o **parámetros**.

$\epsilon$ es el error irreducible y en principio asumimos que está distribuido normalmente con media 0 y una cierta varianza $\sigma$.  

## Estimación de parámetros

Supongamos que tenemos conjunto de datos $\{(x_1,y_1),(x_2,y_2),..,(x_n,y_n)\}$ y pretendemos estimar cuales son los valores de los coeficientes  $\hat{\beta}_0$ y $\hat{\beta}_1$ que hacen que el modelo lineal se ajuste mejor a nuestros datos.

**Autotexto**
Date cuenta de la notación empleada. Llamamos $\beta_0$ y $beta_1$ a los paramétros del modelo lineal que describe a la población. Por el contrario llamamos $\hat{\beta_0}$ y $\hat{\beta_1}$ a los parámetros estimados a partir de una muestra. Para cada muestra distinta que tengamos de una población, los valores estimados de los parámetros son distintos. 
**Fin Autotexto**

El procedimiento para estimar los coeficientes se denomina *ajuste por mínimos cuadrados* y consiste esencialmente en minimizar la suma de las distancias al cuadrado entre los $y_i$ reales y los $\hat{y_i}= \beta_0 + \beta_1 x_i$ estimados.


Sea $\hat{y}_i = \hat{\beta_0}+ \hat{\beta}_1 x_i$, una predicción de $Y$ basada en el valor de $x_i$ para un valor dado de los parámetros $\hat{\beta_0}$ y $\hat{\beta_1}$ 

Al error entre el valor real y la predicción $e_i= y_i - \hat{y_i}$  se le  denomina   **residuo** asociado a la i-esima observación.

Definamos la suma de los cuadrados de los residuos ($RSS$) como 

$$RSS= e_1^2 + e_2^2 + \dots + e_n^2$$

o equivalentemente como 

$$RSS= \sum_{i=1}^{n}(y_i - \hat{\beta_0}+ \hat{\beta}_1 x_i)^2 $$

El método de  mínimos cuadrados escoge los valores de $\hat{\beta_0}$ y $\hat{\beta_1}$ que minimizan $RSS$. Por tanto debe cumplirse que 

$$ \frac{\partial RSS}{\partial \beta_0}=0$$

$$ \frac{\partial RSS}{\partial \beta_1}=0$$

Resolviendo las ecuaciones anteriores, los valores que minimizan $RSS$ son  

$$
\hat{\beta_1} = \frac{\sum_{i=1}^{n}(y_i-\bar{y})(x_i-\bar{x})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}=
\frac{Cov(x,y)}{\sigma^2_{x}}

$$


$$
\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}
$$



Veamos un ejemplo, ajustamos un modelo lineal de las ventas frente al presupuesto de anuncios en `TV`.
Calculamos la estimación de los coeficientes del modelo lineal, usando las formulas anteriores y las comparamos con los resultados de la función de R que ajusta modelos lineales `lm`

```{r,echo=TRUE}
##Estimación manual

beta1=cov(adv$TV,adv$Sales)/var(adv$TV);beta1
beta0= mean(adv$Sales) - beta1*mean(adv$TV);beta0

##Estimación con lm

mod=lm(formula=Sales~TV,data=adv)
summ=summary(mod);summ
```

Veamos gráficamente, la recta ajustada y los residuos

```{r,echo=FALSE}
plot(adv$TV,adv$Sales,col=2,pch=19,cex=0.8,xlab="TV",ylab="Sales")
abline(beta0,beta1,lwd=2)
segments(adv$TV,adv$Sales,adv$TV,mod$fitted.values,col=4)
```

Conozcamos con más detalle la función `lm` de R, cuyo cometido es ajustar modelos lineales. 
La sintaxis básica de esta función es `lm(formula, data)` donde 

- `formula`: Es una descripción simbólica del modelo. Casi todas las funciones de R que ajustan modelos estadísticos usan la sintaxis de fórmulas para definir los modelos. En el ejemplo anterior hemos ejecutado
`lm(formula=Sales~TV,data=adv)`. La fórmula `Sales ~ TV` indica que queremos modelar las ventas `Sales` en función del presupuesto gastado en televisión `TV`.
Si quisiéramos definir un modelo con más variables predictoras escribiríamos `Sales ~ TV + Radio + Newspaper` o también `Sales ~ .` para indicar el resto de las variables disponibles en nuestros datos.
También es posible seleccionar todas menos algunas de ellas con el signo -, 
por ejemplo `y~.-x1` para usar todas las variables menos `x1`.

-`data` es un data frame que contenga las columnas referenciadas en la fórmula.  


Hay muchos más argumentos opcionales que se pueden pasar a la función `lm`, puedes consultarlos mediante la ayuda de R `?lm`.


## Precisión en la estimación de los parámetros

Normalmente, la  estimación de los parámetros la realizamos a partir de una muestra de datos de una cierta población. Y para cada muestra distinta tendremos una estimación distinta de los parámetros. 

Para estimar la dispersión en la estimación de los parámetros en todas las posibles muestras de valores $\{(x_i,y_i)\}$ de tamaño n, procedemos de la misma manera que hicimos en la unidad 5 en los cálculos de intervalos de confianza. 

Dada una muestra de datos $\{(x_i,y_i)\}$ podríamos realizar un proceso de obtención de muestras de bootstrap y estimar la distribución de los parámetros estimados. En este caso, vamos a fiarnos de los métodos tradicionales de estimación de estas dispersiones, y dejaremos que R las calcule por nosotros.

Para mostrar algunas propiedades de la incertidumbre en la determinación de los coeficientes, partiremos de un modelo lineal conocido (esto nunca pasará en la realidad). Generaremos diferentes muestras de valores $\{(x_i,y_i)\}$ de tamaño finito, estimamos los coeficientes para cada muestra y visualizamos las distribuciones. 

Supongamos una población de $Y$ y $X$ que responde a un modelo $Y=2 + 3X + \epsilon$ 
donde $\epsilon=\mathcal{N}(0,0.5)$ (Normal de media 0 y $\sigma=0.5$).

Simulemos un número elevado de muestras de tamaño 10 y veamos la distribución de los parámetros estimados 

Veamos primero las rectas ajustadas para 10 muestras de tamaño 10

```{r}
N=100
ng=10
a=2
b=3
s=0.5

res<- data.frame(samp=factor(1:10)) %>% group_by(samp) %>% do({
  x=runif(ng,0,1)
  y=a + b*x + rnorm(ng,0,s)
  data.frame(x,y)
})

ggplot(res) + geom_point(aes(x,y,color=samp)) + geom_smooth(aes(x,y,color=samp),method="lm",se=FALSE) +
  geom_abline(aes(slope=3,intercept=2))
```

Para cada muestra se obtiene una recta distinta caracterizada por un valor distinto de los parámetros estimados $\hat{\beta_0}$ y $\hat{\beta_1}$. 

Ahora simulemos 10000 muestras y estudiemos las distribuciones de los parámetros estimados. 



```{r,echo=TRUE,cache=TRUE,fig.height=4.8}
nsim=10000
N=10
a=2
b=3
s=0.5
res=data.frame()
for(i in 1:nsim){
  x=runif(N)
  y=a + b*x + rnorm(N,0,s)
  mod=lm(y~x)
  sum=summary(mod)
  ae=mod$coefficients[1]
  be=mod$coefficients[2]
  # asd=sum$coefficients[1,2]
  # bsd=sum$coefficients[2,2]
  res <- rbind(res,data.frame(a,b,ae,be))
}
asd=sd(res$ae)
bsd=sd(res$be)
par(mfrow=c(1,2))
hist(res$ae,probability = TRUE,main="Histograma beta0",xlab="beta0")
curve(dnorm(x,a,asd),1,5,col=2,add=TRUE)
hist(res$be,probability = TRUE,main="Histograma beta1",xlab="beta1")
curve(dnorm(x,b,bsd),1,5,col=2,add=TRUE)
par(mfrow=c(1,1))
```


Como se observa en la figura, la distribución de parámetros estimados es Normal. Como vamos a ver ahora la varianza de estas distribuciones depende de la $\sigma$ del $\epsilon$ (error irreducible), del tamaño de la muestra y de la dispersión de la variable  $x$

El error estándar para la estimación de los parámetros de regresión lineal se puede calcular de forma exacta y viene dado por las siguientes fórmulas:

$$SE(\hat{\beta_1})^2=\frac{\sigma^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2} \; , \;
SE(\hat{\beta_0})^2 = \sigma^2\left( \frac{1}{n} +  \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2} \right)$$

donde $\sigma^2=Var(\epsilon)$

Normalmente no conocemos  $\sigma$ pero la estimaremos a partir de la muestra mediante el error estándar de los residuos ($RSE$)

$$ RSE = \sqrt{\frac{RSS}{n-2}}= \sqrt{\frac{\sum_{i=1}^{n}(y_i - \hat{\beta_0}+ \hat{\beta}_1 x_i)^2 }{n-2}}$$

Al igual que para los valores de los parámetros, los valores de los errores estándar pueden calcularse a partir de los valores de las observaciones $(x_i,y_i)$. De todas formas, no te preocupes si estas fórmulas te parecen complicadas ya que la función `lm` de R calcula por ti el error estándar de los parámetros. 

En el ejemplo utilizado anteriormente para `Sales ~ TV` la estimación de coeficientes y sus errores estádar serían: 

```{r, results='as.is'}
mod=lm(formula=Sales~TV,data=adv)
# Para obteter los coeficientes debemos llamar a la función summary del modelo
summ=summary(mod)
coef=coef(summ)[,1:2]
knitr::kable(coef)
```


Una vez conocido el error estándar,  si el tamaño de la muestra es suficientemente grande, podemos calcular el intervalo de confianza al 95 % para el valor de los coeficientes como 

$$(\hat{\beta_1} - 2 SE(\hat{\beta_1})\, , \,\hat{\beta_1} + 2 SE(\hat{\beta_1}))$$

En general, como la desviación típica de los parámetros los estimamos a partir de valores muestrales, lo que ocurre es que la variable $\frac{\hat{\beta_1} - \beta_1}{SE(\hat{\beta_1})}$ sigue una distribución t de Student con $n-2$ grados de libertad. Por tanto el intervalo de confianza lo calcularemos como  

$$(\hat{\beta_1} - f_c SE(\hat{\beta_1})\, , \,\hat{\beta_1} + f_c SE(\hat{\beta_1}))$$

donde $f_c=qt(0.975,df=n-2)$ es el factor de cobertura análogo al que vimos en la unidad 5.

Se calcula normalmente el intervalo de confianza asociado al parámetro $\beta_1$, la pendiente de la recta, ya que un valor de este que contenga al 0, querrá decir que la relación lineal entre $Y$ y $X$ no estadísticamente significativa. 

Recordemos lo que significa un intervalo de confianza, que es que existe un 95% de probabilidad de que la estimación del intervalo para una muestra dada contenga al parámetro $\beta$ real. 
Simulemos un número elevado de muestras del ejemplo anterior y veamos en cuantas ellas el intervalo de confianza estimado contiene al $\beta_1$ real que es 3.

```{r,echo=FALSE,cache=TRUE,fig.height=4.5}
set.seed(5234)
nsim=5000
N=10 #Tamaño de las muestras
a=2
b=3
s=0.5

res=data.frame()
for(i in 1:nsim){
  x=runif(N)
  y=a + b*x + rnorm(N,0,s)
  mod=lm(y~x)
  sum=summary(mod)
  ae=mod$coefficients[1]
  be=mod$coefficients[2]
  asd=sum$coefficients[1,2]
  bsd=sum$coefficients[2,2]
  res <- rbind(res,data.frame(a,b,ae,be,asd,bsd))
}
# factor de cobertura
f=qt(0.975,N-2)
# Calculo si los intervalos de confianza contienen a b
# Intervalo de confianza para beta 1 be +/- f*bsd
res <- res %>% mutate(out=ifelse(be-f*bsd >b | be+f*bsd <b,TRUE,FALSE))
```

```{r}
ggplot(res %>% sample_n(100)) + geom_linerange(aes(x=1:100,ymin=be-f*bsd,ymax=be+f*bsd,color=out)) + 
  geom_hline(aes(yintercept=b),color="black")

```

El número de intervalos de confianza que no contienen $\beta=$ `r b`  es 
`r sum(res$out)` de `r nsim` simulaciones  (`r round(100*mean(res$out),3)` %).


## Test de Hipótesis

A partir de los errores estándar también podemos hacer contrastes de hipótesis, para determinar si los coeficientes son significativamente distintos de 0.

Típicamente lo que queremos saber es si la relación entre la respuesta $Y$ y los inputs $X$ detectada por el modelo lineal es real o ha podido ser fruto de la casualidad debido a la muestra. 

El test hipótesis más común establece como hipótesis nula:

- $H_0$: No hay relación entre $Y$ y $X$, frente a la hipótesis alternativa  
- $H_A$: Existe alguna relación entre $Y$ y $X$

De forma matemática, esto equivale a: 

- $H_0$: $\beta_1=0$  
- $H_A$: $\beta_1 \neq 0$


En primer lugar calculamos el t-valor asociado

$$t=\frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})}$$

Si la hipótesis nula es cierta $t$ sigue una distribución t de Student con $n-2$ grados de libertad

El p-valor lo calcularemos como la probabilidad de obtener un valor de $|t|$ mayor que el medido en la muestra, suponiendo cierta $H_0$ 

`p-valor = 2 * (1 - pt(abs(t),df=n-2))`

Veamos de nuevo un ejemplo en R, usando  los datos de *Advertising*. Ajustamos el modelo `Sales~TV`.

```{r}
mod=lm(Sales~TV,data=adv)
# La función summary muestra los resultados principales del ajuste, incluyendo los parámetros, 
# errores estandar, t valores y p valores
summary(mod)
```

En la tabla se muestra la estimación de los coeficientes, el error estándar, el estadístico t asociado y el p-valor considerando como hipótesis nula $H_0: \beta_i=0$. En este caso $\beta_1$ vale $0.047$ con un error estándar de $0.0026$. Se ve ya a simple vista que $0.047 \pm 2 \times 0.0026$ no contiene al 0. De todas formas el cálculo preciso del p-valor nos dice que la probabilidad de encontrar un valor tan extremo o más que $0.047$ dadas las características de la muestra y supuesta cierta la hipótesis nula es menor de $2 10^{-16}$. 

Para hacer más visible la significación de las estimaciones de los parámetros R añade tres asteriscos `***` al lado del p-valor si este es menor de $0.001$, añade `**` si es menor de $0.01$ y `*`si es menor de $0.05$

Es posible guardar en un data frame la tabla de estimaciones de la siguiente manera

```{r}
mod=lm(Sales~TV,data=adv)
summ=summary(mod)
coefs= summ$coefficients
knitr::kable(coefs)
```


## Determinación de la precisión global del modelo

En primer lugar puede medirse  el **error estándar de los residuos**

$$ RSE = \sqrt{\frac{RSS}{n-2}}= \sqrt{\frac{\sum_{i=1}^{n}(y_i - \hat{\beta_0}+ \hat{\beta}_1 x_i)^2 }{n-2}}$$

Cuanto más pequeño sea el $RSE$ mejor será el ajuste. El problema de esta medida es que depende de la escala de la variable $Y$, y una escala puede depender de cosas arbitrarias como las unidades elegidas para medir una magnitud, por tanto es un valor que cambia de un caso a otro y existen unos valores de referencia para saber a partir del valor numérico de $RSE$ como de bueno es un ajuste lineal

**Autotexto**
Fíjate que dividimos el error estándar de los residuos por $n-2$ en lugar de por $n-1$ como haciamos con la varianza. Esto es debido a razones similares a las que argumentamos en el caso de la varianza muestral, de por qué debiamos dividir por $n-1$ y no por $n$.
En este caso el $RSE$ pretende estimar el error irreducible $\sigma$ de modelo. Como se usan dos estimaciones muestrales $\hat{\beta_0}$ y $\hat{\beta_1}$ en lugar de los parámetros verdaderos $\beta_1$ y $\beta_2$, que desconocemos, para que $RSE$ sea un estimador no sesgado debemos dividir por $n-2$ 
**Fin Autotexto**

### $R^2$

El parámetro $R^2$ es una buena alternativa para medir la precisión del ajuste ya que se define como una proporción

$$R^2=\frac{TSS-RSS}{TSS}= 1- \frac{RSS}{TSS}$$

donde $TSS=\sum_i(y_i-\bar{y})^2$ y $RSS=\sum_i(y_i-\hat{y_i})^2$. 

$RSS$ está relacionada con  la varianza de los residuos, es decir la media de las desviaciones cuadráticas de los valores de las respuestas $y_i$ frente a los valores ajustados $\hat{y_i}= \beta_0 + \beta_1 x_i$. 
Mientras que $TSS$ está relacionada con la varianza de la respuesta, es decir la media de las desviaciones cuadráticas de los valores de las respuestas $y_i$ frente a su media.

Por tanto $R^2$ puede interpretarse como la fracción de la varianza total de $Y$ explicada por el modelo.

Para el caso del modelo lineal $R^2=r^2$, donde 

$$ r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}
  { \sqrt{\sum_{i=1}^{n} ( x_i - \bar{x})^2} \sqrt{\sum_{i=1}^{n} ( y_i - \bar{y})^2} }$$

es el coeficiente de correlación lineal.

El $R^2$ es útil para determinación de la precisión de otro tipo de modelos, pero en ese caso $R^2 \neq r^2$. Debe calcularse según la definición original. 

$R^2$ toma valores siempre entre 0 y 1. Será uno cuando la relación entre $Y$ y $X$ sea perfectamente lineal, con error irreducible 0. En el otro extremo será 0 cuando no hay ninguna relación entre $Y$ y $X$. 


### Error de predicción 

Una vez estimados los parámetros del modelo, para un valor nuevo de $X=x$ podemos estimar la respuesta como:

$$ \hat{y} = \hat{\beta_0} + \hat{\beta_1} x $$


Pero ya hemos visto que existe una incertidumbre en la determinación de los parámetros, además de que el modelo tiene implícito un término de error irreducible. Por tanto esta estimación tiene una incertidumbre. La buena noticia es que esta incertidumbre la podemos estimar. 

Debemos distinguir entre lo que es el error en la estimación (error reducible) y el error en la predicción (error reducible más error irreducible). 

- Los errores estándar que hemos visto para la estimación de los coeficientes tienen que ver con el **error reducible**. A partir de ellos puedo calcular un intervalo de confianza para determinar  como de cerca está $\hat{y}$ de $f(X)=E(Y|X)$. 

- Además, en la práctica asumir un modelo lineal para $f(X)$ es una aproximación de la realidad. Está es otra fuente potencial de error que se denomina *sesgo (bias)*. De momento nos olvidaremos de esta fuente de error. 

- Pero además, aunque el modelo lineal fuera perfecto y conociéramos los coeficientes verdaderos, aun nos queda una fuente de error: el error irreducible $\epsilon$, es decir lo que varía $Y$ respecto de $\hat{Y}$. Para dar un intervalo de confianza a las predicciones debemos tener en cuenta los errores estándar de los parámetros y una estimación del error irreducible.


De forma concreta, podemos estimar el  **Intervalo de confianza** para la estimación ($E(Y|X=x)$) como 

$$\hat{y} \pm  f_c \sigma \sqrt{\frac{1}{n} +\frac{(x-\bar{x})^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2}} $$

Y un intervalo de confianza para la predicción como 

$$\hat{y} \pm  f_c \sigma \sqrt{1 + \frac{1}{n} +\frac{(x-\bar{x})^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2}} $$

donde $\sigma$ lo estimamos mediante el $RSE= \sqrt{\frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{n-2}}$ y  $f_c= qt(0.975,df=n-2)$ es el factor de cobertura que para muestras grandes vales aproximadamente 2. 

Veamos de nuevo un ejemplo basado en el modelo `Sales~TV` en el conjunto de datos `adv`. En la gráfica se muestran con lineas azules los  intervalos de confianza para la estimación de $E(Y|X=x)$ y en gris los intervalos de confianza para las predicciones. 



```{r}
mod=lm(Sales~TV,data=adv)
summ=summary(mod)
rse=summ$sigma
f=qt(0.975,nrow(adv)-2)
xx=seq(0,300,1)
pred1=predict(mod,newdata = data.frame(TV=xx),se.fit = TRUE,interval = "confidence")
pred2=predict(mod,newdata = data.frame(TV=xx),se.fit = TRUE,interval = "prediction")
pred.df=data.frame(x=xx,pred2$fit,se.conf=pred1$se.fit)

ggplot(adv) + geom_point(aes(TV,Sales),color="orange") + geom_line(aes(x,fit),data=pred.df,color="black") + 
  geom_line(aes(x,fit-f*se.conf),data=pred.df,color="blue2") + 
  geom_line(aes(x,fit+f*se.conf),data=pred.df,color="blue2") + 
  geom_line(aes(x,lwr),data=pred.df,color="grey") + 
  geom_line(aes(x,upr),data=pred.df,color="grey")

```

Para hacer predicciones en R usando un modelo entrenado usaremos la función genérica `predict`.
Cada tipo de modelo de aprendizaje estadístico en R, tiene una función `predict` asociada. 
Observa las llamadas a `predict` en el ejemplo anterior.

```{r, eval=FALSE}
pred1=predict(mod,newdata = data.frame(TV=xx),se.fit = TRUE,interval = "confidence")
pred2=predict(mod,newdata = data.frame(TV=xx),se.fit = TRUE,interval = "prediction")
```

Se pasan como argumentos:

- `mod`: el modelo ajustado con `lm`. 
- `newdata`: un data frame conteniendo por columnas los datos (nuevos) de las variables predictoras. En este caso `newdata`  solo tiene una columna llamada `TV`, con los valores sobre los cuales queremos hacer las predicciones. Si no se proporciona un argumento `newdata`, `predict` devuelve las predicciones sobre el conjunto de entrenamiento. 
- `se.fit`: TRUE/FALSE si queremos estimación de error estándar de las predicciones. Argumento opcional. 
- `interval`: intervalo de estimación (`confidence`) o de predicción (`prediction`). Argumento opcional. 



Para terminar la sección, veamos un ejemplo de ajuste significativo pero poco útil. Cuando tenemos un número elevado de datos, es posible encontrar casos en los que el ajuste obtiene un coeficiente con un p-valor muy bajo pero sin embargo no tiene ninguna utilidad predictiva, ya que el error irreducible $\epsilon$ es bastante mayor que la variabilidad de las predicciones $\hat{y_i}$en todo el rango de $X$ observadas. Por ejemplo 

```{r,echo=TRUE}
N=1000
x=runif(N,0,1);y= 0.1*x + rnorm(N,0,0.4)
df=data.frame(x,y)
mod = lm(y~x,data=df)
summary(mod)
```


```{r}
plot(x,y,cex=0.5)
xx=seq(0,1,.01)
pred=predict(mod,newdata = data.frame(x=xx),interval = "confidence")
lines(xx,pred[,1],col=2)
lines(xx,pred[,2],col=2)
lines(xx,pred[,3],col=2)
```




# Regresión lineal múltiple

Hasta ahora hemos estudiado la regresión lineal simple,  un método para predecir una respuesta en base a una sola variable predictora. Sin embargo, en la práctica a menudo tenemos más de un predictor. Por ejemplo, en los datos de publicidad, hemos examinado la relación entre las ventas y la publicidad televisiva, pero también tenemos datos de la cantidad de dinero gastado en  publicidad en radio y en prensa. Y nos gustaría saber si las ventas están relacionadas con el presupuesto de publicidad para  estos dos medios. ¿Cómo podemos ampliar nuestro análisis para acomodar estos dos predictores adicionales?

Un modelo lineal con $p$ variables predictoras se expresa como: 

$$Y= \beta_0 +  \beta_2 X_1 +  \beta_2 X_2 + \dots +  \beta_p X_p + \epsilon$$

El objetivo es el mismo que en el caso simple, determinar los valores de los parámetros $\hat{\beta_j}$ que minimizan la suma de cuadrados de los residuos. Una vez estimados los parámetros podemos hacer predicciones con 

$$\hat{y}= \hat{\beta_0} +  \hat{\beta_2} x_1 +  \hat{\beta_2} x_2 + \dots +  \hat{\beta_p} x_p$$

Para derivar los resultados hay usar algo de álgebra matricial. Construyamos la matriz $X$ con las observaciones de cada input en columnas, con 1's en la primera columna.

$$
  \mathbf{X}  = \;
   \begin{pmatrix}
      1 & x_{11} &  .. & x_{1p}\\
      1 & x_{21} &  .. & x_{2p}\\
      ..         &  .. & ..    \\
      1 & x_{n1} & ..  & x_{np}    
  \end{pmatrix}
$$

Sea $\mathbf{\beta}$ el vector de parámetros

$$
\mathbf{\beta} = \;
   \begin{pmatrix}
   \beta_0 \\
   \beta_1 \\
    ..     \\
   \beta_p
   \end{pmatrix}
$$


La suma de los cuadrados de los residuos es

$$RSS= \sum_{i=1}^{n}(y_i - \hat{\beta_0} + \hat{\beta}_1 x_1 + \dots + \hat{\beta}_p x_p)^2 $$

que puede escribirse en forma matricial como 

$$RSS = (\mathbf{y}-\mathbf{X}\beta)^T (\mathbf{y}-\mathbf{X}\beta)$$

La condición de mínimo puede escribirse de forma vectorial

$$ \frac{\partial RSS}{\partial \beta}= −2 \mathbf{X}^T (\mathbf{y}-\mathbf{X}\beta) $$

Y el valor de los parámetros $\beta$ que minimizan los RSS viene dado por

$$\hat{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} $$.

El operador $^T$ significa matriz traspuesta, es decir una matriz que respecto a la original tiene intercambiadas las filas por las columnas. Los productos que se indican siguen la regla del producto de matrices, es decir $C=AB$ donde el elemento de la fila $i$ y columna $j$ de $C$ viene dado por

$$c_{ij} = \sum_{k=1}^{k=m} a_{ik} b_{kj}$$


De nuevo no te preocupes si tienes poca experiencia con el álgebra matricial y estas fórmulas te resultan difíciles de interpretar. En este curso, y en general en la experiencia del científico de datos, los ordenadores hacen los cálculos por ti. Aunque ello no quita que sea recomendable conocer la teoría que hay detrás de los métodos. 



## Precisión de los coeficientes

Los errores típicos en la estimación de los coeficientes vienen dados por

$$ Var(\hat{\beta}) = (\mathbf{X}^T \mathbf{X}) \sigma^2  $$

donde $\sigma$ lo estimaremos, igualmente,  a partir de error estándar de los residuos 

$$ \hat{\sigma^2} = RSE = \frac{1}{n-p-1} \sum_i (y_i - \hat{y_i})^2$$

Podemos realizar contrastes de hipótesis de la misma manera que en el caso de la regresión simple para cada uno de los coeficientes estimados. 


Veamos un ejemplo, con los datos de publicidad pero ahora utilizando todas las variables predictoras disponibles.

Ajustemos ahora un modelo: 
$Sales = \beta_0 + \beta_1 TV + \beta_2 Radio +\beta_3 Newspaper$

```{r}
mod=lm(Sales ~ TV + Radio + Newspaper, data=adv)
summary(mod)
```

## Interpretación de los coeficientes

El coeficiente $\beta_j$ puede interpretarse como el efecto promedio que tiene en $Y$ incrementar en una unidad $X_j$.

Esta interpretación es correcta cuando los predictores $X_j$ no están correlacionados. En este caso también cada coeficiente puede estimarse y contrastarse de forma individual

Sin embargo si los predictores están correlacionados:

- La varianza de todos los coeficientes tiende a aumentar, a veces
dramáticamente

- Las interpretaciones pueden ser  peligrosas. Hay que tener en cuenta que cuando $X_j$ cambia,
todo lo demás cambia.

Veamos en nuestro ejemplo la estimación de coeficientes del modelo con sus errores estándar y la correlación entre las variables predictoras:

```{r,echo=FALSE,results='as.is'}
library(knitr)
mod=lm(Sales ~ TV + Radio + Newspaper, data=adv)
summ=summary(mod)
kable(summ$coefficients,digits = 3)
```


Correlaciones:


```{r, echo=FALSE,results='as.is'}
kable(round(cor(adv),2))
```

Se observa que la mayor correlación entre las ventas y las variables predictoras se da con la variable `TV`y esto se refleja en la estimación de coeficientes del modelo. Sin embargo tanto `Radio` como `Newspaper` también presentan correlación con las ventas y están correlacionadas entre si con un coeficiente de $0.35$. Está correlación es la responsable de que la estimación del coeficiente para `Newspaper` sea compatible con 0. Toda la dependencia queda absorvida en el coeficiente estimado para `Radio`.



## Algunas Preguntas Importantes


1. ¿Es al menos uno de los predictores $X_1, X_2,\dots,X_p$ útil para predecir el valor de $Y$?

2. ¿Ayudan todos los predictores a  explicar $Y$, o solo es útil un subconjunto de ellos?

3. ¿Cómo de bien se ajusta  el modelo a los datos?


### ¿Es al menos un predictor útil?


En este caso debemos hacer un contraste de hipótesis donde la hipótesis nula es: 

$$ H_0: \beta_1=\beta_2=\dots=\beta_p=0$$
La respuesta nos la da el estadístico 

$$ F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}  \sim F_{p,n-p-1}$$

que sigue un distribución F de Snedecor con $p$ y $n-p-1$ grados de libertad. 

De nuevo no te preocupes demasiado por cómo es esta distribución, si tienes curiosidad puedes consultar algún libro de Estadística de las referencias de las unidades 4 y 5 y leer la secciones relacionadas con el análisis de varianza o test ANOVA. 

De todas formas, lo único que nos interesa es que el estadístico F lo calculamos a partir de los datos de la muestra con la formula anterior y que supuesta la hipótesis nula sigue una distribución F con $p$ y $n-p-1$ grados de libertad. Dejaremos que R, calcule el p-valor asociado a ese valor y nos diga como es de probable supuesta cierta la hipótesis nula encontrar un valor tan o más extremo que el encontrado en nuestra muestra. 

```{r,echo=FALSE}
mod=lm(Sales ~ ., data=adv)
summ=summary(mod)
summ
```

Podemos ver en la última linea que el valor de $F$ calculado en nuestra muestra vale `r summ$fstatistic[1]` y el p-valor asociado es `r 2*(1-pf(summ$fstatistic[1],df1=1,df2=198))`. Estos valores pueden calcularse, si lo necesitáramos usando las funciones de R relativas a la distribución F.

```{r}
# Estadístico F
summ$fstatistic
# p-valor
2*(1-pf(summ$fstatistic[1],df1=summ$fstatistic[2],df2=summ$fstatistic[3]))
```


También puede mirarse a los p-valores asociados a los coeficientes. Pero con muchas variables, es normal que aunque todos los coeficientes del *modelo real* sean nulos que para nuestra muestra de datos algún coeficiente salga no nulo con p-valor <0.05 por puro azar. 



### ¿Como de bueno es el ajuste del modelo?

Al igual que en la regresión simple podemos seguir utilizando como métricas de precisión del ajuste a: 

- Error estándar de los residuos $RSE = \sqrt{\frac{RSS}{n-p-1}}= \sqrt{\frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{n-p-1}}$

- $R^2=\frac{TSS-RSS}{TSS}= 1- \frac{RSS}{TSS}$  donde $TSS=\sum_i(y_i-\bar{y})^2$ y $RSS=\sum_i(y_i-\hat{y_i})^2$


En el caso multi-lineal $R^2$ coincide con la correlación lineal entre las $y_i$ observadas y las $\hat{y_i}$ ajustadas.  

De todas formas, al aumentar el número de variables aumenta la posibilidad de tener un sobre-entrenamiento en nuestros datos de entrenamiento. Como veremos más adelante, es posible estimar el efecto en error de test introduciendo  efectos asociados al tamaño del modelo para evitar un sobre-entrenamiento indeseable para nuestros modelos.

### ¿Cuales son las variables importantes?

Ya hemos visto que en la regresión multi-lineal, fijarse únicamente en los p-valores de los coeficientes individuales puede llevar a confusiones. 

La alternativa más directa, es evaluar todos los posibles modelos que pueden formarse con las p variables y elegir la mejor combinación usando algún criterio que corrija el error de entrenamiento con el tamaño (número de variables elegidas) del modelo.

Sin embargo, está alternativa cuando el número de variables es grande es inviable. El número de modelos a evaluar crece con $2^p$. Para $p=40$ tenemos ¡más de 1 billón ($10^{12}$) de posibles modelos! 

Por tanto tenemos que idear una alternativa que busque sobre una selección de todos los posibles modelos. En la última sección de esta unidad  discutiremos con mayor profundidad los métodos y criterios  para la elección de un subconjunto óptimo de variables para nuestro modelo.



# Predictores Cualitativos


Hasta ahora hemos tratado con variables predictoras continuas, pero es posible trabajar también con predictores cualitativos, es decir que toman solo un conjunto discreto de valores, sin ningún orden definido.

Veamos por ejemplo los datos  de *credit card* en la siguiente figura.

```{r,echo=FALSE,cache=TRUE}
credit=read.csv("../Datasets/Credit.csv")
credit=credit[-1]
pairs(credit)
```


Además de las variables cuantitativas, hay cuatro variables cualitativas : `Gender` (Male/Female) , `Student`(Yes/No), `Married`( Yes/No) , y `Ethnicity` (Caucásicos , afroamericanos (AA) o asiático).



Si queremos predecir el balance de la tarjeta de crédito en función de si la persona considerada es estudiante o no, debemos definir una nueva variable:

$$ 
x_i = 
\begin{cases}
1 \; \text{si la persona i-esima es estudiante} \\
0 \; \text{si la persona i-esima no es estudiante} \\
\end{cases}
$$

quedando el modelo lineal en 

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i = 
\begin{cases}
\beta_0 + \beta_1  + \epsilon_i &  \text{si la persona i-esima es estudiante} \\
\beta_0  + \epsilon_i           &  \text{si la persona i-esima no es estudiante} \\
\end{cases}
$$


Veamos cómo serían los coeficientes de un modelo lineal cuya variable respuesta es el balance de la tarjeta de crédito (es decir, el gasto mensual) frente a la variable binaria estudiante. En R definimos el modelo como si se tratase de una variable cuantitativa, no tenemos que preocuparnos de hacer la variable binaria. 

```{r,results='as.is'}
res = summary(lm(Balance ~ Student,data=credit)) 
kable(res$coefficients,digits=3)
```

Los coeficientes estimados tienen una interpretación directa:

- El balance estimado para un estudiante es de 396 \$ mayor que un no estudiante.
- El balance estimado para un no estudiante es de 480 \$. `StudentNo` es el nivel base que queda englobado en el `Intercept` .

Con más de dos niveles ($m$) debemos crear $m-1$ variables *dummy*. En el caso de la etnia hay 3 valores posibles: asiática, caucásica, afro-americana . Debemos crear 2 variables nuevas 

$$ 
x_{1i} = 
\begin{cases}
1 \; \text{si la persona i-esima es asiática} \\
0 \; \text{si la persona i-esima no es asiática} \\
\end{cases}
$$

$$ 
x_{2i} = 
\begin{cases}
1 \; \text{si la persona i-esima es caucásica} \\
0 \; \text{si la persona i-esima no es caucásica} \\
\end{cases}
$$

Y ambas variables forman parte del modelo lineal

$$
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i = 
\begin{cases}
\beta_0 + \beta_1  + \epsilon_i &  \text{si la persona i-esima es asiática} \\
\beta_0 + \beta_2  + \epsilon_i &  \text{si la persona i-esima es caucásica} \\
\beta_0  + \epsilon_i           &  \text{si la persona i-esima es afro-americana} \\
\end{cases}
$$

Observemos, que se define siempre una variable *dummy* menos que el número de niveles. El nivel sin variable dummy, en este caso los afro-americanos, recibe el nombre de *baseline*.

Veamos los resultados del ajuste usando la variable `Ethnicity`. Para el modelo, de nuevo, solo tenemos que decir que `Etnicity` es un predictor y ya se encarga el de crear las variables *dummy* asociadas. 


```{r,results='as.is'}
res = summary(lm(Balance ~ Ethnicity,data=credit)) 
kable(res$coefficients,digits=3)
```

# Extensiones del modelo lineal

## Interacción entre variables

En nuestro análisis anterior  de los datos de publicidad, hemos supuesto que el efecto en las ventas de aumentar una unidad el gasto en un  medio publicitario es independiente de la cantidad gastada en los otros medios de comunicación.

Por ejemplo, el modelo lineal para las ventas

$$Sales = \beta_0 + \beta_1 TV + \beta_2 Radio$$

establece que el efecto promedio de las ventas del aumento de una sola unidad del presupuesto en  TV será siempre $\beta_1$, independientemente de la cantidad gastada en la radio.


Para ver si nos hemos dejado alguna dependencia importante en le modelo es conveniente mirar los residuos. 

Veamos  el ajuste y los residuos de un modelo  $Sales = \beta_0 + \beta_1 TV + \beta_2 Radio$

```{r,echo=FALSE,fig.height=3.7,cache =TRUE,results='hide'}
mod=lm(Sales ~ TV + Radio, data=adv)

x=seq(0,300,10)
y=seq(0,50,5)
new.data = expand.grid(TV=x,Radio=y)
new.data$pred = predict(mod,newdata = new.data)
adv$pred=mod$fitted.values

z=matrix(new.data$pred,nrow=length(x))

persp3D(x,y,z,facets=TRUE,
        zlab="Sales",xlab="TV", ylab="Radio",theta=30, phi=30,colkey = FALSE,r=5,d=5)
scatter3D(adv$TV, adv$Radio, adv$Sales,col=1,add=TRUE,pch=19,cex=0.7)
apply(adv,1,function(row) lines3D(x=rep(row[2],2),y=rep(row[3],2),z=c(row[5],row[6]),
                                     colkey=FALSE,add=TRUE,col=1))
```

Se observa que cuando las cantidades gastadas en TV y Radio son grandes a la vez, el modelo sub-estima los valores, mientras que cuando las cantidades son altas en uno solo de los 2 medios, el modelo sobre-estima.
Este hecho nos está indicando que hay una sinergia entre los anuncios simultáneos en ambos medios, que hace que se incrementen las ventas

Añadir un término de interacción al modelo equivale a transformar el modelo en:

$$Sales = \beta_0 + \beta_1 TV + \beta_2 Radio + \beta_3 TV*Radio $$ 

En R definimos este modelo usando como fórmula `Sales ~ TV*Radio`. También puede usarse el operador `:`, el modelo definido por `Sales ~ TV:Radio` solo contendría el término de interacción y no los términos independientes de `Radio`y `TV`. Resumiendo `Sales ~ TV*Radio` es lo mismo que `Sales ~ TV + Radio + TV:Radio`.

```{r}
mod=lm(Sales ~ TV*Radio, data=adv)
summary(mod)
```

Los resultados sugieren que las interacciones son importantes:

- El  p-valor para el término de interacción de TV × radio es extremadamente bajo, lo que indica que existe una fuerte evidencia de $H_A: \beta_3 \neq 0$ .
- El $R^2$ para el modelo con interacción es de 0.968, en comparación con  el 0.897 para el modelo predice las ventas usando TV y Radio sin un término de interacción.

### Interacción con variables cualitativas

Consideremos de nuevo los datos de crédito y consideremos que queremos predecir el `balance` en función del  `income` y de la variable `Student`. Un modelo sin interacción sería

$$
balance \sim \beta_0 + \beta_1 \, income +  
\begin{cases}
\beta_2   &  \text{si la persona i-esima es estudiante} \\
0         &  \text{si la persona i-esima no es estudiante} \\
\end{cases}
$$
$$
= \beta_1 \, income + 
\begin{cases}
\beta_0 + \beta_2   &  \text{si la persona i-esima es estudiante} \\
\beta_0           &  \text{si la persona i-esima no es estudiante} \\
\end{cases}
$$

El *balance* para  estudiantes y los no estudiantes crece con *income* con la misma pendiente pero con distinta ordenada en el origen.

Si añadimos un término de interacción tenemos: 

$$
balance \sim \beta_0 + \beta_1 income +  
\begin{cases}
\beta_2  + \beta_3\, income &  \text{si  estudiante} \\
0                                     &  \text{si  no es estudiante} \\
\end{cases}
$$
$$
= 
\begin{cases}
(\beta_0 + \beta_2) + (\beta_1 + \beta_3)\, income   &  \text{si  estudiante} \\
\beta_0 +  \beta_1 \, income                         & \text{si no estudiante} \\
\end{cases}
$$

Veamos las diferencias entre ambos modelos

```{r}
summary(lm(Balance ~ Income + Student,data=credit))$coefficients
summary(lm(Balance ~ Income*Student,data=credit))$coefficients 

```

Y de forma gráfica 

```{r,echo=FALSE,fig.height=5,fig.width=10}
m1 = lm(Balance ~ Income + Student,data=credit)
m2 = lm(Balance ~ Income*Student,data=credit)

new.data.s=data.frame(Income=seq(0,200,50),Student="Yes")
new.data.ns=data.frame(Income=seq(0,200,50),Student="No")

y1s=predict(m1,new.data.s)
y1ns=predict(m1,new.data.ns)

y2s=predict(m2,new.data.s)
y2ns=predict(m2,new.data.ns)

res=data.frame(Income=rep(seq(0,200,50),4),Student=rep(c("Yes","No"),each=10),
               model=rep(c("Independent","Interaction"),2,each=5),
               Balance=c(y1s,y2s,y1ns,y2ns))

ggplot(res) + geom_point(aes(Income,Balance,color=Student),size=1,alpha=0.5,data=credit) + 
  geom_line(aes(Income,Balance,color=Student)) + facet_wrap(~model)

```

El modelo sin interacción, ajusta dos rectas paralelas en función de los ingresos, una para `StudentYes` y otra para `StudentNo`, en cambio en el modelo con interacción son dos rectas con pendiente e intercepto distintos. 

## No linealidad

Es posible extender el modelo lineal a relaciones no lineales usando el marco de la regresión múltiple. 

Por ejemplo, podemos hace ajustes polinómicos, añadiendo nuevas variables que representan las potencias de la variable predictora. 


Estudiemos la dependencia de la demanda eléctrica española diaria  con la temperatura media.

Ajustemos a un polinomio de orden 2

$$Demanda = \beta_0 + \beta_1 T + \beta_2 T^2$$


```{r,echo=TRUE}
demanda = read.csv("../Datasets/demanda_diaria.csv",sep=";",stringsAsFactors = FALSE)
demanda=mutate(demanda,fecha=as.Date(fecha),
               wd=factor(weekdays(fecha),levels=c("lunes","martes","miércoles","jueves",
                                                  "viernes","sábado","domingo")))

# Filtro solo días martes, miercoles y jueves para no tener que considerar la dependencia con el día de la semana
dem_mxj  = demanda %>% filter(wd %in% c("martes", "miércoles", "jueves"),demanda_mwh>25000)
# Elimino festivos
festivos=c("01-01","01-06","05-01","08-15","08-15","10-12","11-01","12-06","12-08","12-24","12-25","12-31")
ss=c("2014-04-17","2014-04-18","2014-04-19","2014-04-20","2015-04-02","2015-04-03","2015-04-04","2015-04-05")
dem_mxj = dem_mxj %>% filter(!format(fecha,"%m-%d") %in% festivos,!fecha %in% ss)

mod = lm(demanda_mwh ~ temperatura_mean + I(temperatura_mean^2),data=dem_mxj)
summary(mod)
```

Predicciones

```{r,fig.height=3.5,echo=FALSE}
tmp=dem_mxj
tmp$pred = predict(mod)
tmp= tmp %>% arrange(temperatura_mean)
ggplot(tmp) +geom_point(aes(temperatura_mean,demanda_mwh)) + 
  geom_line(aes(temperatura_mean,pred),col="red")
```

Evaluemos la inclusión de términos polinómicos de  más alto orden

```{r,results='as.is'}
# Evaluemos  4 modelos
models = c("demanda_mwh ~ temperatura_mean",
  "demanda_mwh ~ temperatura_mean + I(temperatura_mean^2)",
  "demanda_mwh ~ temperatura_mean + I(temperatura_mean^2) + I(temperatura_mean^3)",
  "demanda_mwh ~ temperatura_mean + I(temperatura_mean^2) + I(temperatura_mean^3) + I(temperatura_mean^4)")

tmp= tmp %>% arrange(temperatura_mean)

res=data.frame()
pred=data.frame()
for(i in 1:4){
  mod=lm(as.formula(models[i]),data=tmp)
  tmp$pred = predict(mod)
  pred=rbind(pred,data.frame(orden=i,tmp[,c("fecha","temperatura_mean","demanda_mwh","pred")]))
  summ= summary(mod)
  res=rbind(res,
            data.frame(orden=i,rse=summ$sigma,r2=summ$r.squared,r2.adj=summ$adj.r.squared))

  }
kable(res)

```

Se observa que el $R^2$ disminuye conforme aumentamos el orden del polinomio de ajuste. La tabla además muestra el $R^2$ ajustado que contiene una penalización debida al aumento de variables predictoras. Esté estadístico también disminuye hasta el orden considerado. 

Veamos las curvas ajustadas

```{r}
 ggplot(pred) + geom_point(aes(temperatura_mean,demanda_mwh)) + 
  geom_line(aes(temperatura_mean,pred,color=factor(orden))) + 
              scale_color_hue("Orden")

```


# Evaluación de modelos

Como ya vimos en la unidad 6, para evaluar el rendimiento de un modelo debemos separar los datos en un conjunto de entrenamiento y otro de test. 

Recordemos de nuevo la diferencia entre los errores de *entrenamiento* y *test*

El **error de test** es el promedio de los errores que resultan de aplicar un cierto método de aprendizaje estadístico en una observación nueva, que no ha sido utilizada para el entrenamiento del modelo.

Por el contrario, el **error de entrenamiento** se calcula fácilmente aplicando el modelo ajustado a las mismas observaciones que se han usado para entrenarlo (determinar sus parámetros en el caso de un modelo paramétrico).

Pero, como ya sabemos, el error de entrenamiento puede ser muy diferente al error de test, y normalmente el primero subestima notablemente el segundo.

![](img/train_vs_test_error.png)

La mejor solución es disponer un gran conjunto de test expresamente designado. Pero, a menudo no es posible disponer de él.

## Estrategia del conjunto de validación

La estrategia más sencilla consiste en dividir aleatoriamente los datos en dos conjuntos. Uno para entrenar el modelo y otro para evaluar los resultados

El modelo se ajusta en el conjunto de entrenamiento, y se utiliza para predecir la
respuesta a las observaciones en el conjunto de validación.

El error de validación  proporciona una estimación del error de test. 
Normalmente se calcula el  MSE en el caso de una respuesta cuantitativa y la tasa de  error de clasificación en 
el caso de una respuesta cualitativa (discreta).


```{r, out.height= 400, out.width= 900, fig.retina = NULL}
knitr::include_graphics("img/validation_set.png")
```

Dividimos los datos aleatoriamente en dos partes: la izquierda para entrenamiento y la derecha  para la validación.

En el ejemplo de la dependencia de la demanda eléctrica con la temperatura, estimemos mediante la estrategia del conjunto de validación, el error de test para ajustes polinómicos de diferente orden. 
Usamos $2/3$ de los datos para entrenamiento y el resto para validación.

Veamos los resultados de los errores de test para diferentes elecciones de los conjuntos de test y valoración

```{r,fig.height=4}

n = nrow(dem_mxj)
nrep=20

set.seed(123)
test.rmse= data.frame()  
train.rmse= data.frame()  
for(i in 1:nrep){
  train=sample(1:n,round(2*n/3))
  test = -train
  for(order in c(1:8)){
    mod=lm(demanda_mwh ~ poly(temperatura_mean, order, raw=TRUE),data=dem_mxj[train,])
    rmse=sqrt(mean((dem_mxj[train,]$demanda_mwh-mod$fitted.values)^2))
    train.rmse=rbind(train.rmse,data.frame(it=i,orden=order,rmse=rmse))
    pred = predict(mod,newdata = dem_mxj[test,])
    rmse=sqrt(mean((dem_mxj[test,]$demanda_mwh-pred)^2))
    test.rmse=rbind(test.rmse,data.frame(it=i,orden=order,rmse=rmse))
    if(rmse>1e5){
      plot(dem_mxj$temperatura_mean[test],pred)
      points(dem_mxj$temperatura_mean[train],dem_mxj$demanda_mwh[train],col=2,cex=0.5)
    }
  }
}

err.df  = rbind(data.frame(set="train",train.rmse),data.frame(set="test",test.rmse))

ggplot(err.df) + 
  geom_point(aes(orden,rmse,color=factor(it)),size=1) + geom_line(aes(orden,rmse,color=factor(it))) + 
  facet_wrap(~set, scales="free") + theme(legend.position="none")

```



Las diferentes curvas corresponden a distintas elecciones de los conjuntos de entrenamiento y validación. Sobre todo, para ajustes con orden polinómico alto se observa mucha variación de los resultados del error de test de un conjunto a otro.

La estimación del error de test sobre el conjunto de validación puede ser  muy
variable, y depende de que observaciones se  incluyan en el conjunto de entrenamiento, y que observaciones se incluyan en el conjunto de validación.

En la estrategia del conjunto de  validación, sólo un subconjunto de las observaciones, aquellas que se incluyen en el conjunto de entrenamiento, se utilizan para ajustar el modelo.
Esto sugiere que el error estimado en el conjunto de validación puede tender a sobrestimar el error de test comparado con un modelo ajustado en el conjunto de datos total.


## Validación cruzada (k-fold cross validation)

El método de validación cruzada ayuda a mejorar las estimaciones del error de test, superando parte de los problemas que hemos mencionado anteriormente.

La idea del método es dividir aleatoriamente los datos en K conjuntos de igual tamaño. 
Entonces, dejamos aparte el conjunto k, ajustamos el modelo en las restantes K-1 partes y hacemos predicciones sobre el conjunto k.

Repetimos el procedimiento para cada parte $k=1,2,\dots,K$ y combinamos los resultados de los errores de test obtenidos  

![](img/k_fold_cv.jpg)



Veamos ahora como estimar el error de test este caso. Llamemos  $C_1,C_2, \dots,C_K$ a los K conjuntos en los que hemos dividido los datos. Cada conjunto contiene $n_k$ observaciones. Si $N$ es múltiplo de K, $n_k=N/K$ y si no haremos el reparto lo más equitativamente posible.

Estimamos el $MSE$ de test como 

$$ CV_{(K)} = \sum_{k=1}^K \frac{n_k}{n} MSE_k$$

donde $MSE_k = \sum_{i\in C_k} (y_i - \hat{y_i})^2/n_k$ e $\hat{y_i}$ es el ajuste obtenido para la observación i-esima por un modelo ajustado con conjunto total con la parte $C_k$ eliminada.

Puede estimarse el error en la estimación del error de test como 

$$ SE(CV_{(K)}) = \sqrt{\frac{(MSE_k - \bar{MSE_k})^2}{K-1}}$$

La fórmula anterior **no es correcta** del todo  pero es útil para hacerse una idea del posible error en la estimación del error de test


Veamos un ejemplo del uso de la validación cruzada, con $k=5$, para estimar error de test para los diferentes ajustes polinómicos de la demanda eléctrica en función de la temperatura 

```{r}
K=5
set.seed(112)
folds = sample(rep(1:K, length = nrow(dem_mxj)))
n=nrow(dem_mxj)
cv.mse= data.frame()  
train.mse= data.frame()  
for(k in 1:K){
  train=which(folds!=k)
  test=-train
  for(order in c(1:8)){
    mod=lm(demanda_mwh ~ poly(temperatura_mean, order, raw=TRUE),data=dem_mxj[train,])
    mse=mean((dem_mxj[train,]$demanda_mwh-mod$fitted.values)^2)
    train.mse=rbind(train.mse,data.frame(k=k,orden=order,num=length(train),mse=mse))
    pred = predict(mod,newdata = dem_mxj[test,])
    mse=mean((dem_mxj[test,]$demanda_mwh-pred)^2)
    cv.mse=rbind(cv.mse,data.frame(k=k,orden=order,num=n-length(train),mse=mse))
  }
}

err.df  = rbind(data.frame(set="train",train.mse),data.frame(set="cv",cv.mse))

cv.err = err.df %>% filter(set == "cv")  %>% group_by(orden) %>%
  summarise(mse=sum(num*mse)/n)


ggplot(err.df %>% filter(set == "cv")) + 
  geom_point(aes(orden,sqrt(mse),color=factor(k)),size=1) + geom_line(aes(orden,sqrt(mse),color=factor(k))) +
  geom_line(aes(orden,sqrt(mse)),data=cv.err,color="black",size=1) +
  scale_color_brewer("fold",palette = "Set1") + 
    theme(legend.position="bottom")

```


La curva negra es la estimación por método de validación cruzada del error de test en función del orden del polinomio de ajuste. Se observa que él mínimo está entre 4 y 6. En este caso, siguiendo la máxima de cuanto más simple mejor, elegiríamos orden 4 para el ajuste de nuestros modelos predictivos de la demanda eléctrica en función de la temperatura.


¿Cuál es la elección adecuada de $K$ ? 

Cuanto más grande es $K$ mayores son los conjuntos de entrenamiento y por tanto menor es el sesgo de los modelos ajustados. En el extremo $K=n$ cada modelo se entrena con $n-1$ observaciones, por tanto todos los modelos entrenados serán muy parecidos al que obtendríamos entrenando con el conjunto completo.

Sin embargo, cuanto más grande es $K$ las predicciones sobre los distintos conjuntos de validación, están muy correlacionadas entre si, por tanto su varianza será mayor (la varianza en series de datos correlacionadas es mayor que si no lo están)

Por tanto, tenemos de nuevo que adoptar un compromiso entre sesgo y varianza. La experiencia y muchos estudios al respecto, recomiendan usar $K=5$ o $K=10$ para la validación cruzada.

# Selección de variables

Ahora vamos a centrarnos en problemas donde disponemos de múltiples variables predictoras. Como ya hemos visto utilizar en los modelos demasiadas variables, puede producir efectos de "sobre-entrenamiento" que empeoren los resultados al aplicar los modelos sobre datos nuevos.

Vamos a ver unos métodos para evaluar el error de test para las diferentes combinaciones de variables que pueden entrar en los modelos. En este curso nos centraremos de nuevo en modelos lineales, aunque las ideas son generalizables a otros modelos más complicados o a problemas de clasificación.


En general estos métodos pretenden reducir el número de predictores que intervienen en los modelos, con dos objetivos:


- **Reducir la varianza**: Se evita el sobre-entrenamiento y por tanto la varianza excesiva en las predicciones en un conjunto de test.

- **Interpretabilidad**: Eliminando las características irrelevantes obtenemos modelos que se interpretan más fácilmente


Estas técnicas se denominan de **Selección de variables** (subset selection) y consisten en identificar el subconjunto de los p predictores para los que tenemos evidencia que realmente están relacionados con la respuesta para después ajustar un modelo lineal restringido a ese conjunto de predictores.


## Selección del mejor subconjunto

La estrategia general para resolver este problema sería  explorar todos los posibles modelos que se pueden crear con p variables. Puede resumirse de la siguiente manera:

1. Sea $\mathcal{M_0}$ el modelo nulo, sin predictores. Este modelo predice simplemente la media de las observaciones de la respuesta $y_i$

2. Para $k=1,2,\dots,p$:
    - Ajustamos los ${p \choose k}$ posibles modelos que usan exactamente k predictores
    - Escogemos el mejor de los ${p \choose k}$ modelos, y le llamamos  $\mathcal{M_k}$. Por mejor entendemos el tenga menor $RSS$ o mayor $R^2$. 
    
3. Por último seleccionaremos el mejor entre $\mathcal{M_0},\dots,\mathcal{M_p}$ mediante algún mecanismo que estime el error de test de cada uno de los modelos, como la validación cruzada. 

Veamos un ejemplo. Con los datos de *Credit Card*, exploramos los posibles modelos para predecir el balance.

```{r}
credit=read.csv("../Datasets/Credit.csv")[-1]
head(credit)
```


Tenemos un total de 10 variables predictoras, contando con las variables dummy creadas para modelar la etnicidad.
Para $k=1,2,\dots,10$ evaluamos todos los posibles modelos que pueden crearse con $k$ variables:

- k=1: $\mathcal{M}_{11}:Balance \sim Income$,  $\mathcal{M}_{12}:Balance \sim Rating$, ...
Elegimos entre $\mathcal{M}_{11}, \dots, \mathcal{M}_{12}$, $\dots$, $\mathcal{M}_{1p}$,  aquel con menor $RSS$ (o mayor $R^2$) en el conjunto de entrenamiento y le llamamos $\mathcal{M}_{1}$

- k=2: $\mathcal{M}_{21}:Balance \sim Income + Rating$,  $\mathcal{M}_{22}:Balance \sim Income + Cards$, ...
Elegimos entre $\mathcal{M}_{21}, \dots, \mathcal{M}_{22}$, $\dots$, $\mathcal{M}_{1\binom{p}{2}}$ aquel con menor $RSS$ (o mayor $R^2$) en el conjunto de entrenamiento y le llamamos $\mathcal{M}_{2}$

- \dots

- k=p: $\mathcal{M}_{p}:Balance \sim .$





Veamos gráficamente el $RSS$ y $R^2$ de todos los posibles modelos realizables con los datos y la linea roja muestra el menor para cada posible número de variables

```{r,cache=TRUE,fig.height=4.5,fig.width=11, echo=FALSE, fig.show='hold'}

x=model.matrix(Balance ~ .,data=credit) 
y=credit$Balance
p=ncol(x)-1
n=nrow(x)
rss=list()
r2=list()
#Null model
mod=lm(y~ 1)
rss[[1]] = sum((y-mod$fitted.values)^2)
r2[[1]] = summary(mod)$r.squared
for(k in 1:p){
  combs=combn(p,k)
  rss[[k+1]]=numeric()
  r2[[k+1]]=numeric()
  for(j in 1:ncol(combs)){
    df=data.frame(Balance=y,x[,combs[,j]+1])
    mod=lm(Balance~.,data=df)
    rss[[k+1]] = c(rss[[k+1]],sum((y-mod$fitted.values)^2))
    r2[[k+1]] = c(r2[[k+1]],summary(mod)$r.squared)
  }
}

par(mfrow=c(1,2))
minrss=NULL
plot(0,rss[[1]],cex=0.5,xlim=c(0,11),ylim=c(0,1e8),ylab="RSS",xlab="Número de variables")
for(k in 0:p){
  nm=length(rss[[k+1]])
  points(rep(k,nm),rss[[k+1]],cex=0.5)
  minrss=c(minrss,min(rss[[k+1]]))
}
lines(0:p,minrss,col=2)

maxr2=NULL
plot(0,r2[[1]],cex=0.5,xlim=c(0,11),ylim=c(0,1),ylab="R square",xlab="Número de variables")
for(k in 0:p){
  nm=length(r2[[k+1]])
  points(rep(k,nm),r2[[k+1]],cex=0.5)
  maxr2=c(maxr2,max(r2[[k+1]]))
}
lines(0:p,maxr2,col=2)
par(mfrow=c(1,1))

```

Ahora por último deberíamos evaluar los modelos $\mathcal{M_0},\dots,\mathcal{M_p}$ mediante validación cruzada para ver cual de ellos da mejor estimación del error de test. 

De todas formas, en el punto anterior ya nos encontramos un problema cuando el número de variables crece por encima de 20. El número de modelos a explorar es $2^p$, con 20 variables tendríamos que explorar ya más de un millón de modelos. 

Por otro lado, la enorme cantidad de modelos a explorar puede hacer tender el método hacia el sobre-aprendizaje. Con $2^p$ modelos donde elegir siempre habrá alguno que por casualidad ajuste mejor los datos de entrenamiento provocando errores mayores en datos de test.  

Para simplificar la exploración de modelos suelen usarse los métodos de selección paso a paso, que tienen dos versiones: 

- Selección hacía delante

- Selección hacía atrás

## Selección hacía delante 

En este método se empieza por el modelo nulo y se va añadiendo un predictor por turno

En cada paso se incorpora la variable que provoca una mayor mejora en la bondad del ajuste:


1. Sea $\mathcal{M_0}$ el modelo nulo, sin predictores. 

2. Para $k=1,2,\dots,p$:

    - Ajustamos los $p-k$ posibles modelos que incorporan un predictor a $\mathcal{M_{k-1}}$ 
    - Escogemos el mejor de los $p-k$ modelos, y le llamamos  $\mathcal{M_k}$. Por mejor entendemos el tenga menor $RSS$ o mayor $R^2$. 
    
3.  Por último seleccionaremos el mejor entre $\mathcal{M_0},\dots,\mathcal{M_p}$ mediante algún mecanismo que estime el error de test de cada uno de los modelos, como la validación cruzada.


La ventaja computacional es clara: en este método se ajustan $1 + p(p + 1)/2$ modelos frente a los $2^p$ modelos de la búsqueda exhaustiva

Por otro lado, este tipo de búsqueda jerárquica no garantiza la obtención del modelo óptimo. Aunque esto, en algún caso puede ser una ventaja en lugar de un problema debido al sobre-aprendizaje. 

Veamos un ejemplo de los modelos que se seleccionarían para  los datos de *credit card*.

Si comparamos la búsqueda completa con la búsqueda hacia delante, la única diferencia, en este ejemplo, está en los modelos con 4 variables. 

Para ello usaremos el paquete `leaps` y en concreto la función `regsubset`, cuyo argumento `method` puede tomar los argumentos "exhaustive", "forward" o  "backward".

La siguiente tabla muestra las variables seleccionadas para los modelos candidatos $\mathcal{M_0},\dots,\mathcal{M_p}$ mediante la busqueda exhaustiva y paso a paso hacía delante.

```{r,eval=TRUE, fig.height=4}
library(leaps)
regfit.full=regsubsets(Balance~., data=credit, nvmax=11, method = "exhaustive")
regfull.summary=summary(regfit.full)

regfit.fdw=regsubsets(Balance~., data=credit, nvmax=11, method = "forward")
regfdw.summary=summary(regfit.fdw)

varmat= regfull.summary$which[,-1]
varnames=dimnames(varmat)[[2]]
vars.full = apply(varmat,1,function(x){paste(varnames[x],collapse = ",")})
p=ncol(varmat)  
varmat= regfdw.summary$which[,-1]
varnames=dimnames(varmat)[[2]]
vars.fdw = apply(varmat,1,function(x){paste(varnames[x],collapse = ",")})

res=data.frame(k=1:p,Full=vars.full, Forward=vars.fdw)
kable(res[1:11,])

```

## Selección hacía atrás

La otro posibilidad de búsqueda paso a paso es la selección hacía atrás

1. Comenzamos por  $\mathcal{M_p}$ el modelo con todos  predictores. 

2. Para $k=p-1,p-2,\dots,1$:
    - Ajustamos los $k$ posibles modelos que eliminan un predictor al modelo del paso anterior $\mathcal{M_{k+1}}$ 
    - Escogemos el mejor de los $k$ modelos, y le llamamos  $\mathcal{M_k}$. Por mejor entendemos el tenga menor $RSS$ o mayor $R^2$. 
    
    

Comparemos, con el mismo ejemplo,  las variables seleccionadas **forward** y **backward**

```{r}
regfit.fdw=regsubsets(Balance~., data=credit, nvmax=11, method = "forward")
regfdw.summary=summary(regfit.fdw)

regfit.bkw=regsubsets(Balance~., data=credit, nvmax=11, method = "backward")
regbkw.summary=summary(regfit.bkw)

varmat= regbkw.summary$which[,-1]
varnames=dimnames(varmat)[[2]]
vars.bck = apply(varmat,1,function(x){paste(varnames[x],collapse = ",")})
p=ncol(varmat)  

varmat= regfdw.summary$which[,-1]
varnames=dimnames(varmat)[[2]]
vars.fdw = apply(varmat,1,function(x){paste(varnames[x],collapse = ",")})

res=data.frame(k=1:p,Backward=vars.bck, Forward=vars.fdw)
kable(res[1:11,])
```

## Elección del mejor modelo

El modelo que contiene todos los predictores siempre tendrá el $RSS$ más pequeño y el 
$R^2$ más grande, ya que estas cantidades están relacionadas con el error de entrenamiento.

Pero, lo que deseamos es elegir un modelo con error de test pequeño, no un modelo
con error de entrenamiento bajo. Por lo tanto , el $RSS$ y el  $R^2$ no son las métricas adecuadas para la selección de la mejor modelo entre una colección de modelos con diferente número de predictores .

Para la estimación del error de test habitualmente se usan dos estrategias:

- Podemos estimar indirectamente el error de test  haciendo un 
ajuste al error de entrenamiento para tener en cuenta el sesgo debido
al sobre-entrenamiento que puede provocar un número mayor de predictores. Para ello existen varios estimadores como el $C_p$, $AIC$, $BIC$  y el  $R_2$ ajustado. 


- O podemos estimar directamente el error de test, usando un enfoque de  conjunto de validación o de  validación cruzada, tal como hemos visto  anteriormente. Este es el enfoque que vamos a utilizar en este curso. Disponiendo de ordenadores potentes, no tiene sentido usar estimaciones aproximadas del error de test. Además  esos métodos solo son válidos para modelos lineales, en cambio la validación cruzada es válida para cualquier tipo de modelo. 


El objetivo, recordemos, es seleccionar entre los modelos $\mathcal{M_0},\dots, \mathcal{M_p}$ seleccionados por el método empleado para la selección de variables, sea la búsqueda completa, o paso a paso, hacia delante o hacía atrás, el modelo en el que el error de test estimado sea el mínimo.  

Calculamos el error en un conjunto de validación o mediante validación cruzada
para cada modelo $\mathcal{M_k}$ en cuestión, y luego
seleccionaremos el $k$ para el cual es el error de test estimado sea mínimo.


Veamos para el ejemplo de Credit Card el RMSE estimado por conjunto de validación y validación cruzada. Lo hacemos para los modelos seleccionados mediante la selección paso a paso hacia delante

```{r,results='hide'}

# Definimos una función predict para simplificar las cosas
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(eval(object$call[[2]]))
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  mat[,names(coefi)] %*% coefi
}

# k-fold cross validation

K=10
nv=11
folds=sample(rep(1:K,length=nrow(credit)))
#table(folds)
cv.errors=matrix(NA,K,nv)

for(k in 1:K){
  print(k)
  # Elegimos los modelos candidatos para cada fold de la validación cruzada
  best.fit=regsubsets(Balance~.,data=credit[folds!=k,],nvmax=nv,method="forward")
  for(i in 1:nv){
    print(i)
    pred=predict(best.fit,credit[folds==k,],id=i)
    cv.errors[k,i]=mean((credit$Balance[folds==k]-pred)^2)
  }
}
rmse.cv=sqrt(apply(cv.errors,2,mean))

plot(rmse.cv,pch=19,type="b")
imin=which.min(rmse.cv)
points(imin,rmse.cv[imin],pch=19,col=2)

```

Se observa que el mínimo error de test se produce para un modelo con 6 variables. Para elegirlas lo haríamos con el conjunto completo de datos y serían Income,Limit,Rating,Cards,Age,StudentYes.

# Qué has aprendido

En esta unidad hemos aprendido a ajustar modelos y realizar predicciones mediante modelos lineales. Aunque las dependencias entre variables en la realidad rara vez son lineales, hemos visto que estos modelos pueden ser muy eficaces y útiles.

Además muchos de los conceptos que hemos desarrollado en este tema, como: 

- medida de la precisión de los modelos
- determinación de la influencia de las distintas variables
- métodos para la determinación del error de tes
- selección de variables

son aplicables a modelos estadísticos más complejos. Ahora ya tienes los conocimientos necesarios para adentrarte por tu cuenta en el aprendizaje de métodos más sofisticados como:

- Modelos aditivos generalizados (GAM)
- Árboles de regresión
- Bosques aleatorios (Random Forests)
- Gradient boosting
- Redes Neuronales

# Autoevaluación

1.  Gráficamente la relación según un modelo lineal entre la variable respuesta $Y$ y las variables predictoras $X_1$,$X_2$ se representa por una

    a. recta
    b. parábola
    c. plano
    d. hiperplano

2.  El procedimiento habitual para estimar los coeficientes de un modelo lineal se denomina 

    a. Estimación por máxima verosimilitud
    b. Ajuste por componentes principales
    c. Optimización convexa
    d. Ajuste de mínimos cuadrados
    
3.  Si ejecutamos en R `lm(dist~speed, data=cars)`. ¿Cuál es la pendiente ($\beta_1$) estimada?

    a. $0.932$  
    b. $1.932$  
    c. $2.932$  
    d. $3.932$ 
    
4.  Si `mod <- lm(dist~speed, data=cars)`, ¿qué comando me proporciona el detalle de los coeficientes, errores y p-valores? 

    a. summary(mod)
    b. summarize(mod)
    c. coef(mod)
    d. predict(mod)
    
5.  Si entreno un modelo lineal sobre los datos `iris`con fórmula 
`Petal.Length ~ .` , ¿qué variables tienen un parámetro asociado distinto de cero con un grado de significación de 0.001? 

    a. Sepal.Length, Petal.Width
    b. Sepal.Length, Petal.Width, Speciesversicolor
    c. Sepal.Length, Petal.Width, Speciesversicolor,Speciesvirginica 
    d. Sepal.Length, Sepal.Width, Petal.Width, Speciesversicolor,Speciesvirginica 

6.  Cómo definirías un modelo lineal para los datos iris  para modelar la variable `Petal.Length` en función de  la variable `Species` y `Petal.Width` incluyendo un término de interacción entre ellas

    a. `lm(Petal.Length ~ Species + Petal.Width, data=iris)`
    b. `lm(Petal.Length ~ Species*Petal.Width, data=iris)`
    c. `lm(Petal.Length ~ Species - Petal.Width, data=iris)`
    d. `lm(Petal.Length ~ Species^Petal.Width, data=iris)`
    
7.  ¿Cuál se estos 4 modelos tiene menor error estándar de los residuos en el conjunto de entrenamiento? (Usa la función summary para averiguarlo)

    a. `lm(Petal.Length ~ Species + Petal.Width, data=iris)`
    b. `lm(Petal.Length ~ Species:Petal.Width, data=iris)`
    c. `lm(Petal.Length ~ Species*Petal.Width, data=iris)`
    d. `lm(Petal.Length ~ .-Species - Petal.Width, data=iris)`

8.  Para ajustar un modelo polinómico de orden 2 la fórmula apropiada es

    a. `y ~ x + I(x^2)`
    b. `y ~ x + x**2`
    c. `y ~ x + x^2` 
    d. Todas las anteriores son válidas

9.  ¿En cuántos grupos es recomendable dividir los datos en procedimiento de validación cruzada)

    a. 5 o 10
    b. Más de 30
    c. Un número par de grupos
    d. Un número impar de grupos
    
10.  Cuál de los siguientes comandos usarías para un proceso de selección de variables paso a paso hacia delante 

    a. `regsubsets(y ~ ., data=mydata, method = "backward")` 
    b. `regsubsets(y ~ ., data=mydata, method = "forward")` 
    c. `regsubsets(y ~ ., data=mydata, method = "forehand")` 
    d. `regsubsets(y ~ ., data=mydata, method = "exhaustive")` 
    


# Soluciones

1-c;2-d;3-d;4-a;5-c;6-b;7-c;8-a;9-a;10-b

# Bibliografía

- "An Introduction to Statistical Learning, with applications in R", G. James, D. Witten,  T. Hastie and R. Tibshirani (Springer, 2013) 

- R for Data Science, Garrett Grolemund, Hadley Wickham. O’Reilly (2016)
  Disponible online en  <http://r4ds.had.co.nz>
  
- Chester Ismay, Albert Y. Kim.  ModernDive: an Introduction to Statistical and Data Sciences via R (2017)
<https://ismayc.github.io/moderndiver-book/references.html>

- Dalgaard P. Introductory statistics with R (Springer, 2002)

## Otros créditos

Some of the figures in this presentation are taken from "An Introduction to Statistical Learning, with applications in R"  (Springer, 2013) with permission from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani 












